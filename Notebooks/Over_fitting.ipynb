{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Over-fitting.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "nhpGyHEY8muE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q opencv-contrib-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THco946mUgpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import required pacakges\n",
        "import numpy as np\n",
        "\n",
        "# ML packages\n",
        "import tensorflow as tf\n",
        "\n",
        "# Visualization packages\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rzXEQLuEUv0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d06c071-e603-439b-c111-2c3358bff0b9"
      },
      "cell_type": "code",
      "source": [
        "# Create the 2-D dataset of points in range [0, 1)\n",
        "numSamples = 200\n",
        "numDimensions = 2\n",
        "\n",
        "trainDataPoints = np.array([np.random.rand(numSamples,), np.random.rand(numSamples,)]).T\n",
        "testDataPoints = np.array([np.random.rand(numSamples,), np.random.rand(numSamples,)]).T\n",
        "\n",
        "# Desired ellipse params\n",
        "# (x - x0)^2/a^2 + (y - y0)^2/b^2 = r^2\n",
        "center = (np.random.random(), np.random.random())\n",
        "a = np.random.random()\n",
        "b = np.random.random()\n",
        "r = np.random.normal(loc=0.5, scale=0.15)\n",
        "print (\"Data params | Center: %s | a: %f | b: %f | r: %f\" % (str(center), a, b, r))\n",
        "\n",
        "trainLabels = ((np.square(trainDataPoints[:, 0] - center[0]) / np.square(a)) + (np.square(trainDataPoints[:, 1] - center[1]) / np.square(b)) > np.square(r))\n",
        "testLabels = ((np.square(testDataPoints[:, 0] - center[0]) / np.square(a)) + (np.square(testDataPoints[:, 1] - center[1]) / np.square(b)) > np.square(r))\n",
        "\n",
        "trainLabelsFloat = trainLabels.astype(float)\n",
        "testLabelsFloat = trainLabels.astype(float)\n",
        "\n",
        "outerPoints = trainDataPoints[trainLabels]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data params | Center: (0.7591445840052834, 0.2840649937960117) | a: 0.738812 | b: 0.592362 | r: 0.511187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WmXXOsLH_hHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def drawEllipse(\n",
        "        img, center, axes, angle,\n",
        "        startAngle, endAngle, color,\n",
        "        thickness=3, lineType=cv2.LINE_AA, shift=10):\n",
        "    center = (\n",
        "        int(round(center[0] * 2**shift)),\n",
        "        int(round(center[1] * 2**shift))\n",
        "    )\n",
        "    axes = (\n",
        "        int(round(axes[0] * 2**shift)),\n",
        "        int(round(axes[1] * 2**shift))\n",
        "    )\n",
        "    cv2.ellipse(\n",
        "        img, center, axes, angle,\n",
        "        startAngle, endAngle, color,\n",
        "        thickness, lineType, shift)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "subGpXnwVM_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1157
        },
        "outputId": "18e3ebcc-3365-41ce-f84a-d435ff4c009c"
      },
      "cell_type": "code",
      "source": [
        "# Create the data set plot\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "plotDim = 1000\n",
        "dataPlot = np.ones((plotDim, plotDim, 3), dtype=np.float32)\n",
        "scaledIndices = np.floor(trainDataPoints * plotDim).astype(np.int32)\n",
        "\n",
        "# Create decision boundary meshgrid\n",
        "grid = np.array([[i, j] for i in range(plotDim) for j in range(plotDim)])\n",
        "for x in range(plotDim):\n",
        "  for y in range(plotDim):\n",
        "    gridLabel = ((np.square((x / plotDim) - center[0]) / np.square(a)) + (np.square((y / plotDim) - center[1]) / np.square(b)) > np.square(r))\n",
        "    if gridLabel:\n",
        "      dataPlot[y, x, :] = [0.5, 0.5, 0.5] # Exchange x and y because of i and j\n",
        "\n",
        "for i in range(scaledIndices.shape[0]):\n",
        "  color = (0.0, 0.0, 0.0)\n",
        "  if trainLabels[i]:\n",
        "    color = (1.0, 0.0, 0.0) # RGB\n",
        "  \n",
        "  cv2.circle(dataPlot, (scaledIndices[i, 0], scaledIndices[i, 1]), 5, color, -1)\n",
        "\n",
        "#drawEllipse(dataPlot, center=center, axes=((a, b) if a > b else (b, a)), angle=0.0, startAngle=0.0, endAngle=360, color=(1.0, 0.0, 1.0), thickness=50)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.axis('off')\n",
        "plt.imshow(dataPlot)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f66fae38c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGUAAARiCAYAAAAN70rmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6701382160>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YUxDvYzYU2rH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3093b761-d771-4c70-8779-9a1d2b5e130e"
      },
      "cell_type": "code",
      "source": [
        "# Perform polynomial logistic regression\n",
        "numIterations = 1000\n",
        "learningRate = 1e-1\n",
        "\n",
        "# Clear previous graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "dataPointPlaceholder = tf.placeholder(tf.float32, shape=(None, numDimensions), name=\"dataPointPlaceholder\")\n",
        "labelPlaceholder = tf.placeholder(tf.float32, shape=(None,), name=\"labelPlaceholder\")\n",
        "\n",
        "# Create the model variables\n",
        "def addMLPNode(dataInput, outDim):\n",
        "\tW = tf.Variable(tf.random_normal([int(dataInput.get_shape()[-1]), outDim]))\n",
        "\tb = tf.Variable(tf.zeros([outDim]))\n",
        "\ty = tf.matmul(dataInput, W) + b\n",
        "\treturn y\n",
        "\n",
        "# Create the model\n",
        "hiddenDimension = 10\n",
        "mlp = tf.nn.tanh(addMLPNode(dataPointPlaceholder, hiddenDimension))\n",
        "mlp = tf.nn.sigmoid(addMLPNode(mlp, 1))\n",
        "y = tf.reshape(mlp, [-1])\n",
        "print (y.get_shape())\n",
        "\n",
        "# Define the loss function\n",
        "epsilon = 1e-8\n",
        "with tf.name_scope('loss') as scope:\n",
        "\tloss = - (labelPlaceholder * tf.log(y + epsilon)) - ((1.0 - labelPlaceholder) * tf.log(1.0 - y + epsilon))\n",
        "\tloss = tf.reduce_mean(loss)\n",
        "  \n",
        "with tf.name_scope('accuracy'):\n",
        "\tcorrectPredictions = tf.equal(tf.round(y), tf.round(labelPlaceholder))\n",
        "\taccuracy = tf.reduce_mean(tf.cast(correctPredictions, tf.float32), name='accuracy')\n",
        "\n",
        "with tf.name_scope('optimizer') as scope:\n",
        "\toptimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FyDS2j-OU4jQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18053
        },
        "outputId": "88d4f8c8-37bb-4e20-956e-0f2480362d79"
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print (\"Training model\")\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "\n",
        "with tf.Session(config=config) as sess:\n",
        "  # Initializing the variables\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "\n",
        "  for i in range(numIterations):\n",
        "    _, trainLoss, trainAccuracy = sess.run([optimizer, loss, accuracy], feed_dict={dataPointPlaceholder: trainDataPoints, labelPlaceholder: trainLabelsFloat})\n",
        "    print (\"Iteration: %d | Loss: %f | Accuracy: %f\" % (i + 1, trainLoss, trainAccuracy))\n",
        "  print (\"Training finished!\")\n",
        "\n",
        "  # Generate predictions on the test set\n",
        "  testSetPredictions, testLoss, testAccuracy = sess.run([y, loss, accuracy], feed_dict={dataPointPlaceholder: testDataPoints, labelPlaceholder: testLabelsFloat})\n",
        "  \n",
        "  # Get decision boundary\n",
        "  gridPredictions = sess.run([y], feed_dict={dataPointPlaceholder: grid / plotDim})[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model\n",
            "Iteration: 1 | Loss: 1.321186 | Accuracy: 0.295000\n",
            "Iteration: 2 | Loss: 0.628778 | Accuracy: 0.660000\n",
            "Iteration: 3 | Loss: 0.490170 | Accuracy: 0.705000\n",
            "Iteration: 4 | Loss: 0.582252 | Accuracy: 0.705000\n",
            "Iteration: 5 | Loss: 0.618125 | Accuracy: 0.705000\n",
            "Iteration: 6 | Loss: 0.576817 | Accuracy: 0.705000\n",
            "Iteration: 7 | Loss: 0.491358 | Accuracy: 0.710000\n",
            "Iteration: 8 | Loss: 0.400967 | Accuracy: 0.780000\n",
            "Iteration: 9 | Loss: 0.341644 | Accuracy: 0.855000\n",
            "Iteration: 10 | Loss: 0.332894 | Accuracy: 0.860000\n",
            "Iteration: 11 | Loss: 0.362701 | Accuracy: 0.840000\n",
            "Iteration: 12 | Loss: 0.394338 | Accuracy: 0.830000\n",
            "Iteration: 13 | Loss: 0.399331 | Accuracy: 0.825000\n",
            "Iteration: 14 | Loss: 0.376209 | Accuracy: 0.840000\n",
            "Iteration: 15 | Loss: 0.341809 | Accuracy: 0.845000\n",
            "Iteration: 16 | Loss: 0.315144 | Accuracy: 0.865000\n",
            "Iteration: 17 | Loss: 0.306153 | Accuracy: 0.875000\n",
            "Iteration: 18 | Loss: 0.312975 | Accuracy: 0.860000\n",
            "Iteration: 19 | Loss: 0.326446 | Accuracy: 0.860000\n",
            "Iteration: 20 | Loss: 0.336883 | Accuracy: 0.855000\n",
            "Iteration: 21 | Loss: 0.338707 | Accuracy: 0.855000\n",
            "Iteration: 22 | Loss: 0.331640 | Accuracy: 0.855000\n",
            "Iteration: 23 | Loss: 0.319504 | Accuracy: 0.870000\n",
            "Iteration: 24 | Loss: 0.307900 | Accuracy: 0.865000\n",
            "Iteration: 25 | Loss: 0.301597 | Accuracy: 0.880000\n",
            "Iteration: 26 | Loss: 0.302326 | Accuracy: 0.860000\n",
            "Iteration: 27 | Loss: 0.307964 | Accuracy: 0.860000\n",
            "Iteration: 28 | Loss: 0.313905 | Accuracy: 0.865000\n",
            "Iteration: 29 | Loss: 0.316022 | Accuracy: 0.860000\n",
            "Iteration: 30 | Loss: 0.313120 | Accuracy: 0.870000\n",
            "Iteration: 31 | Loss: 0.307172 | Accuracy: 0.860000\n",
            "Iteration: 32 | Loss: 0.301541 | Accuracy: 0.870000\n",
            "Iteration: 33 | Loss: 0.298749 | Accuracy: 0.880000\n",
            "Iteration: 34 | Loss: 0.299239 | Accuracy: 0.880000\n",
            "Iteration: 35 | Loss: 0.301596 | Accuracy: 0.870000\n",
            "Iteration: 36 | Loss: 0.303717 | Accuracy: 0.870000\n",
            "Iteration: 37 | Loss: 0.304066 | Accuracy: 0.870000\n",
            "Iteration: 38 | Loss: 0.302354 | Accuracy: 0.870000\n",
            "Iteration: 39 | Loss: 0.299452 | Accuracy: 0.870000\n",
            "Iteration: 40 | Loss: 0.296755 | Accuracy: 0.880000\n",
            "Iteration: 41 | Loss: 0.295357 | Accuracy: 0.875000\n",
            "Iteration: 42 | Loss: 0.295469 | Accuracy: 0.865000\n",
            "Iteration: 43 | Loss: 0.296401 | Accuracy: 0.870000\n",
            "Iteration: 44 | Loss: 0.297078 | Accuracy: 0.870000\n",
            "Iteration: 45 | Loss: 0.296759 | Accuracy: 0.870000\n",
            "Iteration: 46 | Loss: 0.295452 | Accuracy: 0.870000\n",
            "Iteration: 47 | Loss: 0.293781 | Accuracy: 0.870000\n",
            "Iteration: 48 | Loss: 0.292493 | Accuracy: 0.870000\n",
            "Iteration: 49 | Loss: 0.291966 | Accuracy: 0.880000\n",
            "Iteration: 50 | Loss: 0.292053 | Accuracy: 0.880000\n",
            "Iteration: 51 | Loss: 0.292269 | Accuracy: 0.880000\n",
            "Iteration: 52 | Loss: 0.292144 | Accuracy: 0.875000\n",
            "Iteration: 53 | Loss: 0.291501 | Accuracy: 0.880000\n",
            "Iteration: 54 | Loss: 0.290509 | Accuracy: 0.880000\n",
            "Iteration: 55 | Loss: 0.289520 | Accuracy: 0.880000\n",
            "Iteration: 56 | Loss: 0.288829 | Accuracy: 0.880000\n",
            "Iteration: 57 | Loss: 0.288494 | Accuracy: 0.875000\n",
            "Iteration: 58 | Loss: 0.288331 | Accuracy: 0.870000\n",
            "Iteration: 59 | Loss: 0.288071 | Accuracy: 0.870000\n",
            "Iteration: 60 | Loss: 0.287551 | Accuracy: 0.870000\n",
            "Iteration: 61 | Loss: 0.286805 | Accuracy: 0.870000\n",
            "Iteration: 62 | Loss: 0.286010 | Accuracy: 0.885000\n",
            "Iteration: 63 | Loss: 0.285340 | Accuracy: 0.885000\n",
            "Iteration: 64 | Loss: 0.284850 | Accuracy: 0.880000\n",
            "Iteration: 65 | Loss: 0.284458 | Accuracy: 0.880000\n",
            "Iteration: 66 | Loss: 0.284026 | Accuracy: 0.880000\n",
            "Iteration: 67 | Loss: 0.283459 | Accuracy: 0.880000\n",
            "Iteration: 68 | Loss: 0.282764 | Accuracy: 0.880000\n",
            "Iteration: 69 | Loss: 0.282029 | Accuracy: 0.885000\n",
            "Iteration: 70 | Loss: 0.281345 | Accuracy: 0.885000\n",
            "Iteration: 71 | Loss: 0.280746 | Accuracy: 0.885000\n",
            "Iteration: 72 | Loss: 0.280192 | Accuracy: 0.885000\n",
            "Iteration: 73 | Loss: 0.279607 | Accuracy: 0.885000\n",
            "Iteration: 74 | Loss: 0.278940 | Accuracy: 0.885000\n",
            "Iteration: 75 | Loss: 0.278200 | Accuracy: 0.885000\n",
            "Iteration: 76 | Loss: 0.277437 | Accuracy: 0.885000\n",
            "Iteration: 77 | Loss: 0.276698 | Accuracy: 0.885000\n",
            "Iteration: 78 | Loss: 0.275993 | Accuracy: 0.885000\n",
            "Iteration: 79 | Loss: 0.275291 | Accuracy: 0.885000\n",
            "Iteration: 80 | Loss: 0.274554 | Accuracy: 0.885000\n",
            "Iteration: 81 | Loss: 0.273760 | Accuracy: 0.885000\n",
            "Iteration: 82 | Loss: 0.272925 | Accuracy: 0.885000\n",
            "Iteration: 83 | Loss: 0.272076 | Accuracy: 0.885000\n",
            "Iteration: 84 | Loss: 0.271234 | Accuracy: 0.885000\n",
            "Iteration: 85 | Loss: 0.270395 | Accuracy: 0.885000\n",
            "Iteration: 86 | Loss: 0.269538 | Accuracy: 0.885000\n",
            "Iteration: 87 | Loss: 0.268644 | Accuracy: 0.885000\n",
            "Iteration: 88 | Loss: 0.267710 | Accuracy: 0.885000\n",
            "Iteration: 89 | Loss: 0.266750 | Accuracy: 0.885000\n",
            "Iteration: 90 | Loss: 0.265778 | Accuracy: 0.885000\n",
            "Iteration: 91 | Loss: 0.264800 | Accuracy: 0.885000\n",
            "Iteration: 92 | Loss: 0.263804 | Accuracy: 0.885000\n",
            "Iteration: 93 | Loss: 0.262777 | Accuracy: 0.885000\n",
            "Iteration: 94 | Loss: 0.261714 | Accuracy: 0.885000\n",
            "Iteration: 95 | Loss: 0.260620 | Accuracy: 0.885000\n",
            "Iteration: 96 | Loss: 0.259503 | Accuracy: 0.885000\n",
            "Iteration: 97 | Loss: 0.258369 | Accuracy: 0.885000\n",
            "Iteration: 98 | Loss: 0.257213 | Accuracy: 0.885000\n",
            "Iteration: 99 | Loss: 0.256028 | Accuracy: 0.885000\n",
            "Iteration: 100 | Loss: 0.254808 | Accuracy: 0.895000\n",
            "Iteration: 101 | Loss: 0.253557 | Accuracy: 0.895000\n",
            "Iteration: 102 | Loss: 0.252279 | Accuracy: 0.895000\n",
            "Iteration: 103 | Loss: 0.250978 | Accuracy: 0.895000\n",
            "Iteration: 104 | Loss: 0.249652 | Accuracy: 0.895000\n",
            "Iteration: 105 | Loss: 0.248295 | Accuracy: 0.895000\n",
            "Iteration: 106 | Loss: 0.246905 | Accuracy: 0.895000\n",
            "Iteration: 107 | Loss: 0.245484 | Accuracy: 0.895000\n",
            "Iteration: 108 | Loss: 0.244035 | Accuracy: 0.900000\n",
            "Iteration: 109 | Loss: 0.242561 | Accuracy: 0.900000\n",
            "Iteration: 110 | Loss: 0.241059 | Accuracy: 0.900000\n",
            "Iteration: 111 | Loss: 0.239528 | Accuracy: 0.910000\n",
            "Iteration: 112 | Loss: 0.237968 | Accuracy: 0.910000\n",
            "Iteration: 113 | Loss: 0.236379 | Accuracy: 0.915000\n",
            "Iteration: 114 | Loss: 0.234764 | Accuracy: 0.915000\n",
            "Iteration: 115 | Loss: 0.233126 | Accuracy: 0.915000\n",
            "Iteration: 116 | Loss: 0.231462 | Accuracy: 0.915000\n",
            "Iteration: 117 | Loss: 0.229774 | Accuracy: 0.915000\n",
            "Iteration: 118 | Loss: 0.228061 | Accuracy: 0.915000\n",
            "Iteration: 119 | Loss: 0.226326 | Accuracy: 0.915000\n",
            "Iteration: 120 | Loss: 0.224572 | Accuracy: 0.915000\n",
            "Iteration: 121 | Loss: 0.222799 | Accuracy: 0.920000\n",
            "Iteration: 122 | Loss: 0.221008 | Accuracy: 0.920000\n",
            "Iteration: 123 | Loss: 0.219201 | Accuracy: 0.920000\n",
            "Iteration: 124 | Loss: 0.217378 | Accuracy: 0.925000\n",
            "Iteration: 125 | Loss: 0.215544 | Accuracy: 0.925000\n",
            "Iteration: 126 | Loss: 0.213699 | Accuracy: 0.925000\n",
            "Iteration: 127 | Loss: 0.211845 | Accuracy: 0.930000\n",
            "Iteration: 128 | Loss: 0.209984 | Accuracy: 0.930000\n",
            "Iteration: 129 | Loss: 0.208118 | Accuracy: 0.930000\n",
            "Iteration: 130 | Loss: 0.206250 | Accuracy: 0.935000\n",
            "Iteration: 131 | Loss: 0.204381 | Accuracy: 0.935000\n",
            "Iteration: 132 | Loss: 0.202514 | Accuracy: 0.935000\n",
            "Iteration: 133 | Loss: 0.200651 | Accuracy: 0.935000\n",
            "Iteration: 134 | Loss: 0.198794 | Accuracy: 0.935000\n",
            "Iteration: 135 | Loss: 0.196945 | Accuracy: 0.935000\n",
            "Iteration: 136 | Loss: 0.195106 | Accuracy: 0.935000\n",
            "Iteration: 137 | Loss: 0.193280 | Accuracy: 0.935000\n",
            "Iteration: 138 | Loss: 0.191469 | Accuracy: 0.940000\n",
            "Iteration: 139 | Loss: 0.189673 | Accuracy: 0.940000\n",
            "Iteration: 140 | Loss: 0.187896 | Accuracy: 0.940000\n",
            "Iteration: 141 | Loss: 0.186140 | Accuracy: 0.940000\n",
            "Iteration: 142 | Loss: 0.184405 | Accuracy: 0.940000\n",
            "Iteration: 143 | Loss: 0.182693 | Accuracy: 0.940000\n",
            "Iteration: 144 | Loss: 0.181006 | Accuracy: 0.940000\n",
            "Iteration: 145 | Loss: 0.179345 | Accuracy: 0.940000\n",
            "Iteration: 146 | Loss: 0.177711 | Accuracy: 0.945000\n",
            "Iteration: 147 | Loss: 0.176106 | Accuracy: 0.945000\n",
            "Iteration: 148 | Loss: 0.174530 | Accuracy: 0.945000\n",
            "Iteration: 149 | Loss: 0.172984 | Accuracy: 0.945000\n",
            "Iteration: 150 | Loss: 0.171469 | Accuracy: 0.945000\n",
            "Iteration: 151 | Loss: 0.169984 | Accuracy: 0.945000\n",
            "Iteration: 152 | Loss: 0.168532 | Accuracy: 0.945000\n",
            "Iteration: 153 | Loss: 0.167111 | Accuracy: 0.945000\n",
            "Iteration: 154 | Loss: 0.165723 | Accuracy: 0.945000\n",
            "Iteration: 155 | Loss: 0.164366 | Accuracy: 0.945000\n",
            "Iteration: 156 | Loss: 0.163041 | Accuracy: 0.945000\n",
            "Iteration: 157 | Loss: 0.161748 | Accuracy: 0.945000\n",
            "Iteration: 158 | Loss: 0.160487 | Accuracy: 0.945000\n",
            "Iteration: 159 | Loss: 0.159257 | Accuracy: 0.945000\n",
            "Iteration: 160 | Loss: 0.158057 | Accuracy: 0.945000\n",
            "Iteration: 161 | Loss: 0.156888 | Accuracy: 0.950000\n",
            "Iteration: 162 | Loss: 0.155749 | Accuracy: 0.945000\n",
            "Iteration: 163 | Loss: 0.154638 | Accuracy: 0.945000\n",
            "Iteration: 164 | Loss: 0.153556 | Accuracy: 0.940000\n",
            "Iteration: 165 | Loss: 0.152502 | Accuracy: 0.940000\n",
            "Iteration: 166 | Loss: 0.151474 | Accuracy: 0.940000\n",
            "Iteration: 167 | Loss: 0.150473 | Accuracy: 0.945000\n",
            "Iteration: 168 | Loss: 0.149497 | Accuracy: 0.945000\n",
            "Iteration: 169 | Loss: 0.148545 | Accuracy: 0.945000\n",
            "Iteration: 170 | Loss: 0.147617 | Accuracy: 0.950000\n",
            "Iteration: 171 | Loss: 0.146712 | Accuracy: 0.950000\n",
            "Iteration: 172 | Loss: 0.145829 | Accuracy: 0.950000\n",
            "Iteration: 173 | Loss: 0.144966 | Accuracy: 0.950000\n",
            "Iteration: 174 | Loss: 0.144125 | Accuracy: 0.950000\n",
            "Iteration: 175 | Loss: 0.143302 | Accuracy: 0.950000\n",
            "Iteration: 176 | Loss: 0.142498 | Accuracy: 0.950000\n",
            "Iteration: 177 | Loss: 0.141712 | Accuracy: 0.955000\n",
            "Iteration: 178 | Loss: 0.140942 | Accuracy: 0.955000\n",
            "Iteration: 179 | Loss: 0.140189 | Accuracy: 0.955000\n",
            "Iteration: 180 | Loss: 0.139451 | Accuracy: 0.955000\n",
            "Iteration: 181 | Loss: 0.138727 | Accuracy: 0.955000\n",
            "Iteration: 182 | Loss: 0.138018 | Accuracy: 0.955000\n",
            "Iteration: 183 | Loss: 0.137321 | Accuracy: 0.955000\n",
            "Iteration: 184 | Loss: 0.136638 | Accuracy: 0.955000\n",
            "Iteration: 185 | Loss: 0.135965 | Accuracy: 0.960000\n",
            "Iteration: 186 | Loss: 0.135304 | Accuracy: 0.960000\n",
            "Iteration: 187 | Loss: 0.134654 | Accuracy: 0.960000\n",
            "Iteration: 188 | Loss: 0.134013 | Accuracy: 0.965000\n",
            "Iteration: 189 | Loss: 0.133381 | Accuracy: 0.965000\n",
            "Iteration: 190 | Loss: 0.132758 | Accuracy: 0.965000\n",
            "Iteration: 191 | Loss: 0.132144 | Accuracy: 0.965000\n",
            "Iteration: 192 | Loss: 0.131536 | Accuracy: 0.965000\n",
            "Iteration: 193 | Loss: 0.130936 | Accuracy: 0.965000\n",
            "Iteration: 194 | Loss: 0.130343 | Accuracy: 0.965000\n",
            "Iteration: 195 | Loss: 0.129755 | Accuracy: 0.965000\n",
            "Iteration: 196 | Loss: 0.129173 | Accuracy: 0.965000\n",
            "Iteration: 197 | Loss: 0.128595 | Accuracy: 0.965000\n",
            "Iteration: 198 | Loss: 0.128023 | Accuracy: 0.965000\n",
            "Iteration: 199 | Loss: 0.127454 | Accuracy: 0.965000\n",
            "Iteration: 200 | Loss: 0.126889 | Accuracy: 0.965000\n",
            "Iteration: 201 | Loss: 0.126327 | Accuracy: 0.965000\n",
            "Iteration: 202 | Loss: 0.125769 | Accuracy: 0.960000\n",
            "Iteration: 203 | Loss: 0.125212 | Accuracy: 0.965000\n",
            "Iteration: 204 | Loss: 0.124657 | Accuracy: 0.965000\n",
            "Iteration: 205 | Loss: 0.124104 | Accuracy: 0.965000\n",
            "Iteration: 206 | Loss: 0.123552 | Accuracy: 0.965000\n",
            "Iteration: 207 | Loss: 0.123001 | Accuracy: 0.965000\n",
            "Iteration: 208 | Loss: 0.122451 | Accuracy: 0.965000\n",
            "Iteration: 209 | Loss: 0.121900 | Accuracy: 0.965000\n",
            "Iteration: 210 | Loss: 0.121350 | Accuracy: 0.965000\n",
            "Iteration: 211 | Loss: 0.120798 | Accuracy: 0.965000\n",
            "Iteration: 212 | Loss: 0.120246 | Accuracy: 0.965000\n",
            "Iteration: 213 | Loss: 0.119693 | Accuracy: 0.965000\n",
            "Iteration: 214 | Loss: 0.119139 | Accuracy: 0.965000\n",
            "Iteration: 215 | Loss: 0.118582 | Accuracy: 0.965000\n",
            "Iteration: 216 | Loss: 0.118024 | Accuracy: 0.965000\n",
            "Iteration: 217 | Loss: 0.117464 | Accuracy: 0.965000\n",
            "Iteration: 218 | Loss: 0.116901 | Accuracy: 0.965000\n",
            "Iteration: 219 | Loss: 0.116336 | Accuracy: 0.965000\n",
            "Iteration: 220 | Loss: 0.115768 | Accuracy: 0.965000\n",
            "Iteration: 221 | Loss: 0.115197 | Accuracy: 0.965000\n",
            "Iteration: 222 | Loss: 0.114624 | Accuracy: 0.965000\n",
            "Iteration: 223 | Loss: 0.114047 | Accuracy: 0.965000\n",
            "Iteration: 224 | Loss: 0.113467 | Accuracy: 0.965000\n",
            "Iteration: 225 | Loss: 0.112884 | Accuracy: 0.965000\n",
            "Iteration: 226 | Loss: 0.112297 | Accuracy: 0.965000\n",
            "Iteration: 227 | Loss: 0.111708 | Accuracy: 0.965000\n",
            "Iteration: 228 | Loss: 0.111115 | Accuracy: 0.965000\n",
            "Iteration: 229 | Loss: 0.110518 | Accuracy: 0.965000\n",
            "Iteration: 230 | Loss: 0.109919 | Accuracy: 0.965000\n",
            "Iteration: 231 | Loss: 0.109316 | Accuracy: 0.965000\n",
            "Iteration: 232 | Loss: 0.108711 | Accuracy: 0.965000\n",
            "Iteration: 233 | Loss: 0.108103 | Accuracy: 0.965000\n",
            "Iteration: 234 | Loss: 0.107491 | Accuracy: 0.965000\n",
            "Iteration: 235 | Loss: 0.106877 | Accuracy: 0.965000\n",
            "Iteration: 236 | Loss: 0.106261 | Accuracy: 0.965000\n",
            "Iteration: 237 | Loss: 0.105643 | Accuracy: 0.965000\n",
            "Iteration: 238 | Loss: 0.105022 | Accuracy: 0.965000\n",
            "Iteration: 239 | Loss: 0.104399 | Accuracy: 0.965000\n",
            "Iteration: 240 | Loss: 0.103775 | Accuracy: 0.965000\n",
            "Iteration: 241 | Loss: 0.103149 | Accuracy: 0.965000\n",
            "Iteration: 242 | Loss: 0.102522 | Accuracy: 0.965000\n",
            "Iteration: 243 | Loss: 0.101893 | Accuracy: 0.965000\n",
            "Iteration: 244 | Loss: 0.101264 | Accuracy: 0.975000\n",
            "Iteration: 245 | Loss: 0.100635 | Accuracy: 0.975000\n",
            "Iteration: 246 | Loss: 0.100005 | Accuracy: 0.975000\n",
            "Iteration: 247 | Loss: 0.099375 | Accuracy: 0.975000\n",
            "Iteration: 248 | Loss: 0.098745 | Accuracy: 0.970000\n",
            "Iteration: 249 | Loss: 0.098116 | Accuracy: 0.970000\n",
            "Iteration: 250 | Loss: 0.097488 | Accuracy: 0.970000\n",
            "Iteration: 251 | Loss: 0.096860 | Accuracy: 0.970000\n",
            "Iteration: 252 | Loss: 0.096234 | Accuracy: 0.970000\n",
            "Iteration: 253 | Loss: 0.095609 | Accuracy: 0.970000\n",
            "Iteration: 254 | Loss: 0.094987 | Accuracy: 0.970000\n",
            "Iteration: 255 | Loss: 0.094366 | Accuracy: 0.970000\n",
            "Iteration: 256 | Loss: 0.093747 | Accuracy: 0.975000\n",
            "Iteration: 257 | Loss: 0.093131 | Accuracy: 0.975000\n",
            "Iteration: 258 | Loss: 0.092518 | Accuracy: 0.975000\n",
            "Iteration: 259 | Loss: 0.091908 | Accuracy: 0.975000\n",
            "Iteration: 260 | Loss: 0.091301 | Accuracy: 0.975000\n",
            "Iteration: 261 | Loss: 0.090698 | Accuracy: 0.975000\n",
            "Iteration: 262 | Loss: 0.090098 | Accuracy: 0.975000\n",
            "Iteration: 263 | Loss: 0.089503 | Accuracy: 0.975000\n",
            "Iteration: 264 | Loss: 0.088911 | Accuracy: 0.975000\n",
            "Iteration: 265 | Loss: 0.088324 | Accuracy: 0.975000\n",
            "Iteration: 266 | Loss: 0.087742 | Accuracy: 0.975000\n",
            "Iteration: 267 | Loss: 0.087164 | Accuracy: 0.975000\n",
            "Iteration: 268 | Loss: 0.086591 | Accuracy: 0.975000\n",
            "Iteration: 269 | Loss: 0.086024 | Accuracy: 0.975000\n",
            "Iteration: 270 | Loss: 0.085461 | Accuracy: 0.980000\n",
            "Iteration: 271 | Loss: 0.084904 | Accuracy: 0.980000\n",
            "Iteration: 272 | Loss: 0.084352 | Accuracy: 0.980000\n",
            "Iteration: 273 | Loss: 0.083806 | Accuracy: 0.980000\n",
            "Iteration: 274 | Loss: 0.083265 | Accuracy: 0.980000\n",
            "Iteration: 275 | Loss: 0.082731 | Accuracy: 0.980000\n",
            "Iteration: 276 | Loss: 0.082201 | Accuracy: 0.980000\n",
            "Iteration: 277 | Loss: 0.081678 | Accuracy: 0.980000\n",
            "Iteration: 278 | Loss: 0.081161 | Accuracy: 0.980000\n",
            "Iteration: 279 | Loss: 0.080649 | Accuracy: 0.980000\n",
            "Iteration: 280 | Loss: 0.080143 | Accuracy: 0.980000\n",
            "Iteration: 281 | Loss: 0.079643 | Accuracy: 0.980000\n",
            "Iteration: 282 | Loss: 0.079149 | Accuracy: 0.980000\n",
            "Iteration: 283 | Loss: 0.078661 | Accuracy: 0.980000\n",
            "Iteration: 284 | Loss: 0.078179 | Accuracy: 0.980000\n",
            "Iteration: 285 | Loss: 0.077703 | Accuracy: 0.980000\n",
            "Iteration: 286 | Loss: 0.077232 | Accuracy: 0.980000\n",
            "Iteration: 287 | Loss: 0.076767 | Accuracy: 0.980000\n",
            "Iteration: 288 | Loss: 0.076308 | Accuracy: 0.980000\n",
            "Iteration: 289 | Loss: 0.075854 | Accuracy: 0.985000\n",
            "Iteration: 290 | Loss: 0.075405 | Accuracy: 0.985000\n",
            "Iteration: 291 | Loss: 0.074963 | Accuracy: 0.985000\n",
            "Iteration: 292 | Loss: 0.074525 | Accuracy: 0.985000\n",
            "Iteration: 293 | Loss: 0.074093 | Accuracy: 0.990000\n",
            "Iteration: 294 | Loss: 0.073666 | Accuracy: 0.990000\n",
            "Iteration: 295 | Loss: 0.073244 | Accuracy: 0.990000\n",
            "Iteration: 296 | Loss: 0.072828 | Accuracy: 0.995000\n",
            "Iteration: 297 | Loss: 0.072416 | Accuracy: 0.995000\n",
            "Iteration: 298 | Loss: 0.072009 | Accuracy: 0.995000\n",
            "Iteration: 299 | Loss: 0.071607 | Accuracy: 0.995000\n",
            "Iteration: 300 | Loss: 0.071209 | Accuracy: 0.995000\n",
            "Iteration: 301 | Loss: 0.070816 | Accuracy: 0.995000\n",
            "Iteration: 302 | Loss: 0.070427 | Accuracy: 0.995000\n",
            "Iteration: 303 | Loss: 0.070043 | Accuracy: 0.995000\n",
            "Iteration: 304 | Loss: 0.069664 | Accuracy: 0.995000\n",
            "Iteration: 305 | Loss: 0.069288 | Accuracy: 0.995000\n",
            "Iteration: 306 | Loss: 0.068916 | Accuracy: 0.995000\n",
            "Iteration: 307 | Loss: 0.068549 | Accuracy: 0.995000\n",
            "Iteration: 308 | Loss: 0.068185 | Accuracy: 0.995000\n",
            "Iteration: 309 | Loss: 0.067826 | Accuracy: 0.995000\n",
            "Iteration: 310 | Loss: 0.067470 | Accuracy: 0.995000\n",
            "Iteration: 311 | Loss: 0.067117 | Accuracy: 0.995000\n",
            "Iteration: 312 | Loss: 0.066769 | Accuracy: 0.995000\n",
            "Iteration: 313 | Loss: 0.066424 | Accuracy: 0.995000\n",
            "Iteration: 314 | Loss: 0.066082 | Accuracy: 0.995000\n",
            "Iteration: 315 | Loss: 0.065744 | Accuracy: 0.995000\n",
            "Iteration: 316 | Loss: 0.065408 | Accuracy: 1.000000\n",
            "Iteration: 317 | Loss: 0.065077 | Accuracy: 1.000000\n",
            "Iteration: 318 | Loss: 0.064748 | Accuracy: 1.000000\n",
            "Iteration: 319 | Loss: 0.064423 | Accuracy: 1.000000\n",
            "Iteration: 320 | Loss: 0.064100 | Accuracy: 1.000000\n",
            "Iteration: 321 | Loss: 0.063781 | Accuracy: 1.000000\n",
            "Iteration: 322 | Loss: 0.063464 | Accuracy: 1.000000\n",
            "Iteration: 323 | Loss: 0.063150 | Accuracy: 1.000000\n",
            "Iteration: 324 | Loss: 0.062839 | Accuracy: 1.000000\n",
            "Iteration: 325 | Loss: 0.062531 | Accuracy: 1.000000\n",
            "Iteration: 326 | Loss: 0.062226 | Accuracy: 1.000000\n",
            "Iteration: 327 | Loss: 0.061923 | Accuracy: 1.000000\n",
            "Iteration: 328 | Loss: 0.061622 | Accuracy: 1.000000\n",
            "Iteration: 329 | Loss: 0.061325 | Accuracy: 1.000000\n",
            "Iteration: 330 | Loss: 0.061029 | Accuracy: 1.000000\n",
            "Iteration: 331 | Loss: 0.060737 | Accuracy: 1.000000\n",
            "Iteration: 332 | Loss: 0.060446 | Accuracy: 1.000000\n",
            "Iteration: 333 | Loss: 0.060158 | Accuracy: 1.000000\n",
            "Iteration: 334 | Loss: 0.059872 | Accuracy: 1.000000\n",
            "Iteration: 335 | Loss: 0.059589 | Accuracy: 1.000000\n",
            "Iteration: 336 | Loss: 0.059308 | Accuracy: 1.000000\n",
            "Iteration: 337 | Loss: 0.059029 | Accuracy: 1.000000\n",
            "Iteration: 338 | Loss: 0.058752 | Accuracy: 1.000000\n",
            "Iteration: 339 | Loss: 0.058477 | Accuracy: 1.000000\n",
            "Iteration: 340 | Loss: 0.058204 | Accuracy: 1.000000\n",
            "Iteration: 341 | Loss: 0.057934 | Accuracy: 1.000000\n",
            "Iteration: 342 | Loss: 0.057665 | Accuracy: 1.000000\n",
            "Iteration: 343 | Loss: 0.057398 | Accuracy: 1.000000\n",
            "Iteration: 344 | Loss: 0.057134 | Accuracy: 1.000000\n",
            "Iteration: 345 | Loss: 0.056871 | Accuracy: 1.000000\n",
            "Iteration: 346 | Loss: 0.056610 | Accuracy: 1.000000\n",
            "Iteration: 347 | Loss: 0.056351 | Accuracy: 1.000000\n",
            "Iteration: 348 | Loss: 0.056094 | Accuracy: 1.000000\n",
            "Iteration: 349 | Loss: 0.055838 | Accuracy: 1.000000\n",
            "Iteration: 350 | Loss: 0.055585 | Accuracy: 1.000000\n",
            "Iteration: 351 | Loss: 0.055333 | Accuracy: 1.000000\n",
            "Iteration: 352 | Loss: 0.055083 | Accuracy: 1.000000\n",
            "Iteration: 353 | Loss: 0.054834 | Accuracy: 1.000000\n",
            "Iteration: 354 | Loss: 0.054588 | Accuracy: 1.000000\n",
            "Iteration: 355 | Loss: 0.054343 | Accuracy: 1.000000\n",
            "Iteration: 356 | Loss: 0.054099 | Accuracy: 1.000000\n",
            "Iteration: 357 | Loss: 0.053857 | Accuracy: 1.000000\n",
            "Iteration: 358 | Loss: 0.053617 | Accuracy: 0.995000\n",
            "Iteration: 359 | Loss: 0.053378 | Accuracy: 0.995000\n",
            "Iteration: 360 | Loss: 0.053141 | Accuracy: 0.995000\n",
            "Iteration: 361 | Loss: 0.052905 | Accuracy: 0.995000\n",
            "Iteration: 362 | Loss: 0.052671 | Accuracy: 0.995000\n",
            "Iteration: 363 | Loss: 0.052438 | Accuracy: 0.995000\n",
            "Iteration: 364 | Loss: 0.052207 | Accuracy: 0.995000\n",
            "Iteration: 365 | Loss: 0.051977 | Accuracy: 0.995000\n",
            "Iteration: 366 | Loss: 0.051749 | Accuracy: 0.995000\n",
            "Iteration: 367 | Loss: 0.051522 | Accuracy: 0.995000\n",
            "Iteration: 368 | Loss: 0.051296 | Accuracy: 0.995000\n",
            "Iteration: 369 | Loss: 0.051072 | Accuracy: 0.995000\n",
            "Iteration: 370 | Loss: 0.050849 | Accuracy: 0.995000\n",
            "Iteration: 371 | Loss: 0.050628 | Accuracy: 0.995000\n",
            "Iteration: 372 | Loss: 0.050407 | Accuracy: 0.995000\n",
            "Iteration: 373 | Loss: 0.050189 | Accuracy: 0.995000\n",
            "Iteration: 374 | Loss: 0.049971 | Accuracy: 0.995000\n",
            "Iteration: 375 | Loss: 0.049755 | Accuracy: 0.995000\n",
            "Iteration: 376 | Loss: 0.049540 | Accuracy: 0.995000\n",
            "Iteration: 377 | Loss: 0.049326 | Accuracy: 0.995000\n",
            "Iteration: 378 | Loss: 0.049113 | Accuracy: 0.995000\n",
            "Iteration: 379 | Loss: 0.048902 | Accuracy: 0.995000\n",
            "Iteration: 380 | Loss: 0.048692 | Accuracy: 0.995000\n",
            "Iteration: 381 | Loss: 0.048483 | Accuracy: 0.995000\n",
            "Iteration: 382 | Loss: 0.048275 | Accuracy: 0.995000\n",
            "Iteration: 383 | Loss: 0.048068 | Accuracy: 0.995000\n",
            "Iteration: 384 | Loss: 0.047863 | Accuracy: 0.995000\n",
            "Iteration: 385 | Loss: 0.047659 | Accuracy: 0.995000\n",
            "Iteration: 386 | Loss: 0.047456 | Accuracy: 0.995000\n",
            "Iteration: 387 | Loss: 0.047254 | Accuracy: 0.995000\n",
            "Iteration: 388 | Loss: 0.047053 | Accuracy: 0.995000\n",
            "Iteration: 389 | Loss: 0.046853 | Accuracy: 0.995000\n",
            "Iteration: 390 | Loss: 0.046655 | Accuracy: 0.995000\n",
            "Iteration: 391 | Loss: 0.046457 | Accuracy: 1.000000\n",
            "Iteration: 392 | Loss: 0.046261 | Accuracy: 1.000000\n",
            "Iteration: 393 | Loss: 0.046065 | Accuracy: 1.000000\n",
            "Iteration: 394 | Loss: 0.045871 | Accuracy: 1.000000\n",
            "Iteration: 395 | Loss: 0.045678 | Accuracy: 1.000000\n",
            "Iteration: 396 | Loss: 0.045486 | Accuracy: 1.000000\n",
            "Iteration: 397 | Loss: 0.045295 | Accuracy: 1.000000\n",
            "Iteration: 398 | Loss: 0.045105 | Accuracy: 1.000000\n",
            "Iteration: 399 | Loss: 0.044916 | Accuracy: 1.000000\n",
            "Iteration: 400 | Loss: 0.044727 | Accuracy: 1.000000\n",
            "Iteration: 401 | Loss: 0.044540 | Accuracy: 1.000000\n",
            "Iteration: 402 | Loss: 0.044354 | Accuracy: 1.000000\n",
            "Iteration: 403 | Loss: 0.044169 | Accuracy: 1.000000\n",
            "Iteration: 404 | Loss: 0.043985 | Accuracy: 1.000000\n",
            "Iteration: 405 | Loss: 0.043802 | Accuracy: 1.000000\n",
            "Iteration: 406 | Loss: 0.043620 | Accuracy: 1.000000\n",
            "Iteration: 407 | Loss: 0.043439 | Accuracy: 1.000000\n",
            "Iteration: 408 | Loss: 0.043259 | Accuracy: 1.000000\n",
            "Iteration: 409 | Loss: 0.043080 | Accuracy: 1.000000\n",
            "Iteration: 410 | Loss: 0.042902 | Accuracy: 1.000000\n",
            "Iteration: 411 | Loss: 0.042724 | Accuracy: 1.000000\n",
            "Iteration: 412 | Loss: 0.042548 | Accuracy: 1.000000\n",
            "Iteration: 413 | Loss: 0.042373 | Accuracy: 1.000000\n",
            "Iteration: 414 | Loss: 0.042198 | Accuracy: 1.000000\n",
            "Iteration: 415 | Loss: 0.042025 | Accuracy: 1.000000\n",
            "Iteration: 416 | Loss: 0.041852 | Accuracy: 1.000000\n",
            "Iteration: 417 | Loss: 0.041680 | Accuracy: 1.000000\n",
            "Iteration: 418 | Loss: 0.041509 | Accuracy: 1.000000\n",
            "Iteration: 419 | Loss: 0.041340 | Accuracy: 1.000000\n",
            "Iteration: 420 | Loss: 0.041171 | Accuracy: 1.000000\n",
            "Iteration: 421 | Loss: 0.041002 | Accuracy: 1.000000\n",
            "Iteration: 422 | Loss: 0.040835 | Accuracy: 1.000000\n",
            "Iteration: 423 | Loss: 0.040669 | Accuracy: 1.000000\n",
            "Iteration: 424 | Loss: 0.040503 | Accuracy: 1.000000\n",
            "Iteration: 425 | Loss: 0.040339 | Accuracy: 1.000000\n",
            "Iteration: 426 | Loss: 0.040175 | Accuracy: 1.000000\n",
            "Iteration: 427 | Loss: 0.040012 | Accuracy: 1.000000\n",
            "Iteration: 428 | Loss: 0.039850 | Accuracy: 1.000000\n",
            "Iteration: 429 | Loss: 0.039689 | Accuracy: 1.000000\n",
            "Iteration: 430 | Loss: 0.039528 | Accuracy: 1.000000\n",
            "Iteration: 431 | Loss: 0.039369 | Accuracy: 1.000000\n",
            "Iteration: 432 | Loss: 0.039210 | Accuracy: 1.000000\n",
            "Iteration: 433 | Loss: 0.039052 | Accuracy: 1.000000\n",
            "Iteration: 434 | Loss: 0.038895 | Accuracy: 1.000000\n",
            "Iteration: 435 | Loss: 0.038739 | Accuracy: 1.000000\n",
            "Iteration: 436 | Loss: 0.038584 | Accuracy: 1.000000\n",
            "Iteration: 437 | Loss: 0.038429 | Accuracy: 1.000000\n",
            "Iteration: 438 | Loss: 0.038276 | Accuracy: 1.000000\n",
            "Iteration: 439 | Loss: 0.038123 | Accuracy: 1.000000\n",
            "Iteration: 440 | Loss: 0.037971 | Accuracy: 1.000000\n",
            "Iteration: 441 | Loss: 0.037819 | Accuracy: 1.000000\n",
            "Iteration: 442 | Loss: 0.037669 | Accuracy: 1.000000\n",
            "Iteration: 443 | Loss: 0.037519 | Accuracy: 1.000000\n",
            "Iteration: 444 | Loss: 0.037370 | Accuracy: 1.000000\n",
            "Iteration: 445 | Loss: 0.037222 | Accuracy: 1.000000\n",
            "Iteration: 446 | Loss: 0.037075 | Accuracy: 1.000000\n",
            "Iteration: 447 | Loss: 0.036928 | Accuracy: 1.000000\n",
            "Iteration: 448 | Loss: 0.036782 | Accuracy: 1.000000\n",
            "Iteration: 449 | Loss: 0.036637 | Accuracy: 1.000000\n",
            "Iteration: 450 | Loss: 0.036493 | Accuracy: 1.000000\n",
            "Iteration: 451 | Loss: 0.036349 | Accuracy: 1.000000\n",
            "Iteration: 452 | Loss: 0.036207 | Accuracy: 1.000000\n",
            "Iteration: 453 | Loss: 0.036065 | Accuracy: 1.000000\n",
            "Iteration: 454 | Loss: 0.035923 | Accuracy: 1.000000\n",
            "Iteration: 455 | Loss: 0.035783 | Accuracy: 1.000000\n",
            "Iteration: 456 | Loss: 0.035643 | Accuracy: 1.000000\n",
            "Iteration: 457 | Loss: 0.035504 | Accuracy: 1.000000\n",
            "Iteration: 458 | Loss: 0.035366 | Accuracy: 1.000000\n",
            "Iteration: 459 | Loss: 0.035228 | Accuracy: 1.000000\n",
            "Iteration: 460 | Loss: 0.035091 | Accuracy: 1.000000\n",
            "Iteration: 461 | Loss: 0.034955 | Accuracy: 1.000000\n",
            "Iteration: 462 | Loss: 0.034820 | Accuracy: 1.000000\n",
            "Iteration: 463 | Loss: 0.034685 | Accuracy: 1.000000\n",
            "Iteration: 464 | Loss: 0.034551 | Accuracy: 1.000000\n",
            "Iteration: 465 | Loss: 0.034418 | Accuracy: 1.000000\n",
            "Iteration: 466 | Loss: 0.034286 | Accuracy: 1.000000\n",
            "Iteration: 467 | Loss: 0.034154 | Accuracy: 1.000000\n",
            "Iteration: 468 | Loss: 0.034023 | Accuracy: 1.000000\n",
            "Iteration: 469 | Loss: 0.033892 | Accuracy: 1.000000\n",
            "Iteration: 470 | Loss: 0.033763 | Accuracy: 1.000000\n",
            "Iteration: 471 | Loss: 0.033634 | Accuracy: 1.000000\n",
            "Iteration: 472 | Loss: 0.033505 | Accuracy: 1.000000\n",
            "Iteration: 473 | Loss: 0.033378 | Accuracy: 1.000000\n",
            "Iteration: 474 | Loss: 0.033251 | Accuracy: 1.000000\n",
            "Iteration: 475 | Loss: 0.033124 | Accuracy: 1.000000\n",
            "Iteration: 476 | Loss: 0.032999 | Accuracy: 1.000000\n",
            "Iteration: 477 | Loss: 0.032874 | Accuracy: 1.000000\n",
            "Iteration: 478 | Loss: 0.032750 | Accuracy: 1.000000\n",
            "Iteration: 479 | Loss: 0.032626 | Accuracy: 1.000000\n",
            "Iteration: 480 | Loss: 0.032503 | Accuracy: 1.000000\n",
            "Iteration: 481 | Loss: 0.032381 | Accuracy: 1.000000\n",
            "Iteration: 482 | Loss: 0.032259 | Accuracy: 1.000000\n",
            "Iteration: 483 | Loss: 0.032138 | Accuracy: 1.000000\n",
            "Iteration: 484 | Loss: 0.032018 | Accuracy: 1.000000\n",
            "Iteration: 485 | Loss: 0.031898 | Accuracy: 1.000000\n",
            "Iteration: 486 | Loss: 0.031779 | Accuracy: 1.000000\n",
            "Iteration: 487 | Loss: 0.031661 | Accuracy: 1.000000\n",
            "Iteration: 488 | Loss: 0.031543 | Accuracy: 1.000000\n",
            "Iteration: 489 | Loss: 0.031426 | Accuracy: 1.000000\n",
            "Iteration: 490 | Loss: 0.031309 | Accuracy: 1.000000\n",
            "Iteration: 491 | Loss: 0.031193 | Accuracy: 1.000000\n",
            "Iteration: 492 | Loss: 0.031078 | Accuracy: 1.000000\n",
            "Iteration: 493 | Loss: 0.030963 | Accuracy: 1.000000\n",
            "Iteration: 494 | Loss: 0.030849 | Accuracy: 1.000000\n",
            "Iteration: 495 | Loss: 0.030736 | Accuracy: 1.000000\n",
            "Iteration: 496 | Loss: 0.030623 | Accuracy: 1.000000\n",
            "Iteration: 497 | Loss: 0.030511 | Accuracy: 1.000000\n",
            "Iteration: 498 | Loss: 0.030400 | Accuracy: 1.000000\n",
            "Iteration: 499 | Loss: 0.030289 | Accuracy: 1.000000\n",
            "Iteration: 500 | Loss: 0.030178 | Accuracy: 1.000000\n",
            "Iteration: 501 | Loss: 0.030068 | Accuracy: 1.000000\n",
            "Iteration: 502 | Loss: 0.029959 | Accuracy: 1.000000\n",
            "Iteration: 503 | Loss: 0.029851 | Accuracy: 1.000000\n",
            "Iteration: 504 | Loss: 0.029743 | Accuracy: 1.000000\n",
            "Iteration: 505 | Loss: 0.029635 | Accuracy: 1.000000\n",
            "Iteration: 506 | Loss: 0.029528 | Accuracy: 1.000000\n",
            "Iteration: 507 | Loss: 0.029422 | Accuracy: 1.000000\n",
            "Iteration: 508 | Loss: 0.029316 | Accuracy: 1.000000\n",
            "Iteration: 509 | Loss: 0.029211 | Accuracy: 1.000000\n",
            "Iteration: 510 | Loss: 0.029107 | Accuracy: 1.000000\n",
            "Iteration: 511 | Loss: 0.029003 | Accuracy: 1.000000\n",
            "Iteration: 512 | Loss: 0.028899 | Accuracy: 1.000000\n",
            "Iteration: 513 | Loss: 0.028796 | Accuracy: 1.000000\n",
            "Iteration: 514 | Loss: 0.028694 | Accuracy: 1.000000\n",
            "Iteration: 515 | Loss: 0.028592 | Accuracy: 1.000000\n",
            "Iteration: 516 | Loss: 0.028491 | Accuracy: 1.000000\n",
            "Iteration: 517 | Loss: 0.028390 | Accuracy: 1.000000\n",
            "Iteration: 518 | Loss: 0.028290 | Accuracy: 1.000000\n",
            "Iteration: 519 | Loss: 0.028191 | Accuracy: 1.000000\n",
            "Iteration: 520 | Loss: 0.028091 | Accuracy: 1.000000\n",
            "Iteration: 521 | Loss: 0.027993 | Accuracy: 1.000000\n",
            "Iteration: 522 | Loss: 0.027895 | Accuracy: 1.000000\n",
            "Iteration: 523 | Loss: 0.027797 | Accuracy: 1.000000\n",
            "Iteration: 524 | Loss: 0.027701 | Accuracy: 1.000000\n",
            "Iteration: 525 | Loss: 0.027604 | Accuracy: 1.000000\n",
            "Iteration: 526 | Loss: 0.027508 | Accuracy: 1.000000\n",
            "Iteration: 527 | Loss: 0.027413 | Accuracy: 1.000000\n",
            "Iteration: 528 | Loss: 0.027318 | Accuracy: 1.000000\n",
            "Iteration: 529 | Loss: 0.027224 | Accuracy: 1.000000\n",
            "Iteration: 530 | Loss: 0.027130 | Accuracy: 1.000000\n",
            "Iteration: 531 | Loss: 0.027036 | Accuracy: 1.000000\n",
            "Iteration: 532 | Loss: 0.026943 | Accuracy: 1.000000\n",
            "Iteration: 533 | Loss: 0.026851 | Accuracy: 1.000000\n",
            "Iteration: 534 | Loss: 0.026759 | Accuracy: 1.000000\n",
            "Iteration: 535 | Loss: 0.026668 | Accuracy: 1.000000\n",
            "Iteration: 536 | Loss: 0.026577 | Accuracy: 1.000000\n",
            "Iteration: 537 | Loss: 0.026487 | Accuracy: 1.000000\n",
            "Iteration: 538 | Loss: 0.026397 | Accuracy: 1.000000\n",
            "Iteration: 539 | Loss: 0.026307 | Accuracy: 1.000000\n",
            "Iteration: 540 | Loss: 0.026218 | Accuracy: 1.000000\n",
            "Iteration: 541 | Loss: 0.026130 | Accuracy: 1.000000\n",
            "Iteration: 542 | Loss: 0.026042 | Accuracy: 1.000000\n",
            "Iteration: 543 | Loss: 0.025954 | Accuracy: 1.000000\n",
            "Iteration: 544 | Loss: 0.025867 | Accuracy: 1.000000\n",
            "Iteration: 545 | Loss: 0.025781 | Accuracy: 1.000000\n",
            "Iteration: 546 | Loss: 0.025695 | Accuracy: 1.000000\n",
            "Iteration: 547 | Loss: 0.025609 | Accuracy: 1.000000\n",
            "Iteration: 548 | Loss: 0.025524 | Accuracy: 1.000000\n",
            "Iteration: 549 | Loss: 0.025439 | Accuracy: 1.000000\n",
            "Iteration: 550 | Loss: 0.025355 | Accuracy: 1.000000\n",
            "Iteration: 551 | Loss: 0.025271 | Accuracy: 1.000000\n",
            "Iteration: 552 | Loss: 0.025187 | Accuracy: 1.000000\n",
            "Iteration: 553 | Loss: 0.025104 | Accuracy: 1.000000\n",
            "Iteration: 554 | Loss: 0.025022 | Accuracy: 1.000000\n",
            "Iteration: 555 | Loss: 0.024940 | Accuracy: 1.000000\n",
            "Iteration: 556 | Loss: 0.024858 | Accuracy: 1.000000\n",
            "Iteration: 557 | Loss: 0.024777 | Accuracy: 1.000000\n",
            "Iteration: 558 | Loss: 0.024696 | Accuracy: 1.000000\n",
            "Iteration: 559 | Loss: 0.024616 | Accuracy: 1.000000\n",
            "Iteration: 560 | Loss: 0.024536 | Accuracy: 1.000000\n",
            "Iteration: 561 | Loss: 0.024456 | Accuracy: 1.000000\n",
            "Iteration: 562 | Loss: 0.024377 | Accuracy: 1.000000\n",
            "Iteration: 563 | Loss: 0.024299 | Accuracy: 1.000000\n",
            "Iteration: 564 | Loss: 0.024220 | Accuracy: 1.000000\n",
            "Iteration: 565 | Loss: 0.024142 | Accuracy: 1.000000\n",
            "Iteration: 566 | Loss: 0.024065 | Accuracy: 1.000000\n",
            "Iteration: 567 | Loss: 0.023988 | Accuracy: 1.000000\n",
            "Iteration: 568 | Loss: 0.023911 | Accuracy: 1.000000\n",
            "Iteration: 569 | Loss: 0.023835 | Accuracy: 1.000000\n",
            "Iteration: 570 | Loss: 0.023759 | Accuracy: 1.000000\n",
            "Iteration: 571 | Loss: 0.023684 | Accuracy: 1.000000\n",
            "Iteration: 572 | Loss: 0.023609 | Accuracy: 1.000000\n",
            "Iteration: 573 | Loss: 0.023534 | Accuracy: 1.000000\n",
            "Iteration: 574 | Loss: 0.023460 | Accuracy: 1.000000\n",
            "Iteration: 575 | Loss: 0.023386 | Accuracy: 1.000000\n",
            "Iteration: 576 | Loss: 0.023313 | Accuracy: 1.000000\n",
            "Iteration: 577 | Loss: 0.023240 | Accuracy: 1.000000\n",
            "Iteration: 578 | Loss: 0.023167 | Accuracy: 1.000000\n",
            "Iteration: 579 | Loss: 0.023095 | Accuracy: 1.000000\n",
            "Iteration: 580 | Loss: 0.023023 | Accuracy: 1.000000\n",
            "Iteration: 581 | Loss: 0.022951 | Accuracy: 1.000000\n",
            "Iteration: 582 | Loss: 0.022880 | Accuracy: 1.000000\n",
            "Iteration: 583 | Loss: 0.022809 | Accuracy: 1.000000\n",
            "Iteration: 584 | Loss: 0.022739 | Accuracy: 1.000000\n",
            "Iteration: 585 | Loss: 0.022669 | Accuracy: 1.000000\n",
            "Iteration: 586 | Loss: 0.022599 | Accuracy: 1.000000\n",
            "Iteration: 587 | Loss: 0.022529 | Accuracy: 1.000000\n",
            "Iteration: 588 | Loss: 0.022460 | Accuracy: 1.000000\n",
            "Iteration: 589 | Loss: 0.022392 | Accuracy: 1.000000\n",
            "Iteration: 590 | Loss: 0.022323 | Accuracy: 1.000000\n",
            "Iteration: 591 | Loss: 0.022255 | Accuracy: 1.000000\n",
            "Iteration: 592 | Loss: 0.022188 | Accuracy: 1.000000\n",
            "Iteration: 593 | Loss: 0.022121 | Accuracy: 1.000000\n",
            "Iteration: 594 | Loss: 0.022054 | Accuracy: 1.000000\n",
            "Iteration: 595 | Loss: 0.021987 | Accuracy: 1.000000\n",
            "Iteration: 596 | Loss: 0.021921 | Accuracy: 1.000000\n",
            "Iteration: 597 | Loss: 0.021855 | Accuracy: 1.000000\n",
            "Iteration: 598 | Loss: 0.021790 | Accuracy: 1.000000\n",
            "Iteration: 599 | Loss: 0.021724 | Accuracy: 1.000000\n",
            "Iteration: 600 | Loss: 0.021659 | Accuracy: 1.000000\n",
            "Iteration: 601 | Loss: 0.021595 | Accuracy: 1.000000\n",
            "Iteration: 602 | Loss: 0.021531 | Accuracy: 1.000000\n",
            "Iteration: 603 | Loss: 0.021467 | Accuracy: 1.000000\n",
            "Iteration: 604 | Loss: 0.021403 | Accuracy: 1.000000\n",
            "Iteration: 605 | Loss: 0.021340 | Accuracy: 1.000000\n",
            "Iteration: 606 | Loss: 0.021277 | Accuracy: 1.000000\n",
            "Iteration: 607 | Loss: 0.021215 | Accuracy: 1.000000\n",
            "Iteration: 608 | Loss: 0.021152 | Accuracy: 1.000000\n",
            "Iteration: 609 | Loss: 0.021090 | Accuracy: 1.000000\n",
            "Iteration: 610 | Loss: 0.021029 | Accuracy: 1.000000\n",
            "Iteration: 611 | Loss: 0.020967 | Accuracy: 1.000000\n",
            "Iteration: 612 | Loss: 0.020906 | Accuracy: 1.000000\n",
            "Iteration: 613 | Loss: 0.020845 | Accuracy: 1.000000\n",
            "Iteration: 614 | Loss: 0.020785 | Accuracy: 1.000000\n",
            "Iteration: 615 | Loss: 0.020725 | Accuracy: 1.000000\n",
            "Iteration: 616 | Loss: 0.020665 | Accuracy: 1.000000\n",
            "Iteration: 617 | Loss: 0.020606 | Accuracy: 1.000000\n",
            "Iteration: 618 | Loss: 0.020546 | Accuracy: 1.000000\n",
            "Iteration: 619 | Loss: 0.020488 | Accuracy: 1.000000\n",
            "Iteration: 620 | Loss: 0.020429 | Accuracy: 1.000000\n",
            "Iteration: 621 | Loss: 0.020371 | Accuracy: 1.000000\n",
            "Iteration: 622 | Loss: 0.020313 | Accuracy: 1.000000\n",
            "Iteration: 623 | Loss: 0.020255 | Accuracy: 1.000000\n",
            "Iteration: 624 | Loss: 0.020197 | Accuracy: 1.000000\n",
            "Iteration: 625 | Loss: 0.020140 | Accuracy: 1.000000\n",
            "Iteration: 626 | Loss: 0.020083 | Accuracy: 1.000000\n",
            "Iteration: 627 | Loss: 0.020027 | Accuracy: 1.000000\n",
            "Iteration: 628 | Loss: 0.019970 | Accuracy: 1.000000\n",
            "Iteration: 629 | Loss: 0.019914 | Accuracy: 1.000000\n",
            "Iteration: 630 | Loss: 0.019858 | Accuracy: 1.000000\n",
            "Iteration: 631 | Loss: 0.019803 | Accuracy: 1.000000\n",
            "Iteration: 632 | Loss: 0.019748 | Accuracy: 1.000000\n",
            "Iteration: 633 | Loss: 0.019693 | Accuracy: 1.000000\n",
            "Iteration: 634 | Loss: 0.019638 | Accuracy: 1.000000\n",
            "Iteration: 635 | Loss: 0.019584 | Accuracy: 1.000000\n",
            "Iteration: 636 | Loss: 0.019529 | Accuracy: 1.000000\n",
            "Iteration: 637 | Loss: 0.019476 | Accuracy: 1.000000\n",
            "Iteration: 638 | Loss: 0.019422 | Accuracy: 1.000000\n",
            "Iteration: 639 | Loss: 0.019369 | Accuracy: 1.000000\n",
            "Iteration: 640 | Loss: 0.019315 | Accuracy: 1.000000\n",
            "Iteration: 641 | Loss: 0.019263 | Accuracy: 1.000000\n",
            "Iteration: 642 | Loss: 0.019210 | Accuracy: 1.000000\n",
            "Iteration: 643 | Loss: 0.019158 | Accuracy: 1.000000\n",
            "Iteration: 644 | Loss: 0.019106 | Accuracy: 1.000000\n",
            "Iteration: 645 | Loss: 0.019054 | Accuracy: 1.000000\n",
            "Iteration: 646 | Loss: 0.019002 | Accuracy: 1.000000\n",
            "Iteration: 647 | Loss: 0.018951 | Accuracy: 1.000000\n",
            "Iteration: 648 | Loss: 0.018900 | Accuracy: 1.000000\n",
            "Iteration: 649 | Loss: 0.018849 | Accuracy: 1.000000\n",
            "Iteration: 650 | Loss: 0.018798 | Accuracy: 1.000000\n",
            "Iteration: 651 | Loss: 0.018748 | Accuracy: 1.000000\n",
            "Iteration: 652 | Loss: 0.018698 | Accuracy: 1.000000\n",
            "Iteration: 653 | Loss: 0.018648 | Accuracy: 1.000000\n",
            "Iteration: 654 | Loss: 0.018598 | Accuracy: 1.000000\n",
            "Iteration: 655 | Loss: 0.018549 | Accuracy: 1.000000\n",
            "Iteration: 656 | Loss: 0.018500 | Accuracy: 1.000000\n",
            "Iteration: 657 | Loss: 0.018451 | Accuracy: 1.000000\n",
            "Iteration: 658 | Loss: 0.018402 | Accuracy: 1.000000\n",
            "Iteration: 659 | Loss: 0.018354 | Accuracy: 1.000000\n",
            "Iteration: 660 | Loss: 0.018305 | Accuracy: 1.000000\n",
            "Iteration: 661 | Loss: 0.018257 | Accuracy: 1.000000\n",
            "Iteration: 662 | Loss: 0.018210 | Accuracy: 1.000000\n",
            "Iteration: 663 | Loss: 0.018162 | Accuracy: 1.000000\n",
            "Iteration: 664 | Loss: 0.018115 | Accuracy: 1.000000\n",
            "Iteration: 665 | Loss: 0.018068 | Accuracy: 1.000000\n",
            "Iteration: 666 | Loss: 0.018021 | Accuracy: 1.000000\n",
            "Iteration: 667 | Loss: 0.017974 | Accuracy: 1.000000\n",
            "Iteration: 668 | Loss: 0.017928 | Accuracy: 1.000000\n",
            "Iteration: 669 | Loss: 0.017881 | Accuracy: 1.000000\n",
            "Iteration: 670 | Loss: 0.017835 | Accuracy: 1.000000\n",
            "Iteration: 671 | Loss: 0.017790 | Accuracy: 1.000000\n",
            "Iteration: 672 | Loss: 0.017744 | Accuracy: 1.000000\n",
            "Iteration: 673 | Loss: 0.017699 | Accuracy: 1.000000\n",
            "Iteration: 674 | Loss: 0.017654 | Accuracy: 1.000000\n",
            "Iteration: 675 | Loss: 0.017609 | Accuracy: 1.000000\n",
            "Iteration: 676 | Loss: 0.017564 | Accuracy: 1.000000\n",
            "Iteration: 677 | Loss: 0.017519 | Accuracy: 1.000000\n",
            "Iteration: 678 | Loss: 0.017475 | Accuracy: 1.000000\n",
            "Iteration: 679 | Loss: 0.017431 | Accuracy: 1.000000\n",
            "Iteration: 680 | Loss: 0.017387 | Accuracy: 1.000000\n",
            "Iteration: 681 | Loss: 0.017343 | Accuracy: 1.000000\n",
            "Iteration: 682 | Loss: 0.017300 | Accuracy: 1.000000\n",
            "Iteration: 683 | Loss: 0.017256 | Accuracy: 1.000000\n",
            "Iteration: 684 | Loss: 0.017213 | Accuracy: 1.000000\n",
            "Iteration: 685 | Loss: 0.017170 | Accuracy: 1.000000\n",
            "Iteration: 686 | Loss: 0.017127 | Accuracy: 1.000000\n",
            "Iteration: 687 | Loss: 0.017085 | Accuracy: 1.000000\n",
            "Iteration: 688 | Loss: 0.017043 | Accuracy: 1.000000\n",
            "Iteration: 689 | Loss: 0.017000 | Accuracy: 1.000000\n",
            "Iteration: 690 | Loss: 0.016958 | Accuracy: 1.000000\n",
            "Iteration: 691 | Loss: 0.016917 | Accuracy: 1.000000\n",
            "Iteration: 692 | Loss: 0.016875 | Accuracy: 1.000000\n",
            "Iteration: 693 | Loss: 0.016834 | Accuracy: 1.000000\n",
            "Iteration: 694 | Loss: 0.016792 | Accuracy: 1.000000\n",
            "Iteration: 695 | Loss: 0.016751 | Accuracy: 1.000000\n",
            "Iteration: 696 | Loss: 0.016711 | Accuracy: 1.000000\n",
            "Iteration: 697 | Loss: 0.016670 | Accuracy: 1.000000\n",
            "Iteration: 698 | Loss: 0.016629 | Accuracy: 1.000000\n",
            "Iteration: 699 | Loss: 0.016589 | Accuracy: 1.000000\n",
            "Iteration: 700 | Loss: 0.016549 | Accuracy: 1.000000\n",
            "Iteration: 701 | Loss: 0.016509 | Accuracy: 1.000000\n",
            "Iteration: 702 | Loss: 0.016469 | Accuracy: 1.000000\n",
            "Iteration: 703 | Loss: 0.016430 | Accuracy: 1.000000\n",
            "Iteration: 704 | Loss: 0.016390 | Accuracy: 1.000000\n",
            "Iteration: 705 | Loss: 0.016351 | Accuracy: 1.000000\n",
            "Iteration: 706 | Loss: 0.016312 | Accuracy: 1.000000\n",
            "Iteration: 707 | Loss: 0.016273 | Accuracy: 1.000000\n",
            "Iteration: 708 | Loss: 0.016234 | Accuracy: 1.000000\n",
            "Iteration: 709 | Loss: 0.016196 | Accuracy: 1.000000\n",
            "Iteration: 710 | Loss: 0.016158 | Accuracy: 1.000000\n",
            "Iteration: 711 | Loss: 0.016119 | Accuracy: 1.000000\n",
            "Iteration: 712 | Loss: 0.016081 | Accuracy: 1.000000\n",
            "Iteration: 713 | Loss: 0.016043 | Accuracy: 1.000000\n",
            "Iteration: 714 | Loss: 0.016006 | Accuracy: 1.000000\n",
            "Iteration: 715 | Loss: 0.015968 | Accuracy: 1.000000\n",
            "Iteration: 716 | Loss: 0.015931 | Accuracy: 1.000000\n",
            "Iteration: 717 | Loss: 0.015894 | Accuracy: 1.000000\n",
            "Iteration: 718 | Loss: 0.015857 | Accuracy: 1.000000\n",
            "Iteration: 719 | Loss: 0.015820 | Accuracy: 1.000000\n",
            "Iteration: 720 | Loss: 0.015783 | Accuracy: 1.000000\n",
            "Iteration: 721 | Loss: 0.015746 | Accuracy: 1.000000\n",
            "Iteration: 722 | Loss: 0.015710 | Accuracy: 1.000000\n",
            "Iteration: 723 | Loss: 0.015674 | Accuracy: 1.000000\n",
            "Iteration: 724 | Loss: 0.015637 | Accuracy: 1.000000\n",
            "Iteration: 725 | Loss: 0.015602 | Accuracy: 1.000000\n",
            "Iteration: 726 | Loss: 0.015566 | Accuracy: 1.000000\n",
            "Iteration: 727 | Loss: 0.015530 | Accuracy: 1.000000\n",
            "Iteration: 728 | Loss: 0.015495 | Accuracy: 1.000000\n",
            "Iteration: 729 | Loss: 0.015459 | Accuracy: 1.000000\n",
            "Iteration: 730 | Loss: 0.015424 | Accuracy: 1.000000\n",
            "Iteration: 731 | Loss: 0.015389 | Accuracy: 1.000000\n",
            "Iteration: 732 | Loss: 0.015354 | Accuracy: 1.000000\n",
            "Iteration: 733 | Loss: 0.015319 | Accuracy: 1.000000\n",
            "Iteration: 734 | Loss: 0.015285 | Accuracy: 1.000000\n",
            "Iteration: 735 | Loss: 0.015250 | Accuracy: 1.000000\n",
            "Iteration: 736 | Loss: 0.015216 | Accuracy: 1.000000\n",
            "Iteration: 737 | Loss: 0.015182 | Accuracy: 1.000000\n",
            "Iteration: 738 | Loss: 0.015148 | Accuracy: 1.000000\n",
            "Iteration: 739 | Loss: 0.015114 | Accuracy: 1.000000\n",
            "Iteration: 740 | Loss: 0.015080 | Accuracy: 1.000000\n",
            "Iteration: 741 | Loss: 0.015047 | Accuracy: 1.000000\n",
            "Iteration: 742 | Loss: 0.015013 | Accuracy: 1.000000\n",
            "Iteration: 743 | Loss: 0.014980 | Accuracy: 1.000000\n",
            "Iteration: 744 | Loss: 0.014947 | Accuracy: 1.000000\n",
            "Iteration: 745 | Loss: 0.014914 | Accuracy: 1.000000\n",
            "Iteration: 746 | Loss: 0.014881 | Accuracy: 1.000000\n",
            "Iteration: 747 | Loss: 0.014848 | Accuracy: 1.000000\n",
            "Iteration: 748 | Loss: 0.014815 | Accuracy: 1.000000\n",
            "Iteration: 749 | Loss: 0.014783 | Accuracy: 1.000000\n",
            "Iteration: 750 | Loss: 0.014750 | Accuracy: 1.000000\n",
            "Iteration: 751 | Loss: 0.014718 | Accuracy: 1.000000\n",
            "Iteration: 752 | Loss: 0.014686 | Accuracy: 1.000000\n",
            "Iteration: 753 | Loss: 0.014654 | Accuracy: 1.000000\n",
            "Iteration: 754 | Loss: 0.014622 | Accuracy: 1.000000\n",
            "Iteration: 755 | Loss: 0.014590 | Accuracy: 1.000000\n",
            "Iteration: 756 | Loss: 0.014559 | Accuracy: 1.000000\n",
            "Iteration: 757 | Loss: 0.014527 | Accuracy: 1.000000\n",
            "Iteration: 758 | Loss: 0.014496 | Accuracy: 1.000000\n",
            "Iteration: 759 | Loss: 0.014465 | Accuracy: 1.000000\n",
            "Iteration: 760 | Loss: 0.014434 | Accuracy: 1.000000\n",
            "Iteration: 761 | Loss: 0.014403 | Accuracy: 1.000000\n",
            "Iteration: 762 | Loss: 0.014372 | Accuracy: 1.000000\n",
            "Iteration: 763 | Loss: 0.014341 | Accuracy: 1.000000\n",
            "Iteration: 764 | Loss: 0.014311 | Accuracy: 1.000000\n",
            "Iteration: 765 | Loss: 0.014280 | Accuracy: 1.000000\n",
            "Iteration: 766 | Loss: 0.014250 | Accuracy: 1.000000\n",
            "Iteration: 767 | Loss: 0.014220 | Accuracy: 1.000000\n",
            "Iteration: 768 | Loss: 0.014190 | Accuracy: 1.000000\n",
            "Iteration: 769 | Loss: 0.014160 | Accuracy: 1.000000\n",
            "Iteration: 770 | Loss: 0.014130 | Accuracy: 1.000000\n",
            "Iteration: 771 | Loss: 0.014100 | Accuracy: 1.000000\n",
            "Iteration: 772 | Loss: 0.014070 | Accuracy: 1.000000\n",
            "Iteration: 773 | Loss: 0.014041 | Accuracy: 1.000000\n",
            "Iteration: 774 | Loss: 0.014012 | Accuracy: 1.000000\n",
            "Iteration: 775 | Loss: 0.013982 | Accuracy: 1.000000\n",
            "Iteration: 776 | Loss: 0.013953 | Accuracy: 1.000000\n",
            "Iteration: 777 | Loss: 0.013924 | Accuracy: 1.000000\n",
            "Iteration: 778 | Loss: 0.013895 | Accuracy: 1.000000\n",
            "Iteration: 779 | Loss: 0.013866 | Accuracy: 1.000000\n",
            "Iteration: 780 | Loss: 0.013838 | Accuracy: 1.000000\n",
            "Iteration: 781 | Loss: 0.013809 | Accuracy: 1.000000\n",
            "Iteration: 782 | Loss: 0.013781 | Accuracy: 1.000000\n",
            "Iteration: 783 | Loss: 0.013752 | Accuracy: 1.000000\n",
            "Iteration: 784 | Loss: 0.013724 | Accuracy: 1.000000\n",
            "Iteration: 785 | Loss: 0.013696 | Accuracy: 1.000000\n",
            "Iteration: 786 | Loss: 0.013668 | Accuracy: 1.000000\n",
            "Iteration: 787 | Loss: 0.013640 | Accuracy: 1.000000\n",
            "Iteration: 788 | Loss: 0.013612 | Accuracy: 1.000000\n",
            "Iteration: 789 | Loss: 0.013584 | Accuracy: 1.000000\n",
            "Iteration: 790 | Loss: 0.013557 | Accuracy: 1.000000\n",
            "Iteration: 791 | Loss: 0.013529 | Accuracy: 1.000000\n",
            "Iteration: 792 | Loss: 0.013502 | Accuracy: 1.000000\n",
            "Iteration: 793 | Loss: 0.013475 | Accuracy: 1.000000\n",
            "Iteration: 794 | Loss: 0.013447 | Accuracy: 1.000000\n",
            "Iteration: 795 | Loss: 0.013420 | Accuracy: 1.000000\n",
            "Iteration: 796 | Loss: 0.013393 | Accuracy: 1.000000\n",
            "Iteration: 797 | Loss: 0.013366 | Accuracy: 1.000000\n",
            "Iteration: 798 | Loss: 0.013340 | Accuracy: 1.000000\n",
            "Iteration: 799 | Loss: 0.013313 | Accuracy: 1.000000\n",
            "Iteration: 800 | Loss: 0.013286 | Accuracy: 1.000000\n",
            "Iteration: 801 | Loss: 0.013260 | Accuracy: 1.000000\n",
            "Iteration: 802 | Loss: 0.013234 | Accuracy: 1.000000\n",
            "Iteration: 803 | Loss: 0.013207 | Accuracy: 1.000000\n",
            "Iteration: 804 | Loss: 0.013181 | Accuracy: 1.000000\n",
            "Iteration: 805 | Loss: 0.013155 | Accuracy: 1.000000\n",
            "Iteration: 806 | Loss: 0.013129 | Accuracy: 1.000000\n",
            "Iteration: 807 | Loss: 0.013103 | Accuracy: 1.000000\n",
            "Iteration: 808 | Loss: 0.013077 | Accuracy: 1.000000\n",
            "Iteration: 809 | Loss: 0.013052 | Accuracy: 1.000000\n",
            "Iteration: 810 | Loss: 0.013026 | Accuracy: 1.000000\n",
            "Iteration: 811 | Loss: 0.013001 | Accuracy: 1.000000\n",
            "Iteration: 812 | Loss: 0.012975 | Accuracy: 1.000000\n",
            "Iteration: 813 | Loss: 0.012950 | Accuracy: 1.000000\n",
            "Iteration: 814 | Loss: 0.012925 | Accuracy: 1.000000\n",
            "Iteration: 815 | Loss: 0.012900 | Accuracy: 1.000000\n",
            "Iteration: 816 | Loss: 0.012875 | Accuracy: 1.000000\n",
            "Iteration: 817 | Loss: 0.012850 | Accuracy: 1.000000\n",
            "Iteration: 818 | Loss: 0.012825 | Accuracy: 1.000000\n",
            "Iteration: 819 | Loss: 0.012800 | Accuracy: 1.000000\n",
            "Iteration: 820 | Loss: 0.012775 | Accuracy: 1.000000\n",
            "Iteration: 821 | Loss: 0.012751 | Accuracy: 1.000000\n",
            "Iteration: 822 | Loss: 0.012726 | Accuracy: 1.000000\n",
            "Iteration: 823 | Loss: 0.012702 | Accuracy: 1.000000\n",
            "Iteration: 824 | Loss: 0.012678 | Accuracy: 1.000000\n",
            "Iteration: 825 | Loss: 0.012653 | Accuracy: 1.000000\n",
            "Iteration: 826 | Loss: 0.012629 | Accuracy: 1.000000\n",
            "Iteration: 827 | Loss: 0.012605 | Accuracy: 1.000000\n",
            "Iteration: 828 | Loss: 0.012581 | Accuracy: 1.000000\n",
            "Iteration: 829 | Loss: 0.012557 | Accuracy: 1.000000\n",
            "Iteration: 830 | Loss: 0.012534 | Accuracy: 1.000000\n",
            "Iteration: 831 | Loss: 0.012510 | Accuracy: 1.000000\n",
            "Iteration: 832 | Loss: 0.012486 | Accuracy: 1.000000\n",
            "Iteration: 833 | Loss: 0.012463 | Accuracy: 1.000000\n",
            "Iteration: 834 | Loss: 0.012439 | Accuracy: 1.000000\n",
            "Iteration: 835 | Loss: 0.012416 | Accuracy: 1.000000\n",
            "Iteration: 836 | Loss: 0.012393 | Accuracy: 1.000000\n",
            "Iteration: 837 | Loss: 0.012369 | Accuracy: 1.000000\n",
            "Iteration: 838 | Loss: 0.012346 | Accuracy: 1.000000\n",
            "Iteration: 839 | Loss: 0.012323 | Accuracy: 1.000000\n",
            "Iteration: 840 | Loss: 0.012300 | Accuracy: 1.000000\n",
            "Iteration: 841 | Loss: 0.012277 | Accuracy: 1.000000\n",
            "Iteration: 842 | Loss: 0.012255 | Accuracy: 1.000000\n",
            "Iteration: 843 | Loss: 0.012232 | Accuracy: 1.000000\n",
            "Iteration: 844 | Loss: 0.012209 | Accuracy: 1.000000\n",
            "Iteration: 845 | Loss: 0.012187 | Accuracy: 1.000000\n",
            "Iteration: 846 | Loss: 0.012164 | Accuracy: 1.000000\n",
            "Iteration: 847 | Loss: 0.012142 | Accuracy: 1.000000\n",
            "Iteration: 848 | Loss: 0.012119 | Accuracy: 1.000000\n",
            "Iteration: 849 | Loss: 0.012097 | Accuracy: 1.000000\n",
            "Iteration: 850 | Loss: 0.012075 | Accuracy: 1.000000\n",
            "Iteration: 851 | Loss: 0.012053 | Accuracy: 1.000000\n",
            "Iteration: 852 | Loss: 0.012031 | Accuracy: 1.000000\n",
            "Iteration: 853 | Loss: 0.012009 | Accuracy: 1.000000\n",
            "Iteration: 854 | Loss: 0.011987 | Accuracy: 1.000000\n",
            "Iteration: 855 | Loss: 0.011965 | Accuracy: 1.000000\n",
            "Iteration: 856 | Loss: 0.011944 | Accuracy: 1.000000\n",
            "Iteration: 857 | Loss: 0.011922 | Accuracy: 1.000000\n",
            "Iteration: 858 | Loss: 0.011900 | Accuracy: 1.000000\n",
            "Iteration: 859 | Loss: 0.011879 | Accuracy: 1.000000\n",
            "Iteration: 860 | Loss: 0.011857 | Accuracy: 1.000000\n",
            "Iteration: 861 | Loss: 0.011836 | Accuracy: 1.000000\n",
            "Iteration: 862 | Loss: 0.011815 | Accuracy: 1.000000\n",
            "Iteration: 863 | Loss: 0.011794 | Accuracy: 1.000000\n",
            "Iteration: 864 | Loss: 0.011773 | Accuracy: 1.000000\n",
            "Iteration: 865 | Loss: 0.011752 | Accuracy: 1.000000\n",
            "Iteration: 866 | Loss: 0.011730 | Accuracy: 1.000000\n",
            "Iteration: 867 | Loss: 0.011710 | Accuracy: 1.000000\n",
            "Iteration: 868 | Loss: 0.011689 | Accuracy: 1.000000\n",
            "Iteration: 869 | Loss: 0.011668 | Accuracy: 1.000000\n",
            "Iteration: 870 | Loss: 0.011647 | Accuracy: 1.000000\n",
            "Iteration: 871 | Loss: 0.011627 | Accuracy: 1.000000\n",
            "Iteration: 872 | Loss: 0.011606 | Accuracy: 1.000000\n",
            "Iteration: 873 | Loss: 0.011586 | Accuracy: 1.000000\n",
            "Iteration: 874 | Loss: 0.011565 | Accuracy: 1.000000\n",
            "Iteration: 875 | Loss: 0.011545 | Accuracy: 1.000000\n",
            "Iteration: 876 | Loss: 0.011524 | Accuracy: 1.000000\n",
            "Iteration: 877 | Loss: 0.011504 | Accuracy: 1.000000\n",
            "Iteration: 878 | Loss: 0.011484 | Accuracy: 1.000000\n",
            "Iteration: 879 | Loss: 0.011464 | Accuracy: 1.000000\n",
            "Iteration: 880 | Loss: 0.011444 | Accuracy: 1.000000\n",
            "Iteration: 881 | Loss: 0.011424 | Accuracy: 1.000000\n",
            "Iteration: 882 | Loss: 0.011404 | Accuracy: 1.000000\n",
            "Iteration: 883 | Loss: 0.011384 | Accuracy: 1.000000\n",
            "Iteration: 884 | Loss: 0.011364 | Accuracy: 1.000000\n",
            "Iteration: 885 | Loss: 0.011345 | Accuracy: 1.000000\n",
            "Iteration: 886 | Loss: 0.011325 | Accuracy: 1.000000\n",
            "Iteration: 887 | Loss: 0.011306 | Accuracy: 1.000000\n",
            "Iteration: 888 | Loss: 0.011286 | Accuracy: 1.000000\n",
            "Iteration: 889 | Loss: 0.011267 | Accuracy: 1.000000\n",
            "Iteration: 890 | Loss: 0.011247 | Accuracy: 1.000000\n",
            "Iteration: 891 | Loss: 0.011228 | Accuracy: 1.000000\n",
            "Iteration: 892 | Loss: 0.011209 | Accuracy: 1.000000\n",
            "Iteration: 893 | Loss: 0.011189 | Accuracy: 1.000000\n",
            "Iteration: 894 | Loss: 0.011170 | Accuracy: 1.000000\n",
            "Iteration: 895 | Loss: 0.011151 | Accuracy: 1.000000\n",
            "Iteration: 896 | Loss: 0.011132 | Accuracy: 1.000000\n",
            "Iteration: 897 | Loss: 0.011113 | Accuracy: 1.000000\n",
            "Iteration: 898 | Loss: 0.011094 | Accuracy: 1.000000\n",
            "Iteration: 899 | Loss: 0.011076 | Accuracy: 1.000000\n",
            "Iteration: 900 | Loss: 0.011057 | Accuracy: 1.000000\n",
            "Iteration: 901 | Loss: 0.011038 | Accuracy: 1.000000\n",
            "Iteration: 902 | Loss: 0.011019 | Accuracy: 1.000000\n",
            "Iteration: 903 | Loss: 0.011001 | Accuracy: 1.000000\n",
            "Iteration: 904 | Loss: 0.010982 | Accuracy: 1.000000\n",
            "Iteration: 905 | Loss: 0.010964 | Accuracy: 1.000000\n",
            "Iteration: 906 | Loss: 0.010945 | Accuracy: 1.000000\n",
            "Iteration: 907 | Loss: 0.010927 | Accuracy: 1.000000\n",
            "Iteration: 908 | Loss: 0.010909 | Accuracy: 1.000000\n",
            "Iteration: 909 | Loss: 0.010891 | Accuracy: 1.000000\n",
            "Iteration: 910 | Loss: 0.010872 | Accuracy: 1.000000\n",
            "Iteration: 911 | Loss: 0.010854 | Accuracy: 1.000000\n",
            "Iteration: 912 | Loss: 0.010836 | Accuracy: 1.000000\n",
            "Iteration: 913 | Loss: 0.010818 | Accuracy: 1.000000\n",
            "Iteration: 914 | Loss: 0.010800 | Accuracy: 1.000000\n",
            "Iteration: 915 | Loss: 0.010782 | Accuracy: 1.000000\n",
            "Iteration: 916 | Loss: 0.010764 | Accuracy: 1.000000\n",
            "Iteration: 917 | Loss: 0.010747 | Accuracy: 1.000000\n",
            "Iteration: 918 | Loss: 0.010729 | Accuracy: 1.000000\n",
            "Iteration: 919 | Loss: 0.010711 | Accuracy: 1.000000\n",
            "Iteration: 920 | Loss: 0.010694 | Accuracy: 1.000000\n",
            "Iteration: 921 | Loss: 0.010676 | Accuracy: 1.000000\n",
            "Iteration: 922 | Loss: 0.010658 | Accuracy: 1.000000\n",
            "Iteration: 923 | Loss: 0.010641 | Accuracy: 1.000000\n",
            "Iteration: 924 | Loss: 0.010624 | Accuracy: 1.000000\n",
            "Iteration: 925 | Loss: 0.010606 | Accuracy: 1.000000\n",
            "Iteration: 926 | Loss: 0.010589 | Accuracy: 1.000000\n",
            "Iteration: 927 | Loss: 0.010572 | Accuracy: 1.000000\n",
            "Iteration: 928 | Loss: 0.010554 | Accuracy: 1.000000\n",
            "Iteration: 929 | Loss: 0.010537 | Accuracy: 1.000000\n",
            "Iteration: 930 | Loss: 0.010520 | Accuracy: 1.000000\n",
            "Iteration: 931 | Loss: 0.010503 | Accuracy: 1.000000\n",
            "Iteration: 932 | Loss: 0.010486 | Accuracy: 1.000000\n",
            "Iteration: 933 | Loss: 0.010469 | Accuracy: 1.000000\n",
            "Iteration: 934 | Loss: 0.010452 | Accuracy: 1.000000\n",
            "Iteration: 935 | Loss: 0.010436 | Accuracy: 1.000000\n",
            "Iteration: 936 | Loss: 0.010419 | Accuracy: 1.000000\n",
            "Iteration: 937 | Loss: 0.010402 | Accuracy: 1.000000\n",
            "Iteration: 938 | Loss: 0.010385 | Accuracy: 1.000000\n",
            "Iteration: 939 | Loss: 0.010369 | Accuracy: 1.000000\n",
            "Iteration: 940 | Loss: 0.010352 | Accuracy: 1.000000\n",
            "Iteration: 941 | Loss: 0.010336 | Accuracy: 1.000000\n",
            "Iteration: 942 | Loss: 0.010319 | Accuracy: 1.000000\n",
            "Iteration: 943 | Loss: 0.010303 | Accuracy: 1.000000\n",
            "Iteration: 944 | Loss: 0.010286 | Accuracy: 1.000000\n",
            "Iteration: 945 | Loss: 0.010270 | Accuracy: 1.000000\n",
            "Iteration: 946 | Loss: 0.010254 | Accuracy: 1.000000\n",
            "Iteration: 947 | Loss: 0.010237 | Accuracy: 1.000000\n",
            "Iteration: 948 | Loss: 0.010221 | Accuracy: 1.000000\n",
            "Iteration: 949 | Loss: 0.010205 | Accuracy: 1.000000\n",
            "Iteration: 950 | Loss: 0.010189 | Accuracy: 1.000000\n",
            "Iteration: 951 | Loss: 0.010173 | Accuracy: 1.000000\n",
            "Iteration: 952 | Loss: 0.010157 | Accuracy: 1.000000\n",
            "Iteration: 953 | Loss: 0.010141 | Accuracy: 1.000000\n",
            "Iteration: 954 | Loss: 0.010125 | Accuracy: 1.000000\n",
            "Iteration: 955 | Loss: 0.010109 | Accuracy: 1.000000\n",
            "Iteration: 956 | Loss: 0.010093 | Accuracy: 1.000000\n",
            "Iteration: 957 | Loss: 0.010077 | Accuracy: 1.000000\n",
            "Iteration: 958 | Loss: 0.010062 | Accuracy: 1.000000\n",
            "Iteration: 959 | Loss: 0.010046 | Accuracy: 1.000000\n",
            "Iteration: 960 | Loss: 0.010030 | Accuracy: 1.000000\n",
            "Iteration: 961 | Loss: 0.010015 | Accuracy: 1.000000\n",
            "Iteration: 962 | Loss: 0.009999 | Accuracy: 1.000000\n",
            "Iteration: 963 | Loss: 0.009984 | Accuracy: 1.000000\n",
            "Iteration: 964 | Loss: 0.009968 | Accuracy: 1.000000\n",
            "Iteration: 965 | Loss: 0.009953 | Accuracy: 1.000000\n",
            "Iteration: 966 | Loss: 0.009938 | Accuracy: 1.000000\n",
            "Iteration: 967 | Loss: 0.009922 | Accuracy: 1.000000\n",
            "Iteration: 968 | Loss: 0.009907 | Accuracy: 1.000000\n",
            "Iteration: 969 | Loss: 0.009892 | Accuracy: 1.000000\n",
            "Iteration: 970 | Loss: 0.009876 | Accuracy: 1.000000\n",
            "Iteration: 971 | Loss: 0.009861 | Accuracy: 1.000000\n",
            "Iteration: 972 | Loss: 0.009846 | Accuracy: 1.000000\n",
            "Iteration: 973 | Loss: 0.009831 | Accuracy: 1.000000\n",
            "Iteration: 974 | Loss: 0.009816 | Accuracy: 1.000000\n",
            "Iteration: 975 | Loss: 0.009801 | Accuracy: 1.000000\n",
            "Iteration: 976 | Loss: 0.009786 | Accuracy: 1.000000\n",
            "Iteration: 977 | Loss: 0.009771 | Accuracy: 1.000000\n",
            "Iteration: 978 | Loss: 0.009756 | Accuracy: 1.000000\n",
            "Iteration: 979 | Loss: 0.009742 | Accuracy: 1.000000\n",
            "Iteration: 980 | Loss: 0.009727 | Accuracy: 1.000000\n",
            "Iteration: 981 | Loss: 0.009712 | Accuracy: 1.000000\n",
            "Iteration: 982 | Loss: 0.009697 | Accuracy: 1.000000\n",
            "Iteration: 983 | Loss: 0.009683 | Accuracy: 1.000000\n",
            "Iteration: 984 | Loss: 0.009668 | Accuracy: 1.000000\n",
            "Iteration: 985 | Loss: 0.009654 | Accuracy: 1.000000\n",
            "Iteration: 986 | Loss: 0.009639 | Accuracy: 1.000000\n",
            "Iteration: 987 | Loss: 0.009625 | Accuracy: 1.000000\n",
            "Iteration: 988 | Loss: 0.009610 | Accuracy: 1.000000\n",
            "Iteration: 989 | Loss: 0.009596 | Accuracy: 1.000000\n",
            "Iteration: 990 | Loss: 0.009581 | Accuracy: 1.000000\n",
            "Iteration: 991 | Loss: 0.009567 | Accuracy: 1.000000\n",
            "Iteration: 992 | Loss: 0.009553 | Accuracy: 1.000000\n",
            "Iteration: 993 | Loss: 0.009538 | Accuracy: 1.000000\n",
            "Iteration: 994 | Loss: 0.009524 | Accuracy: 1.000000\n",
            "Iteration: 995 | Loss: 0.009510 | Accuracy: 1.000000\n",
            "Iteration: 996 | Loss: 0.009496 | Accuracy: 1.000000\n",
            "Iteration: 997 | Loss: 0.009482 | Accuracy: 1.000000\n",
            "Iteration: 998 | Loss: 0.009468 | Accuracy: 1.000000\n",
            "Iteration: 999 | Loss: 0.009454 | Accuracy: 1.000000\n",
            "Iteration: 1000 | Loss: 0.009440 | Accuracy: 1.000000\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GsasOmNdU6zZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab62d481-9f17-41ba-dc25-100c713c5726"
      },
      "cell_type": "code",
      "source": [
        "print (\"Test Loss: %f | Test Accuracy: %f\" % (testLoss, testAccuracy))\n",
        "\n",
        "testSetPredictions = np.squeeze(testSetPredictions)\n",
        "threshold = 0.5\n",
        "testSetPredictions = testSetPredictions > threshold"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 5.930548 | Test Accuracy: 0.530000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aJjxrQTqU7eQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1157
        },
        "outputId": "9b72a62d-6470-4640-9538-77fbf759e761"
      },
      "cell_type": "code",
      "source": [
        "# Create the predictions plot\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "plotDim = 1000\n",
        "dataPlot = np.ones((plotDim, plotDim, 3), dtype=np.float32)\n",
        "scaledIndices = np.floor(testDataPoints * plotDim).astype(np.int32)\n",
        "\n",
        "# Create decision boundary meshgrid\n",
        "for idx in range(grid.shape[0]):\n",
        "    if gridPredictions[idx] > 0.5:\n",
        "      dataPlot[grid[idx, 1], grid[idx, 0], :] = [0.5, 0.5, 0.5]\n",
        "\n",
        "for i in range(scaledIndices.shape[0]):\n",
        "  color = (0.0, 0.0, 0.0)\n",
        "  if testSetPredictions[i]:\n",
        "    color = (1.0, 0.0, 0.0) # RGB\n",
        "  \n",
        "  cv2.circle(dataPlot, (scaledIndices[i, 0], scaledIndices[i, 1]), 5, color, -1)\n",
        "\n",
        "#drawEllipse(dataPlot, center=center, axes=((a, b) if a > b else (b, a)), angle=0.0, startAngle=0.0, endAngle=360, color=(1.0, 0.0, 1.0), thickness=50)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.axis('off')\n",
        "plt.imshow(dataPlot)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f67013ecd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGUAAARiCAYAAAAN70rmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f6704500128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "h_daCNOzmXV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Add L2 Regularization**"
      ]
    },
    {
      "metadata": {
        "id": "TVHPRwwDmMrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8f0669e-7b75-42d8-a3e2-643ad576309f"
      },
      "cell_type": "code",
      "source": [
        "# Perform polynomial logistic regression\n",
        "numIterations = 1000\n",
        "learningRate = 1e-1\n",
        "\n",
        "# Clear previous graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "dataPointPlaceholder = tf.placeholder(tf.float32, shape=(None, numDimensions), name=\"dataPointPlaceholder\")\n",
        "labelPlaceholder = tf.placeholder(tf.float32, shape=(None,), name=\"labelPlaceholder\")\n",
        "\n",
        "# Create the model variables\n",
        "def addMLPNode(dataInput, outDim):\n",
        "\tW = tf.Variable(tf.random_normal([int(dataInput.get_shape()[-1]), outDim]))\n",
        "\tb = tf.Variable(tf.zeros([outDim]))\n",
        "\ty = tf.matmul(dataInput, W) + b\n",
        "\treturn y, W\n",
        "\n",
        "# Create the model\n",
        "hiddenDimension = 10\n",
        "mlp, W1 = addMLPNode(dataPointPlaceholder, hiddenDimension)\n",
        "mlp = tf.nn.tanh(mlp)\n",
        "mlp, W2 = addMLPNode(mlp, 1)\n",
        "mlp = tf.nn.sigmoid(mlp)\n",
        "y = tf.reshape(mlp, [-1])\n",
        "print (y.get_shape())\n",
        "\n",
        "# Define the loss function\n",
        "epsilon = 1e-8\n",
        "regLambda = 1e-2\n",
        "with tf.name_scope('loss') as scope:\n",
        "\tloss = - (labelPlaceholder * tf.log(y + epsilon)) - ((1.0 - labelPlaceholder) * tf.log(1.0 - y + epsilon))\n",
        "\tloss = tf.reduce_mean(loss) + regLambda * (tf.reduce_sum(W1) + tf.reduce_sum(W2))\n",
        "  \n",
        "with tf.name_scope('accuracy'):\n",
        "\tcorrectPredictions = tf.equal(tf.round(y), tf.round(labelPlaceholder))\n",
        "\taccuracy = tf.reduce_mean(tf.cast(correctPredictions, tf.float32), name='accuracy')\n",
        "\n",
        "with tf.name_scope('optimizer') as scope:\n",
        "\toptimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O2qGq5kUoRD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18053
        },
        "outputId": "7896d70f-cd34-4b04-9d7a-3ae437bf21fb"
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print (\"Training model with regularization\")\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "\n",
        "with tf.Session(config=config) as sess:\n",
        "  # Initializing the variables\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "\n",
        "  for i in range(numIterations):\n",
        "    _, trainLoss, trainAccuracy = sess.run([optimizer, loss, accuracy], feed_dict={dataPointPlaceholder: trainDataPoints, labelPlaceholder: trainLabelsFloat})\n",
        "    print (\"Iteration: %d | Loss: %f | Accuracy: %f\" % (i + 1, trainLoss, trainAccuracy))\n",
        "  print (\"Training finished!\")\n",
        "\n",
        "  # Generate predictions on the test set\n",
        "  testSetPredictions, testLoss, testAccuracy = sess.run([y, loss, accuracy], feed_dict={dataPointPlaceholder: testDataPoints, labelPlaceholder: testLabelsFloat})\n",
        "  \n",
        "  # Get decision boundary\n",
        "  gridPredictions = sess.run([y], feed_dict={dataPointPlaceholder: grid / plotDim})[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model with regularization\n",
            "Iteration: 1 | Loss: 1.324650 | Accuracy: 0.190000\n",
            "Iteration: 2 | Loss: 0.956664 | Accuracy: 0.480000\n",
            "Iteration: 3 | Loss: 0.878992 | Accuracy: 0.705000\n",
            "Iteration: 4 | Loss: 0.801781 | Accuracy: 0.705000\n",
            "Iteration: 5 | Loss: 0.679142 | Accuracy: 0.705000\n",
            "Iteration: 6 | Loss: 0.547733 | Accuracy: 0.705000\n",
            "Iteration: 7 | Loss: 0.456872 | Accuracy: 0.745000\n",
            "Iteration: 8 | Loss: 0.427615 | Accuracy: 0.875000\n",
            "Iteration: 9 | Loss: 0.420993 | Accuracy: 0.850000\n",
            "Iteration: 10 | Loss: 0.393476 | Accuracy: 0.840000\n",
            "Iteration: 11 | Loss: 0.345847 | Accuracy: 0.860000\n",
            "Iteration: 12 | Loss: 0.301938 | Accuracy: 0.860000\n",
            "Iteration: 13 | Loss: 0.279829 | Accuracy: 0.875000\n",
            "Iteration: 14 | Loss: 0.278168 | Accuracy: 0.855000\n",
            "Iteration: 15 | Loss: 0.281159 | Accuracy: 0.845000\n",
            "Iteration: 16 | Loss: 0.274970 | Accuracy: 0.840000\n",
            "Iteration: 17 | Loss: 0.257290 | Accuracy: 0.855000\n",
            "Iteration: 18 | Loss: 0.235140 | Accuracy: 0.855000\n",
            "Iteration: 19 | Loss: 0.218003 | Accuracy: 0.860000\n",
            "Iteration: 20 | Loss: 0.210966 | Accuracy: 0.870000\n",
            "Iteration: 21 | Loss: 0.210979 | Accuracy: 0.855000\n",
            "Iteration: 22 | Loss: 0.209738 | Accuracy: 0.855000\n",
            "Iteration: 23 | Loss: 0.201605 | Accuracy: 0.860000\n",
            "Iteration: 24 | Loss: 0.188169 | Accuracy: 0.865000\n",
            "Iteration: 25 | Loss: 0.175362 | Accuracy: 0.870000\n",
            "Iteration: 26 | Loss: 0.167474 | Accuracy: 0.870000\n",
            "Iteration: 27 | Loss: 0.163872 | Accuracy: 0.845000\n",
            "Iteration: 28 | Loss: 0.160471 | Accuracy: 0.840000\n",
            "Iteration: 29 | Loss: 0.153704 | Accuracy: 0.845000\n",
            "Iteration: 30 | Loss: 0.143250 | Accuracy: 0.845000\n",
            "Iteration: 31 | Loss: 0.131727 | Accuracy: 0.855000\n",
            "Iteration: 32 | Loss: 0.122183 | Accuracy: 0.870000\n",
            "Iteration: 33 | Loss: 0.115455 | Accuracy: 0.870000\n",
            "Iteration: 34 | Loss: 0.109565 | Accuracy: 0.870000\n",
            "Iteration: 35 | Loss: 0.101857 | Accuracy: 0.865000\n",
            "Iteration: 36 | Loss: 0.091703 | Accuracy: 0.870000\n",
            "Iteration: 37 | Loss: 0.080856 | Accuracy: 0.870000\n",
            "Iteration: 38 | Loss: 0.071365 | Accuracy: 0.865000\n",
            "Iteration: 39 | Loss: 0.063503 | Accuracy: 0.870000\n",
            "Iteration: 40 | Loss: 0.055794 | Accuracy: 0.860000\n",
            "Iteration: 41 | Loss: 0.046769 | Accuracy: 0.865000\n",
            "Iteration: 42 | Loss: 0.036442 | Accuracy: 0.865000\n",
            "Iteration: 43 | Loss: 0.026096 | Accuracy: 0.875000\n",
            "Iteration: 44 | Loss: 0.016799 | Accuracy: 0.880000\n",
            "Iteration: 45 | Loss: 0.008286 | Accuracy: 0.875000\n",
            "Iteration: 46 | Loss: -0.000611 | Accuracy: 0.870000\n",
            "Iteration: 47 | Loss: -0.010544 | Accuracy: 0.875000\n",
            "Iteration: 48 | Loss: -0.020976 | Accuracy: 0.880000\n",
            "Iteration: 49 | Loss: -0.030974 | Accuracy: 0.875000\n",
            "Iteration: 50 | Loss: -0.040334 | Accuracy: 0.885000\n",
            "Iteration: 51 | Loss: -0.049682 | Accuracy: 0.885000\n",
            "Iteration: 52 | Loss: -0.059615 | Accuracy: 0.885000\n",
            "Iteration: 53 | Loss: -0.070020 | Accuracy: 0.885000\n",
            "Iteration: 54 | Loss: -0.080315 | Accuracy: 0.885000\n",
            "Iteration: 55 | Loss: -0.090209 | Accuracy: 0.880000\n",
            "Iteration: 56 | Loss: -0.100023 | Accuracy: 0.880000\n",
            "Iteration: 57 | Loss: -0.110216 | Accuracy: 0.880000\n",
            "Iteration: 58 | Loss: -0.120807 | Accuracy: 0.880000\n",
            "Iteration: 59 | Loss: -0.131428 | Accuracy: 0.885000\n",
            "Iteration: 60 | Loss: -0.141876 | Accuracy: 0.890000\n",
            "Iteration: 61 | Loss: -0.152362 | Accuracy: 0.890000\n",
            "Iteration: 62 | Loss: -0.163205 | Accuracy: 0.890000\n",
            "Iteration: 63 | Loss: -0.174431 | Accuracy: 0.890000\n",
            "Iteration: 64 | Loss: -0.185812 | Accuracy: 0.890000\n",
            "Iteration: 65 | Loss: -0.197219 | Accuracy: 0.890000\n",
            "Iteration: 66 | Loss: -0.208780 | Accuracy: 0.890000\n",
            "Iteration: 67 | Loss: -0.220667 | Accuracy: 0.890000\n",
            "Iteration: 68 | Loss: -0.232831 | Accuracy: 0.890000\n",
            "Iteration: 69 | Loss: -0.245087 | Accuracy: 0.890000\n",
            "Iteration: 70 | Loss: -0.257363 | Accuracy: 0.890000\n",
            "Iteration: 71 | Loss: -0.269761 | Accuracy: 0.890000\n",
            "Iteration: 72 | Loss: -0.282355 | Accuracy: 0.890000\n",
            "Iteration: 73 | Loss: -0.295054 | Accuracy: 0.890000\n",
            "Iteration: 74 | Loss: -0.307715 | Accuracy: 0.895000\n",
            "Iteration: 75 | Loss: -0.320312 | Accuracy: 0.905000\n",
            "Iteration: 76 | Loss: -0.332925 | Accuracy: 0.905000\n",
            "Iteration: 77 | Loss: -0.345584 | Accuracy: 0.905000\n",
            "Iteration: 78 | Loss: -0.358228 | Accuracy: 0.905000\n",
            "Iteration: 79 | Loss: -0.370820 | Accuracy: 0.905000\n",
            "Iteration: 80 | Loss: -0.383412 | Accuracy: 0.905000\n",
            "Iteration: 81 | Loss: -0.396062 | Accuracy: 0.905000\n",
            "Iteration: 82 | Loss: -0.408750 | Accuracy: 0.910000\n",
            "Iteration: 83 | Loss: -0.421426 | Accuracy: 0.920000\n",
            "Iteration: 84 | Loss: -0.434103 | Accuracy: 0.920000\n",
            "Iteration: 85 | Loss: -0.446827 | Accuracy: 0.920000\n",
            "Iteration: 86 | Loss: -0.459598 | Accuracy: 0.920000\n",
            "Iteration: 87 | Loss: -0.472376 | Accuracy: 0.920000\n",
            "Iteration: 88 | Loss: -0.485152 | Accuracy: 0.920000\n",
            "Iteration: 89 | Loss: -0.497954 | Accuracy: 0.925000\n",
            "Iteration: 90 | Loss: -0.510788 | Accuracy: 0.925000\n",
            "Iteration: 91 | Loss: -0.523628 | Accuracy: 0.930000\n",
            "Iteration: 92 | Loss: -0.536459 | Accuracy: 0.935000\n",
            "Iteration: 93 | Loss: -0.549301 | Accuracy: 0.935000\n",
            "Iteration: 94 | Loss: -0.562163 | Accuracy: 0.935000\n",
            "Iteration: 95 | Loss: -0.575029 | Accuracy: 0.935000\n",
            "Iteration: 96 | Loss: -0.587886 | Accuracy: 0.940000\n",
            "Iteration: 97 | Loss: -0.600746 | Accuracy: 0.940000\n",
            "Iteration: 98 | Loss: -0.613617 | Accuracy: 0.940000\n",
            "Iteration: 99 | Loss: -0.626486 | Accuracy: 0.940000\n",
            "Iteration: 100 | Loss: -0.639349 | Accuracy: 0.940000\n",
            "Iteration: 101 | Loss: -0.652214 | Accuracy: 0.940000\n",
            "Iteration: 102 | Loss: -0.665090 | Accuracy: 0.940000\n",
            "Iteration: 103 | Loss: -0.677968 | Accuracy: 0.940000\n",
            "Iteration: 104 | Loss: -0.690844 | Accuracy: 0.940000\n",
            "Iteration: 105 | Loss: -0.703727 | Accuracy: 0.940000\n",
            "Iteration: 106 | Loss: -0.716619 | Accuracy: 0.940000\n",
            "Iteration: 107 | Loss: -0.729516 | Accuracy: 0.940000\n",
            "Iteration: 108 | Loss: -0.742419 | Accuracy: 0.940000\n",
            "Iteration: 109 | Loss: -0.755335 | Accuracy: 0.940000\n",
            "Iteration: 110 | Loss: -0.768267 | Accuracy: 0.945000\n",
            "Iteration: 111 | Loss: -0.781210 | Accuracy: 0.950000\n",
            "Iteration: 112 | Loss: -0.794166 | Accuracy: 0.950000\n",
            "Iteration: 113 | Loss: -0.807139 | Accuracy: 0.950000\n",
            "Iteration: 114 | Loss: -0.820129 | Accuracy: 0.955000\n",
            "Iteration: 115 | Loss: -0.833135 | Accuracy: 0.960000\n",
            "Iteration: 116 | Loss: -0.846159 | Accuracy: 0.960000\n",
            "Iteration: 117 | Loss: -0.859206 | Accuracy: 0.960000\n",
            "Iteration: 118 | Loss: -0.872273 | Accuracy: 0.960000\n",
            "Iteration: 119 | Loss: -0.885360 | Accuracy: 0.960000\n",
            "Iteration: 120 | Loss: -0.898468 | Accuracy: 0.960000\n",
            "Iteration: 121 | Loss: -0.911600 | Accuracy: 0.960000\n",
            "Iteration: 122 | Loss: -0.924753 | Accuracy: 0.960000\n",
            "Iteration: 123 | Loss: -0.937930 | Accuracy: 0.960000\n",
            "Iteration: 124 | Loss: -0.951132 | Accuracy: 0.960000\n",
            "Iteration: 125 | Loss: -0.964360 | Accuracy: 0.960000\n",
            "Iteration: 126 | Loss: -0.977611 | Accuracy: 0.965000\n",
            "Iteration: 127 | Loss: -0.990889 | Accuracy: 0.965000\n",
            "Iteration: 128 | Loss: -1.004193 | Accuracy: 0.965000\n",
            "Iteration: 129 | Loss: -1.017524 | Accuracy: 0.965000\n",
            "Iteration: 130 | Loss: -1.030882 | Accuracy: 0.965000\n",
            "Iteration: 131 | Loss: -1.044268 | Accuracy: 0.965000\n",
            "Iteration: 132 | Loss: -1.057683 | Accuracy: 0.965000\n",
            "Iteration: 133 | Loss: -1.071126 | Accuracy: 0.960000\n",
            "Iteration: 134 | Loss: -1.084598 | Accuracy: 0.960000\n",
            "Iteration: 135 | Loss: -1.098099 | Accuracy: 0.960000\n",
            "Iteration: 136 | Loss: -1.111629 | Accuracy: 0.960000\n",
            "Iteration: 137 | Loss: -1.125188 | Accuracy: 0.960000\n",
            "Iteration: 138 | Loss: -1.138776 | Accuracy: 0.960000\n",
            "Iteration: 139 | Loss: -1.152393 | Accuracy: 0.960000\n",
            "Iteration: 140 | Loss: -1.166038 | Accuracy: 0.965000\n",
            "Iteration: 141 | Loss: -1.179711 | Accuracy: 0.965000\n",
            "Iteration: 142 | Loss: -1.193411 | Accuracy: 0.970000\n",
            "Iteration: 143 | Loss: -1.207139 | Accuracy: 0.970000\n",
            "Iteration: 144 | Loss: -1.220893 | Accuracy: 0.970000\n",
            "Iteration: 145 | Loss: -1.234673 | Accuracy: 0.970000\n",
            "Iteration: 146 | Loss: -1.248478 | Accuracy: 0.970000\n",
            "Iteration: 147 | Loss: -1.262308 | Accuracy: 0.970000\n",
            "Iteration: 148 | Loss: -1.276161 | Accuracy: 0.970000\n",
            "Iteration: 149 | Loss: -1.290038 | Accuracy: 0.970000\n",
            "Iteration: 150 | Loss: -1.303936 | Accuracy: 0.970000\n",
            "Iteration: 151 | Loss: -1.317856 | Accuracy: 0.970000\n",
            "Iteration: 152 | Loss: -1.331795 | Accuracy: 0.970000\n",
            "Iteration: 153 | Loss: -1.345754 | Accuracy: 0.970000\n",
            "Iteration: 154 | Loss: -1.359731 | Accuracy: 0.970000\n",
            "Iteration: 155 | Loss: -1.373724 | Accuracy: 0.970000\n",
            "Iteration: 156 | Loss: -1.387734 | Accuracy: 0.975000\n",
            "Iteration: 157 | Loss: -1.401759 | Accuracy: 0.975000\n",
            "Iteration: 158 | Loss: -1.415797 | Accuracy: 0.975000\n",
            "Iteration: 159 | Loss: -1.429849 | Accuracy: 0.975000\n",
            "Iteration: 160 | Loss: -1.443911 | Accuracy: 0.975000\n",
            "Iteration: 161 | Loss: -1.457985 | Accuracy: 0.975000\n",
            "Iteration: 162 | Loss: -1.472069 | Accuracy: 0.975000\n",
            "Iteration: 163 | Loss: -1.486161 | Accuracy: 0.975000\n",
            "Iteration: 164 | Loss: -1.500261 | Accuracy: 0.975000\n",
            "Iteration: 165 | Loss: -1.514369 | Accuracy: 0.975000\n",
            "Iteration: 166 | Loss: -1.528483 | Accuracy: 0.975000\n",
            "Iteration: 167 | Loss: -1.542603 | Accuracy: 0.975000\n",
            "Iteration: 168 | Loss: -1.556729 | Accuracy: 0.975000\n",
            "Iteration: 169 | Loss: -1.570859 | Accuracy: 0.975000\n",
            "Iteration: 170 | Loss: -1.584994 | Accuracy: 0.975000\n",
            "Iteration: 171 | Loss: -1.599133 | Accuracy: 0.975000\n",
            "Iteration: 172 | Loss: -1.613275 | Accuracy: 0.975000\n",
            "Iteration: 173 | Loss: -1.627420 | Accuracy: 0.975000\n",
            "Iteration: 174 | Loss: -1.641569 | Accuracy: 0.975000\n",
            "Iteration: 175 | Loss: -1.655721 | Accuracy: 0.975000\n",
            "Iteration: 176 | Loss: -1.669876 | Accuracy: 0.975000\n",
            "Iteration: 177 | Loss: -1.684033 | Accuracy: 0.975000\n",
            "Iteration: 178 | Loss: -1.698194 | Accuracy: 0.975000\n",
            "Iteration: 179 | Loss: -1.712358 | Accuracy: 0.975000\n",
            "Iteration: 180 | Loss: -1.726524 | Accuracy: 0.975000\n",
            "Iteration: 181 | Loss: -1.740694 | Accuracy: 0.975000\n",
            "Iteration: 182 | Loss: -1.754867 | Accuracy: 0.975000\n",
            "Iteration: 183 | Loss: -1.769043 | Accuracy: 0.975000\n",
            "Iteration: 184 | Loss: -1.783222 | Accuracy: 0.975000\n",
            "Iteration: 185 | Loss: -1.797406 | Accuracy: 0.975000\n",
            "Iteration: 186 | Loss: -1.811593 | Accuracy: 0.975000\n",
            "Iteration: 187 | Loss: -1.825784 | Accuracy: 0.975000\n",
            "Iteration: 188 | Loss: -1.839980 | Accuracy: 0.975000\n",
            "Iteration: 189 | Loss: -1.854180 | Accuracy: 0.975000\n",
            "Iteration: 190 | Loss: -1.868384 | Accuracy: 0.975000\n",
            "Iteration: 191 | Loss: -1.882593 | Accuracy: 0.975000\n",
            "Iteration: 192 | Loss: -1.896807 | Accuracy: 0.975000\n",
            "Iteration: 193 | Loss: -1.911026 | Accuracy: 0.975000\n",
            "Iteration: 194 | Loss: -1.925250 | Accuracy: 0.975000\n",
            "Iteration: 195 | Loss: -1.939480 | Accuracy: 0.975000\n",
            "Iteration: 196 | Loss: -1.953715 | Accuracy: 0.975000\n",
            "Iteration: 197 | Loss: -1.967956 | Accuracy: 0.975000\n",
            "Iteration: 198 | Loss: -1.982203 | Accuracy: 0.975000\n",
            "Iteration: 199 | Loss: -1.996456 | Accuracy: 0.975000\n",
            "Iteration: 200 | Loss: -2.010715 | Accuracy: 0.975000\n",
            "Iteration: 201 | Loss: -2.024981 | Accuracy: 0.975000\n",
            "Iteration: 202 | Loss: -2.039253 | Accuracy: 0.975000\n",
            "Iteration: 203 | Loss: -2.053532 | Accuracy: 0.975000\n",
            "Iteration: 204 | Loss: -2.067818 | Accuracy: 0.975000\n",
            "Iteration: 205 | Loss: -2.082110 | Accuracy: 0.975000\n",
            "Iteration: 206 | Loss: -2.096410 | Accuracy: 0.975000\n",
            "Iteration: 207 | Loss: -2.110717 | Accuracy: 0.975000\n",
            "Iteration: 208 | Loss: -2.125031 | Accuracy: 0.975000\n",
            "Iteration: 209 | Loss: -2.139354 | Accuracy: 0.975000\n",
            "Iteration: 210 | Loss: -2.153683 | Accuracy: 0.975000\n",
            "Iteration: 211 | Loss: -2.168021 | Accuracy: 0.980000\n",
            "Iteration: 212 | Loss: -2.182366 | Accuracy: 0.980000\n",
            "Iteration: 213 | Loss: -2.196720 | Accuracy: 0.980000\n",
            "Iteration: 214 | Loss: -2.211082 | Accuracy: 0.980000\n",
            "Iteration: 215 | Loss: -2.225452 | Accuracy: 0.985000\n",
            "Iteration: 216 | Loss: -2.239831 | Accuracy: 0.985000\n",
            "Iteration: 217 | Loss: -2.254218 | Accuracy: 0.985000\n",
            "Iteration: 218 | Loss: -2.268614 | Accuracy: 0.985000\n",
            "Iteration: 219 | Loss: -2.283019 | Accuracy: 0.985000\n",
            "Iteration: 220 | Loss: -2.297433 | Accuracy: 0.985000\n",
            "Iteration: 221 | Loss: -2.311856 | Accuracy: 0.985000\n",
            "Iteration: 222 | Loss: -2.326288 | Accuracy: 0.985000\n",
            "Iteration: 223 | Loss: -2.340730 | Accuracy: 0.985000\n",
            "Iteration: 224 | Loss: -2.355181 | Accuracy: 0.980000\n",
            "Iteration: 225 | Loss: -2.369641 | Accuracy: 0.980000\n",
            "Iteration: 226 | Loss: -2.384111 | Accuracy: 0.980000\n",
            "Iteration: 227 | Loss: -2.398591 | Accuracy: 0.980000\n",
            "Iteration: 228 | Loss: -2.413080 | Accuracy: 0.980000\n",
            "Iteration: 229 | Loss: -2.427578 | Accuracy: 0.980000\n",
            "Iteration: 230 | Loss: -2.442087 | Accuracy: 0.980000\n",
            "Iteration: 231 | Loss: -2.456606 | Accuracy: 0.980000\n",
            "Iteration: 232 | Loss: -2.471134 | Accuracy: 0.980000\n",
            "Iteration: 233 | Loss: -2.485673 | Accuracy: 0.980000\n",
            "Iteration: 234 | Loss: -2.500222 | Accuracy: 0.980000\n",
            "Iteration: 235 | Loss: -2.514781 | Accuracy: 0.980000\n",
            "Iteration: 236 | Loss: -2.529351 | Accuracy: 0.980000\n",
            "Iteration: 237 | Loss: -2.543931 | Accuracy: 0.980000\n",
            "Iteration: 238 | Loss: -2.558520 | Accuracy: 0.980000\n",
            "Iteration: 239 | Loss: -2.573121 | Accuracy: 0.980000\n",
            "Iteration: 240 | Loss: -2.587731 | Accuracy: 0.980000\n",
            "Iteration: 241 | Loss: -2.602353 | Accuracy: 0.980000\n",
            "Iteration: 242 | Loss: -2.616985 | Accuracy: 0.980000\n",
            "Iteration: 243 | Loss: -2.631628 | Accuracy: 0.980000\n",
            "Iteration: 244 | Loss: -2.646281 | Accuracy: 0.980000\n",
            "Iteration: 245 | Loss: -2.660945 | Accuracy: 0.980000\n",
            "Iteration: 246 | Loss: -2.675619 | Accuracy: 0.980000\n",
            "Iteration: 247 | Loss: -2.690305 | Accuracy: 0.980000\n",
            "Iteration: 248 | Loss: -2.705002 | Accuracy: 0.980000\n",
            "Iteration: 249 | Loss: -2.719709 | Accuracy: 0.980000\n",
            "Iteration: 250 | Loss: -2.734427 | Accuracy: 0.980000\n",
            "Iteration: 251 | Loss: -2.749156 | Accuracy: 0.980000\n",
            "Iteration: 252 | Loss: -2.763896 | Accuracy: 0.980000\n",
            "Iteration: 253 | Loss: -2.778647 | Accuracy: 0.975000\n",
            "Iteration: 254 | Loss: -2.793410 | Accuracy: 0.975000\n",
            "Iteration: 255 | Loss: -2.808183 | Accuracy: 0.975000\n",
            "Iteration: 256 | Loss: -2.822968 | Accuracy: 0.975000\n",
            "Iteration: 257 | Loss: -2.837763 | Accuracy: 0.975000\n",
            "Iteration: 258 | Loss: -2.852570 | Accuracy: 0.975000\n",
            "Iteration: 259 | Loss: -2.867388 | Accuracy: 0.975000\n",
            "Iteration: 260 | Loss: -2.882217 | Accuracy: 0.975000\n",
            "Iteration: 261 | Loss: -2.897058 | Accuracy: 0.975000\n",
            "Iteration: 262 | Loss: -2.911909 | Accuracy: 0.975000\n",
            "Iteration: 263 | Loss: -2.926772 | Accuracy: 0.975000\n",
            "Iteration: 264 | Loss: -2.941647 | Accuracy: 0.975000\n",
            "Iteration: 265 | Loss: -2.956532 | Accuracy: 0.975000\n",
            "Iteration: 266 | Loss: -2.971430 | Accuracy: 0.975000\n",
            "Iteration: 267 | Loss: -2.986338 | Accuracy: 0.975000\n",
            "Iteration: 268 | Loss: -3.001258 | Accuracy: 0.975000\n",
            "Iteration: 269 | Loss: -3.016190 | Accuracy: 0.975000\n",
            "Iteration: 270 | Loss: -3.031133 | Accuracy: 0.975000\n",
            "Iteration: 271 | Loss: -3.046088 | Accuracy: 0.975000\n",
            "Iteration: 272 | Loss: -3.061054 | Accuracy: 0.975000\n",
            "Iteration: 273 | Loss: -3.076032 | Accuracy: 0.975000\n",
            "Iteration: 274 | Loss: -3.091021 | Accuracy: 0.975000\n",
            "Iteration: 275 | Loss: -3.106022 | Accuracy: 0.975000\n",
            "Iteration: 276 | Loss: -3.121034 | Accuracy: 0.975000\n",
            "Iteration: 277 | Loss: -3.136058 | Accuracy: 0.975000\n",
            "Iteration: 278 | Loss: -3.151094 | Accuracy: 0.975000\n",
            "Iteration: 279 | Loss: -3.166141 | Accuracy: 0.975000\n",
            "Iteration: 280 | Loss: -3.181200 | Accuracy: 0.975000\n",
            "Iteration: 281 | Loss: -3.196270 | Accuracy: 0.975000\n",
            "Iteration: 282 | Loss: -3.211353 | Accuracy: 0.975000\n",
            "Iteration: 283 | Loss: -3.226446 | Accuracy: 0.975000\n",
            "Iteration: 284 | Loss: -3.241552 | Accuracy: 0.975000\n",
            "Iteration: 285 | Loss: -3.256669 | Accuracy: 0.975000\n",
            "Iteration: 286 | Loss: -3.271797 | Accuracy: 0.975000\n",
            "Iteration: 287 | Loss: -3.286938 | Accuracy: 0.975000\n",
            "Iteration: 288 | Loss: -3.302090 | Accuracy: 0.975000\n",
            "Iteration: 289 | Loss: -3.317254 | Accuracy: 0.975000\n",
            "Iteration: 290 | Loss: -3.332429 | Accuracy: 0.975000\n",
            "Iteration: 291 | Loss: -3.347616 | Accuracy: 0.975000\n",
            "Iteration: 292 | Loss: -3.362815 | Accuracy: 0.975000\n",
            "Iteration: 293 | Loss: -3.378026 | Accuracy: 0.975000\n",
            "Iteration: 294 | Loss: -3.393248 | Accuracy: 0.975000\n",
            "Iteration: 295 | Loss: -3.408482 | Accuracy: 0.975000\n",
            "Iteration: 296 | Loss: -3.423728 | Accuracy: 0.975000\n",
            "Iteration: 297 | Loss: -3.438985 | Accuracy: 0.975000\n",
            "Iteration: 298 | Loss: -3.454254 | Accuracy: 0.975000\n",
            "Iteration: 299 | Loss: -3.469534 | Accuracy: 0.975000\n",
            "Iteration: 300 | Loss: -3.484827 | Accuracy: 0.975000\n",
            "Iteration: 301 | Loss: -3.500130 | Accuracy: 0.975000\n",
            "Iteration: 302 | Loss: -3.515446 | Accuracy: 0.975000\n",
            "Iteration: 303 | Loss: -3.530773 | Accuracy: 0.975000\n",
            "Iteration: 304 | Loss: -3.546112 | Accuracy: 0.975000\n",
            "Iteration: 305 | Loss: -3.561462 | Accuracy: 0.975000\n",
            "Iteration: 306 | Loss: -3.576824 | Accuracy: 0.975000\n",
            "Iteration: 307 | Loss: -3.592198 | Accuracy: 0.975000\n",
            "Iteration: 308 | Loss: -3.607583 | Accuracy: 0.975000\n",
            "Iteration: 309 | Loss: -3.622980 | Accuracy: 0.975000\n",
            "Iteration: 310 | Loss: -3.638388 | Accuracy: 0.975000\n",
            "Iteration: 311 | Loss: -3.653808 | Accuracy: 0.975000\n",
            "Iteration: 312 | Loss: -3.669240 | Accuracy: 0.975000\n",
            "Iteration: 313 | Loss: -3.684683 | Accuracy: 0.975000\n",
            "Iteration: 314 | Loss: -3.700137 | Accuracy: 0.975000\n",
            "Iteration: 315 | Loss: -3.715603 | Accuracy: 0.975000\n",
            "Iteration: 316 | Loss: -3.731080 | Accuracy: 0.975000\n",
            "Iteration: 317 | Loss: -3.746570 | Accuracy: 0.975000\n",
            "Iteration: 318 | Loss: -3.762070 | Accuracy: 0.975000\n",
            "Iteration: 319 | Loss: -3.777582 | Accuracy: 0.975000\n",
            "Iteration: 320 | Loss: -3.793105 | Accuracy: 0.975000\n",
            "Iteration: 321 | Loss: -3.808639 | Accuracy: 0.975000\n",
            "Iteration: 322 | Loss: -3.824185 | Accuracy: 0.975000\n",
            "Iteration: 323 | Loss: -3.839743 | Accuracy: 0.975000\n",
            "Iteration: 324 | Loss: -3.855312 | Accuracy: 0.975000\n",
            "Iteration: 325 | Loss: -3.870892 | Accuracy: 0.975000\n",
            "Iteration: 326 | Loss: -3.886483 | Accuracy: 0.975000\n",
            "Iteration: 327 | Loss: -3.902086 | Accuracy: 0.975000\n",
            "Iteration: 328 | Loss: -3.917700 | Accuracy: 0.975000\n",
            "Iteration: 329 | Loss: -3.933325 | Accuracy: 0.975000\n",
            "Iteration: 330 | Loss: -3.948961 | Accuracy: 0.975000\n",
            "Iteration: 331 | Loss: -3.964609 | Accuracy: 0.975000\n",
            "Iteration: 332 | Loss: -3.980267 | Accuracy: 0.975000\n",
            "Iteration: 333 | Loss: -3.995938 | Accuracy: 0.975000\n",
            "Iteration: 334 | Loss: -4.011619 | Accuracy: 0.975000\n",
            "Iteration: 335 | Loss: -4.027311 | Accuracy: 0.975000\n",
            "Iteration: 336 | Loss: -4.043015 | Accuracy: 0.975000\n",
            "Iteration: 337 | Loss: -4.058729 | Accuracy: 0.975000\n",
            "Iteration: 338 | Loss: -4.074455 | Accuracy: 0.975000\n",
            "Iteration: 339 | Loss: -4.090190 | Accuracy: 0.975000\n",
            "Iteration: 340 | Loss: -4.105939 | Accuracy: 0.975000\n",
            "Iteration: 341 | Loss: -4.121697 | Accuracy: 0.975000\n",
            "Iteration: 342 | Loss: -4.137467 | Accuracy: 0.975000\n",
            "Iteration: 343 | Loss: -4.153248 | Accuracy: 0.975000\n",
            "Iteration: 344 | Loss: -4.169039 | Accuracy: 0.975000\n",
            "Iteration: 345 | Loss: -4.184842 | Accuracy: 0.975000\n",
            "Iteration: 346 | Loss: -4.200655 | Accuracy: 0.975000\n",
            "Iteration: 347 | Loss: -4.216479 | Accuracy: 0.975000\n",
            "Iteration: 348 | Loss: -4.232314 | Accuracy: 0.975000\n",
            "Iteration: 349 | Loss: -4.248160 | Accuracy: 0.975000\n",
            "Iteration: 350 | Loss: -4.264017 | Accuracy: 0.975000\n",
            "Iteration: 351 | Loss: -4.279884 | Accuracy: 0.975000\n",
            "Iteration: 352 | Loss: -4.295763 | Accuracy: 0.975000\n",
            "Iteration: 353 | Loss: -4.311652 | Accuracy: 0.975000\n",
            "Iteration: 354 | Loss: -4.327552 | Accuracy: 0.975000\n",
            "Iteration: 355 | Loss: -4.343462 | Accuracy: 0.975000\n",
            "Iteration: 356 | Loss: -4.359383 | Accuracy: 0.975000\n",
            "Iteration: 357 | Loss: -4.375315 | Accuracy: 0.975000\n",
            "Iteration: 358 | Loss: -4.391257 | Accuracy: 0.975000\n",
            "Iteration: 359 | Loss: -4.407210 | Accuracy: 0.975000\n",
            "Iteration: 360 | Loss: -4.423174 | Accuracy: 0.975000\n",
            "Iteration: 361 | Loss: -4.439148 | Accuracy: 0.975000\n",
            "Iteration: 362 | Loss: -4.455133 | Accuracy: 0.975000\n",
            "Iteration: 363 | Loss: -4.471128 | Accuracy: 0.975000\n",
            "Iteration: 364 | Loss: -4.487134 | Accuracy: 0.975000\n",
            "Iteration: 365 | Loss: -4.503150 | Accuracy: 0.975000\n",
            "Iteration: 366 | Loss: -4.519177 | Accuracy: 0.975000\n",
            "Iteration: 367 | Loss: -4.535214 | Accuracy: 0.975000\n",
            "Iteration: 368 | Loss: -4.551262 | Accuracy: 0.975000\n",
            "Iteration: 369 | Loss: -4.567320 | Accuracy: 0.975000\n",
            "Iteration: 370 | Loss: -4.583388 | Accuracy: 0.975000\n",
            "Iteration: 371 | Loss: -4.599467 | Accuracy: 0.975000\n",
            "Iteration: 372 | Loss: -4.615556 | Accuracy: 0.975000\n",
            "Iteration: 373 | Loss: -4.631655 | Accuracy: 0.975000\n",
            "Iteration: 374 | Loss: -4.647765 | Accuracy: 0.975000\n",
            "Iteration: 375 | Loss: -4.663885 | Accuracy: 0.975000\n",
            "Iteration: 376 | Loss: -4.680015 | Accuracy: 0.975000\n",
            "Iteration: 377 | Loss: -4.696155 | Accuracy: 0.975000\n",
            "Iteration: 378 | Loss: -4.712306 | Accuracy: 0.975000\n",
            "Iteration: 379 | Loss: -4.728467 | Accuracy: 0.975000\n",
            "Iteration: 380 | Loss: -4.744638 | Accuracy: 0.975000\n",
            "Iteration: 381 | Loss: -4.760818 | Accuracy: 0.975000\n",
            "Iteration: 382 | Loss: -4.777010 | Accuracy: 0.975000\n",
            "Iteration: 383 | Loss: -4.793211 | Accuracy: 0.975000\n",
            "Iteration: 384 | Loss: -4.809422 | Accuracy: 0.975000\n",
            "Iteration: 385 | Loss: -4.825644 | Accuracy: 0.975000\n",
            "Iteration: 386 | Loss: -4.841876 | Accuracy: 0.975000\n",
            "Iteration: 387 | Loss: -4.858117 | Accuracy: 0.975000\n",
            "Iteration: 388 | Loss: -4.874369 | Accuracy: 0.975000\n",
            "Iteration: 389 | Loss: -4.890630 | Accuracy: 0.975000\n",
            "Iteration: 390 | Loss: -4.906901 | Accuracy: 0.975000\n",
            "Iteration: 391 | Loss: -4.923182 | Accuracy: 0.975000\n",
            "Iteration: 392 | Loss: -4.939474 | Accuracy: 0.975000\n",
            "Iteration: 393 | Loss: -4.955775 | Accuracy: 0.975000\n",
            "Iteration: 394 | Loss: -4.972086 | Accuracy: 0.975000\n",
            "Iteration: 395 | Loss: -4.988406 | Accuracy: 0.975000\n",
            "Iteration: 396 | Loss: -5.004737 | Accuracy: 0.975000\n",
            "Iteration: 397 | Loss: -5.021077 | Accuracy: 0.975000\n",
            "Iteration: 398 | Loss: -5.037428 | Accuracy: 0.975000\n",
            "Iteration: 399 | Loss: -5.053788 | Accuracy: 0.975000\n",
            "Iteration: 400 | Loss: -5.070158 | Accuracy: 0.975000\n",
            "Iteration: 401 | Loss: -5.086536 | Accuracy: 0.975000\n",
            "Iteration: 402 | Loss: -5.102926 | Accuracy: 0.975000\n",
            "Iteration: 403 | Loss: -5.119326 | Accuracy: 0.975000\n",
            "Iteration: 404 | Loss: -5.135734 | Accuracy: 0.975000\n",
            "Iteration: 405 | Loss: -5.152153 | Accuracy: 0.975000\n",
            "Iteration: 406 | Loss: -5.168580 | Accuracy: 0.975000\n",
            "Iteration: 407 | Loss: -5.185016 | Accuracy: 0.975000\n",
            "Iteration: 408 | Loss: -5.201463 | Accuracy: 0.975000\n",
            "Iteration: 409 | Loss: -5.217918 | Accuracy: 0.975000\n",
            "Iteration: 410 | Loss: -5.234381 | Accuracy: 0.975000\n",
            "Iteration: 411 | Loss: -5.250849 | Accuracy: 0.975000\n",
            "Iteration: 412 | Loss: -5.267316 | Accuracy: 0.975000\n",
            "Iteration: 413 | Loss: -5.283763 | Accuracy: 0.975000\n",
            "Iteration: 414 | Loss: -5.300155 | Accuracy: 0.970000\n",
            "Iteration: 415 | Loss: -5.316375 | Accuracy: 0.975000\n",
            "Iteration: 416 | Loss: -5.332287 | Accuracy: 0.970000\n",
            "Iteration: 417 | Loss: -5.347515 | Accuracy: 0.970000\n",
            "Iteration: 418 | Loss: -5.363245 | Accuracy: 0.970000\n",
            "Iteration: 419 | Loss: -5.380043 | Accuracy: 0.965000\n",
            "Iteration: 420 | Loss: -5.399040 | Accuracy: 0.970000\n",
            "Iteration: 421 | Loss: -5.415752 | Accuracy: 0.970000\n",
            "Iteration: 422 | Loss: -5.430700 | Accuracy: 0.970000\n",
            "Iteration: 423 | Loss: -5.447985 | Accuracy: 0.965000\n",
            "Iteration: 424 | Loss: -5.465796 | Accuracy: 0.975000\n",
            "Iteration: 425 | Loss: -5.481714 | Accuracy: 0.970000\n",
            "Iteration: 426 | Loss: -5.497926 | Accuracy: 0.970000\n",
            "Iteration: 427 | Loss: -5.515395 | Accuracy: 0.970000\n",
            "Iteration: 428 | Loss: -5.532039 | Accuracy: 0.975000\n",
            "Iteration: 429 | Loss: -5.548075 | Accuracy: 0.970000\n",
            "Iteration: 430 | Loss: -5.565138 | Accuracy: 0.970000\n",
            "Iteration: 431 | Loss: -5.582091 | Accuracy: 0.975000\n",
            "Iteration: 432 | Loss: -5.598239 | Accuracy: 0.970000\n",
            "Iteration: 433 | Loss: -5.615062 | Accuracy: 0.970000\n",
            "Iteration: 434 | Loss: -5.632106 | Accuracy: 0.975000\n",
            "Iteration: 435 | Loss: -5.648413 | Accuracy: 0.970000\n",
            "Iteration: 436 | Loss: -5.665119 | Accuracy: 0.970000\n",
            "Iteration: 437 | Loss: -5.682169 | Accuracy: 0.975000\n",
            "Iteration: 438 | Loss: -5.698619 | Accuracy: 0.970000\n",
            "Iteration: 439 | Loss: -5.715278 | Accuracy: 0.970000\n",
            "Iteration: 440 | Loss: -5.732304 | Accuracy: 0.975000\n",
            "Iteration: 441 | Loss: -5.748873 | Accuracy: 0.970000\n",
            "Iteration: 442 | Loss: -5.765522 | Accuracy: 0.970000\n",
            "Iteration: 443 | Loss: -5.782516 | Accuracy: 0.975000\n",
            "Iteration: 444 | Loss: -5.799185 | Accuracy: 0.970000\n",
            "Iteration: 445 | Loss: -5.815848 | Accuracy: 0.970000\n",
            "Iteration: 446 | Loss: -5.832807 | Accuracy: 0.975000\n",
            "Iteration: 447 | Loss: -5.849560 | Accuracy: 0.970000\n",
            "Iteration: 448 | Loss: -5.866248 | Accuracy: 0.970000\n",
            "Iteration: 449 | Loss: -5.883177 | Accuracy: 0.975000\n",
            "Iteration: 450 | Loss: -5.899998 | Accuracy: 0.975000\n",
            "Iteration: 451 | Loss: -5.916724 | Accuracy: 0.970000\n",
            "Iteration: 452 | Loss: -5.933627 | Accuracy: 0.970000\n",
            "Iteration: 453 | Loss: -5.950501 | Accuracy: 0.975000\n",
            "Iteration: 454 | Loss: -5.967275 | Accuracy: 0.970000\n",
            "Iteration: 455 | Loss: -5.984158 | Accuracy: 0.970000\n",
            "Iteration: 456 | Loss: -6.001071 | Accuracy: 0.975000\n",
            "Iteration: 457 | Loss: -6.017898 | Accuracy: 0.970000\n",
            "Iteration: 458 | Loss: -6.034773 | Accuracy: 0.970000\n",
            "Iteration: 459 | Loss: -6.051707 | Accuracy: 0.970000\n",
            "Iteration: 460 | Loss: -6.068589 | Accuracy: 0.970000\n",
            "Iteration: 461 | Loss: -6.085472 | Accuracy: 0.970000\n",
            "Iteration: 462 | Loss: -6.102416 | Accuracy: 0.970000\n",
            "Iteration: 463 | Loss: -6.119347 | Accuracy: 0.970000\n",
            "Iteration: 464 | Loss: -6.136254 | Accuracy: 0.975000\n",
            "Iteration: 465 | Loss: -6.153203 | Accuracy: 0.970000\n",
            "Iteration: 466 | Loss: -6.170170 | Accuracy: 0.970000\n",
            "Iteration: 467 | Loss: -6.187114 | Accuracy: 0.970000\n",
            "Iteration: 468 | Loss: -6.204073 | Accuracy: 0.970000\n",
            "Iteration: 469 | Loss: -6.221064 | Accuracy: 0.970000\n",
            "Iteration: 470 | Loss: -6.238047 | Accuracy: 0.970000\n",
            "Iteration: 471 | Loss: -6.255026 | Accuracy: 0.970000\n",
            "Iteration: 472 | Loss: -6.272032 | Accuracy: 0.970000\n",
            "Iteration: 473 | Loss: -6.289049 | Accuracy: 0.970000\n",
            "Iteration: 474 | Loss: -6.306058 | Accuracy: 0.970000\n",
            "Iteration: 475 | Loss: -6.323080 | Accuracy: 0.970000\n",
            "Iteration: 476 | Loss: -6.340121 | Accuracy: 0.970000\n",
            "Iteration: 477 | Loss: -6.357164 | Accuracy: 0.970000\n",
            "Iteration: 478 | Loss: -6.374208 | Accuracy: 0.970000\n",
            "Iteration: 479 | Loss: -6.391267 | Accuracy: 0.970000\n",
            "Iteration: 480 | Loss: -6.408339 | Accuracy: 0.970000\n",
            "Iteration: 481 | Loss: -6.425413 | Accuracy: 0.970000\n",
            "Iteration: 482 | Loss: -6.442492 | Accuracy: 0.970000\n",
            "Iteration: 483 | Loss: -6.459587 | Accuracy: 0.970000\n",
            "Iteration: 484 | Loss: -6.476689 | Accuracy: 0.970000\n",
            "Iteration: 485 | Loss: -6.493794 | Accuracy: 0.970000\n",
            "Iteration: 486 | Loss: -6.510910 | Accuracy: 0.970000\n",
            "Iteration: 487 | Loss: -6.528037 | Accuracy: 0.970000\n",
            "Iteration: 488 | Loss: -6.545170 | Accuracy: 0.970000\n",
            "Iteration: 489 | Loss: -6.562309 | Accuracy: 0.970000\n",
            "Iteration: 490 | Loss: -6.579457 | Accuracy: 0.970000\n",
            "Iteration: 491 | Loss: -6.596616 | Accuracy: 0.970000\n",
            "Iteration: 492 | Loss: -6.613783 | Accuracy: 0.970000\n",
            "Iteration: 493 | Loss: -6.630954 | Accuracy: 0.970000\n",
            "Iteration: 494 | Loss: -6.648137 | Accuracy: 0.970000\n",
            "Iteration: 495 | Loss: -6.665328 | Accuracy: 0.970000\n",
            "Iteration: 496 | Loss: -6.682525 | Accuracy: 0.970000\n",
            "Iteration: 497 | Loss: -6.699730 | Accuracy: 0.970000\n",
            "Iteration: 498 | Loss: -6.716944 | Accuracy: 0.970000\n",
            "Iteration: 499 | Loss: -6.734167 | Accuracy: 0.970000\n",
            "Iteration: 500 | Loss: -6.751397 | Accuracy: 0.970000\n",
            "Iteration: 501 | Loss: -6.768634 | Accuracy: 0.970000\n",
            "Iteration: 502 | Loss: -6.785881 | Accuracy: 0.970000\n",
            "Iteration: 503 | Loss: -6.803134 | Accuracy: 0.970000\n",
            "Iteration: 504 | Loss: -6.820397 | Accuracy: 0.970000\n",
            "Iteration: 505 | Loss: -6.837666 | Accuracy: 0.970000\n",
            "Iteration: 506 | Loss: -6.854944 | Accuracy: 0.970000\n",
            "Iteration: 507 | Loss: -6.872230 | Accuracy: 0.970000\n",
            "Iteration: 508 | Loss: -6.889524 | Accuracy: 0.970000\n",
            "Iteration: 509 | Loss: -6.906826 | Accuracy: 0.970000\n",
            "Iteration: 510 | Loss: -6.924135 | Accuracy: 0.970000\n",
            "Iteration: 511 | Loss: -6.941453 | Accuracy: 0.970000\n",
            "Iteration: 512 | Loss: -6.958778 | Accuracy: 0.970000\n",
            "Iteration: 513 | Loss: -6.976111 | Accuracy: 0.970000\n",
            "Iteration: 514 | Loss: -6.993452 | Accuracy: 0.970000\n",
            "Iteration: 515 | Loss: -7.010800 | Accuracy: 0.970000\n",
            "Iteration: 516 | Loss: -7.028157 | Accuracy: 0.970000\n",
            "Iteration: 517 | Loss: -7.045522 | Accuracy: 0.970000\n",
            "Iteration: 518 | Loss: -7.062893 | Accuracy: 0.970000\n",
            "Iteration: 519 | Loss: -7.080273 | Accuracy: 0.970000\n",
            "Iteration: 520 | Loss: -7.097662 | Accuracy: 0.970000\n",
            "Iteration: 521 | Loss: -7.115057 | Accuracy: 0.970000\n",
            "Iteration: 522 | Loss: -7.132460 | Accuracy: 0.970000\n",
            "Iteration: 523 | Loss: -7.149871 | Accuracy: 0.970000\n",
            "Iteration: 524 | Loss: -7.167289 | Accuracy: 0.970000\n",
            "Iteration: 525 | Loss: -7.184716 | Accuracy: 0.965000\n",
            "Iteration: 526 | Loss: -7.202149 | Accuracy: 0.965000\n",
            "Iteration: 527 | Loss: -7.219591 | Accuracy: 0.965000\n",
            "Iteration: 528 | Loss: -7.237041 | Accuracy: 0.965000\n",
            "Iteration: 529 | Loss: -7.254498 | Accuracy: 0.965000\n",
            "Iteration: 530 | Loss: -7.271962 | Accuracy: 0.965000\n",
            "Iteration: 531 | Loss: -7.289433 | Accuracy: 0.965000\n",
            "Iteration: 532 | Loss: -7.306914 | Accuracy: 0.965000\n",
            "Iteration: 533 | Loss: -7.324401 | Accuracy: 0.965000\n",
            "Iteration: 534 | Loss: -7.341896 | Accuracy: 0.965000\n",
            "Iteration: 535 | Loss: -7.359398 | Accuracy: 0.965000\n",
            "Iteration: 536 | Loss: -7.376908 | Accuracy: 0.965000\n",
            "Iteration: 537 | Loss: -7.394425 | Accuracy: 0.965000\n",
            "Iteration: 538 | Loss: -7.411951 | Accuracy: 0.965000\n",
            "Iteration: 539 | Loss: -7.429482 | Accuracy: 0.960000\n",
            "Iteration: 540 | Loss: -7.447024 | Accuracy: 0.960000\n",
            "Iteration: 541 | Loss: -7.464571 | Accuracy: 0.960000\n",
            "Iteration: 542 | Loss: -7.482125 | Accuracy: 0.960000\n",
            "Iteration: 543 | Loss: -7.499688 | Accuracy: 0.960000\n",
            "Iteration: 544 | Loss: -7.517258 | Accuracy: 0.960000\n",
            "Iteration: 545 | Loss: -7.534835 | Accuracy: 0.960000\n",
            "Iteration: 546 | Loss: -7.552419 | Accuracy: 0.960000\n",
            "Iteration: 547 | Loss: -7.570011 | Accuracy: 0.960000\n",
            "Iteration: 548 | Loss: -7.587611 | Accuracy: 0.960000\n",
            "Iteration: 549 | Loss: -7.605218 | Accuracy: 0.960000\n",
            "Iteration: 550 | Loss: -7.622832 | Accuracy: 0.960000\n",
            "Iteration: 551 | Loss: -7.640454 | Accuracy: 0.960000\n",
            "Iteration: 552 | Loss: -7.658082 | Accuracy: 0.960000\n",
            "Iteration: 553 | Loss: -7.675719 | Accuracy: 0.960000\n",
            "Iteration: 554 | Loss: -7.693363 | Accuracy: 0.960000\n",
            "Iteration: 555 | Loss: -7.711013 | Accuracy: 0.960000\n",
            "Iteration: 556 | Loss: -7.728672 | Accuracy: 0.960000\n",
            "Iteration: 557 | Loss: -7.746337 | Accuracy: 0.960000\n",
            "Iteration: 558 | Loss: -7.764010 | Accuracy: 0.960000\n",
            "Iteration: 559 | Loss: -7.781690 | Accuracy: 0.960000\n",
            "Iteration: 560 | Loss: -7.799377 | Accuracy: 0.960000\n",
            "Iteration: 561 | Loss: -7.817072 | Accuracy: 0.960000\n",
            "Iteration: 562 | Loss: -7.834773 | Accuracy: 0.960000\n",
            "Iteration: 563 | Loss: -7.852482 | Accuracy: 0.960000\n",
            "Iteration: 564 | Loss: -7.870199 | Accuracy: 0.960000\n",
            "Iteration: 565 | Loss: -7.887922 | Accuracy: 0.960000\n",
            "Iteration: 566 | Loss: -7.905653 | Accuracy: 0.960000\n",
            "Iteration: 567 | Loss: -7.923390 | Accuracy: 0.960000\n",
            "Iteration: 568 | Loss: -7.941135 | Accuracy: 0.960000\n",
            "Iteration: 569 | Loss: -7.958887 | Accuracy: 0.960000\n",
            "Iteration: 570 | Loss: -7.976646 | Accuracy: 0.960000\n",
            "Iteration: 571 | Loss: -7.994412 | Accuracy: 0.960000\n",
            "Iteration: 572 | Loss: -8.012186 | Accuracy: 0.960000\n",
            "Iteration: 573 | Loss: -8.029966 | Accuracy: 0.960000\n",
            "Iteration: 574 | Loss: -8.047754 | Accuracy: 0.960000\n",
            "Iteration: 575 | Loss: -8.065548 | Accuracy: 0.960000\n",
            "Iteration: 576 | Loss: -8.083349 | Accuracy: 0.960000\n",
            "Iteration: 577 | Loss: -8.101159 | Accuracy: 0.960000\n",
            "Iteration: 578 | Loss: -8.118974 | Accuracy: 0.960000\n",
            "Iteration: 579 | Loss: -8.136797 | Accuracy: 0.960000\n",
            "Iteration: 580 | Loss: -8.154626 | Accuracy: 0.960000\n",
            "Iteration: 581 | Loss: -8.172463 | Accuracy: 0.960000\n",
            "Iteration: 582 | Loss: -8.190308 | Accuracy: 0.960000\n",
            "Iteration: 583 | Loss: -8.208158 | Accuracy: 0.960000\n",
            "Iteration: 584 | Loss: -8.226015 | Accuracy: 0.960000\n",
            "Iteration: 585 | Loss: -8.243880 | Accuracy: 0.960000\n",
            "Iteration: 586 | Loss: -8.261753 | Accuracy: 0.960000\n",
            "Iteration: 587 | Loss: -8.279632 | Accuracy: 0.960000\n",
            "Iteration: 588 | Loss: -8.297517 | Accuracy: 0.960000\n",
            "Iteration: 589 | Loss: -8.315410 | Accuracy: 0.960000\n",
            "Iteration: 590 | Loss: -8.333308 | Accuracy: 0.960000\n",
            "Iteration: 591 | Loss: -8.351215 | Accuracy: 0.960000\n",
            "Iteration: 592 | Loss: -8.369128 | Accuracy: 0.960000\n",
            "Iteration: 593 | Loss: -8.387049 | Accuracy: 0.960000\n",
            "Iteration: 594 | Loss: -8.404975 | Accuracy: 0.960000\n",
            "Iteration: 595 | Loss: -8.422908 | Accuracy: 0.960000\n",
            "Iteration: 596 | Loss: -8.440850 | Accuracy: 0.960000\n",
            "Iteration: 597 | Loss: -8.458797 | Accuracy: 0.960000\n",
            "Iteration: 598 | Loss: -8.476752 | Accuracy: 0.960000\n",
            "Iteration: 599 | Loss: -8.494713 | Accuracy: 0.960000\n",
            "Iteration: 600 | Loss: -8.512681 | Accuracy: 0.960000\n",
            "Iteration: 601 | Loss: -8.530655 | Accuracy: 0.960000\n",
            "Iteration: 602 | Loss: -8.548637 | Accuracy: 0.960000\n",
            "Iteration: 603 | Loss: -8.566625 | Accuracy: 0.960000\n",
            "Iteration: 604 | Loss: -8.584620 | Accuracy: 0.960000\n",
            "Iteration: 605 | Loss: -8.602621 | Accuracy: 0.955000\n",
            "Iteration: 606 | Loss: -8.620631 | Accuracy: 0.955000\n",
            "Iteration: 607 | Loss: -8.638646 | Accuracy: 0.955000\n",
            "Iteration: 608 | Loss: -8.656669 | Accuracy: 0.955000\n",
            "Iteration: 609 | Loss: -8.674697 | Accuracy: 0.955000\n",
            "Iteration: 610 | Loss: -8.692732 | Accuracy: 0.955000\n",
            "Iteration: 611 | Loss: -8.710773 | Accuracy: 0.955000\n",
            "Iteration: 612 | Loss: -8.728823 | Accuracy: 0.955000\n",
            "Iteration: 613 | Loss: -8.746879 | Accuracy: 0.955000\n",
            "Iteration: 614 | Loss: -8.764941 | Accuracy: 0.955000\n",
            "Iteration: 615 | Loss: -8.783009 | Accuracy: 0.955000\n",
            "Iteration: 616 | Loss: -8.801085 | Accuracy: 0.955000\n",
            "Iteration: 617 | Loss: -8.819166 | Accuracy: 0.955000\n",
            "Iteration: 618 | Loss: -8.837255 | Accuracy: 0.955000\n",
            "Iteration: 619 | Loss: -8.855350 | Accuracy: 0.955000\n",
            "Iteration: 620 | Loss: -8.873453 | Accuracy: 0.955000\n",
            "Iteration: 621 | Loss: -8.891561 | Accuracy: 0.955000\n",
            "Iteration: 622 | Loss: -8.909678 | Accuracy: 0.955000\n",
            "Iteration: 623 | Loss: -8.927798 | Accuracy: 0.955000\n",
            "Iteration: 624 | Loss: -8.945926 | Accuracy: 0.955000\n",
            "Iteration: 625 | Loss: -8.964061 | Accuracy: 0.955000\n",
            "Iteration: 626 | Loss: -8.982202 | Accuracy: 0.955000\n",
            "Iteration: 627 | Loss: -9.000350 | Accuracy: 0.955000\n",
            "Iteration: 628 | Loss: -9.018504 | Accuracy: 0.955000\n",
            "Iteration: 629 | Loss: -9.036665 | Accuracy: 0.955000\n",
            "Iteration: 630 | Loss: -9.054832 | Accuracy: 0.955000\n",
            "Iteration: 631 | Loss: -9.073007 | Accuracy: 0.955000\n",
            "Iteration: 632 | Loss: -9.091187 | Accuracy: 0.955000\n",
            "Iteration: 633 | Loss: -9.109373 | Accuracy: 0.955000\n",
            "Iteration: 634 | Loss: -9.127566 | Accuracy: 0.955000\n",
            "Iteration: 635 | Loss: -9.145766 | Accuracy: 0.955000\n",
            "Iteration: 636 | Loss: -9.163973 | Accuracy: 0.955000\n",
            "Iteration: 637 | Loss: -9.182184 | Accuracy: 0.955000\n",
            "Iteration: 638 | Loss: -9.200404 | Accuracy: 0.955000\n",
            "Iteration: 639 | Loss: -9.218629 | Accuracy: 0.955000\n",
            "Iteration: 640 | Loss: -9.236861 | Accuracy: 0.955000\n",
            "Iteration: 641 | Loss: -9.255098 | Accuracy: 0.955000\n",
            "Iteration: 642 | Loss: -9.273343 | Accuracy: 0.955000\n",
            "Iteration: 643 | Loss: -9.291593 | Accuracy: 0.955000\n",
            "Iteration: 644 | Loss: -9.309849 | Accuracy: 0.955000\n",
            "Iteration: 645 | Loss: -9.328110 | Accuracy: 0.955000\n",
            "Iteration: 646 | Loss: -9.346373 | Accuracy: 0.955000\n",
            "Iteration: 647 | Loss: -9.364639 | Accuracy: 0.950000\n",
            "Iteration: 648 | Loss: -9.382900 | Accuracy: 0.955000\n",
            "Iteration: 649 | Loss: -9.401152 | Accuracy: 0.950000\n",
            "Iteration: 650 | Loss: -9.419372 | Accuracy: 0.960000\n",
            "Iteration: 651 | Loss: -9.437547 | Accuracy: 0.950000\n",
            "Iteration: 652 | Loss: -9.455604 | Accuracy: 0.960000\n",
            "Iteration: 653 | Loss: -9.473574 | Accuracy: 0.950000\n",
            "Iteration: 654 | Loss: -9.491296 | Accuracy: 0.960000\n",
            "Iteration: 655 | Loss: -9.509293 | Accuracy: 0.960000\n",
            "Iteration: 656 | Loss: -9.527422 | Accuracy: 0.960000\n",
            "Iteration: 657 | Loss: -9.546608 | Accuracy: 0.950000\n",
            "Iteration: 658 | Loss: -9.565798 | Accuracy: 0.960000\n",
            "Iteration: 659 | Loss: -9.584404 | Accuracy: 0.955000\n",
            "Iteration: 660 | Loss: -9.602302 | Accuracy: 0.950000\n",
            "Iteration: 661 | Loss: -9.620160 | Accuracy: 0.960000\n",
            "Iteration: 662 | Loss: -9.638776 | Accuracy: 0.950000\n",
            "Iteration: 663 | Loss: -9.657659 | Accuracy: 0.960000\n",
            "Iteration: 664 | Loss: -9.676188 | Accuracy: 0.955000\n",
            "Iteration: 665 | Loss: -9.694248 | Accuracy: 0.950000\n",
            "Iteration: 666 | Loss: -9.712397 | Accuracy: 0.960000\n",
            "Iteration: 667 | Loss: -9.731050 | Accuracy: 0.950000\n",
            "Iteration: 668 | Loss: -9.749720 | Accuracy: 0.955000\n",
            "Iteration: 669 | Loss: -9.768040 | Accuracy: 0.955000\n",
            "Iteration: 670 | Loss: -9.786235 | Accuracy: 0.950000\n",
            "Iteration: 671 | Loss: -9.804666 | Accuracy: 0.960000\n",
            "Iteration: 672 | Loss: -9.823300 | Accuracy: 0.950000\n",
            "Iteration: 673 | Loss: -9.841766 | Accuracy: 0.950000\n",
            "Iteration: 674 | Loss: -9.860049 | Accuracy: 0.960000\n",
            "Iteration: 675 | Loss: -9.878443 | Accuracy: 0.950000\n",
            "Iteration: 676 | Loss: -9.896998 | Accuracy: 0.955000\n",
            "Iteration: 677 | Loss: -9.915526 | Accuracy: 0.955000\n",
            "Iteration: 678 | Loss: -9.933908 | Accuracy: 0.950000\n",
            "Iteration: 679 | Loss: -9.952295 | Accuracy: 0.955000\n",
            "Iteration: 680 | Loss: -9.970814 | Accuracy: 0.950000\n",
            "Iteration: 681 | Loss: -9.989358 | Accuracy: 0.955000\n",
            "Iteration: 682 | Loss: -10.007818 | Accuracy: 0.955000\n",
            "Iteration: 683 | Loss: -10.026243 | Accuracy: 0.950000\n",
            "Iteration: 684 | Loss: -10.044731 | Accuracy: 0.955000\n",
            "Iteration: 685 | Loss: -10.063280 | Accuracy: 0.950000\n",
            "Iteration: 686 | Loss: -10.081796 | Accuracy: 0.955000\n",
            "Iteration: 687 | Loss: -10.100265 | Accuracy: 0.955000\n",
            "Iteration: 688 | Loss: -10.118757 | Accuracy: 0.950000\n",
            "Iteration: 689 | Loss: -10.137300 | Accuracy: 0.955000\n",
            "Iteration: 690 | Loss: -10.155849 | Accuracy: 0.955000\n",
            "Iteration: 691 | Loss: -10.174369 | Accuracy: 0.950000\n",
            "Iteration: 692 | Loss: -10.192882 | Accuracy: 0.955000\n",
            "Iteration: 693 | Loss: -10.211422 | Accuracy: 0.950000\n",
            "Iteration: 694 | Loss: -10.229991 | Accuracy: 0.955000\n",
            "Iteration: 695 | Loss: -10.248550 | Accuracy: 0.955000\n",
            "Iteration: 696 | Loss: -10.267096 | Accuracy: 0.950000\n",
            "Iteration: 697 | Loss: -10.285650 | Accuracy: 0.955000\n",
            "Iteration: 698 | Loss: -10.304227 | Accuracy: 0.950000\n",
            "Iteration: 699 | Loss: -10.322816 | Accuracy: 0.955000\n",
            "Iteration: 700 | Loss: -10.341395 | Accuracy: 0.955000\n",
            "Iteration: 701 | Loss: -10.359974 | Accuracy: 0.950000\n",
            "Iteration: 702 | Loss: -10.378564 | Accuracy: 0.955000\n",
            "Iteration: 703 | Loss: -10.397170 | Accuracy: 0.955000\n",
            "Iteration: 704 | Loss: -10.415782 | Accuracy: 0.955000\n",
            "Iteration: 705 | Loss: -10.434391 | Accuracy: 0.955000\n",
            "Iteration: 706 | Loss: -10.453001 | Accuracy: 0.955000\n",
            "Iteration: 707 | Loss: -10.471623 | Accuracy: 0.955000\n",
            "Iteration: 708 | Loss: -10.490254 | Accuracy: 0.955000\n",
            "Iteration: 709 | Loss: -10.508893 | Accuracy: 0.955000\n",
            "Iteration: 710 | Loss: -10.527531 | Accuracy: 0.955000\n",
            "Iteration: 711 | Loss: -10.546172 | Accuracy: 0.955000\n",
            "Iteration: 712 | Loss: -10.564823 | Accuracy: 0.955000\n",
            "Iteration: 713 | Loss: -10.583483 | Accuracy: 0.955000\n",
            "Iteration: 714 | Loss: -10.602149 | Accuracy: 0.955000\n",
            "Iteration: 715 | Loss: -10.620815 | Accuracy: 0.955000\n",
            "Iteration: 716 | Loss: -10.639487 | Accuracy: 0.955000\n",
            "Iteration: 717 | Loss: -10.658169 | Accuracy: 0.955000\n",
            "Iteration: 718 | Loss: -10.676855 | Accuracy: 0.955000\n",
            "Iteration: 719 | Loss: -10.695548 | Accuracy: 0.955000\n",
            "Iteration: 720 | Loss: -10.714245 | Accuracy: 0.955000\n",
            "Iteration: 721 | Loss: -10.732946 | Accuracy: 0.955000\n",
            "Iteration: 722 | Loss: -10.751656 | Accuracy: 0.955000\n",
            "Iteration: 723 | Loss: -10.770371 | Accuracy: 0.955000\n",
            "Iteration: 724 | Loss: -10.789091 | Accuracy: 0.955000\n",
            "Iteration: 725 | Loss: -10.807817 | Accuracy: 0.955000\n",
            "Iteration: 726 | Loss: -10.826548 | Accuracy: 0.955000\n",
            "Iteration: 727 | Loss: -10.845284 | Accuracy: 0.955000\n",
            "Iteration: 728 | Loss: -10.864028 | Accuracy: 0.955000\n",
            "Iteration: 729 | Loss: -10.882777 | Accuracy: 0.955000\n",
            "Iteration: 730 | Loss: -10.901531 | Accuracy: 0.955000\n",
            "Iteration: 731 | Loss: -10.920291 | Accuracy: 0.955000\n",
            "Iteration: 732 | Loss: -10.939056 | Accuracy: 0.955000\n",
            "Iteration: 733 | Loss: -10.957826 | Accuracy: 0.955000\n",
            "Iteration: 734 | Loss: -10.976604 | Accuracy: 0.955000\n",
            "Iteration: 735 | Loss: -10.995385 | Accuracy: 0.955000\n",
            "Iteration: 736 | Loss: -11.014174 | Accuracy: 0.955000\n",
            "Iteration: 737 | Loss: -11.032967 | Accuracy: 0.955000\n",
            "Iteration: 738 | Loss: -11.051764 | Accuracy: 0.955000\n",
            "Iteration: 739 | Loss: -11.070573 | Accuracy: 0.955000\n",
            "Iteration: 740 | Loss: -11.089381 | Accuracy: 0.955000\n",
            "Iteration: 741 | Loss: -11.108198 | Accuracy: 0.955000\n",
            "Iteration: 742 | Loss: -11.127019 | Accuracy: 0.955000\n",
            "Iteration: 743 | Loss: -11.145846 | Accuracy: 0.955000\n",
            "Iteration: 744 | Loss: -11.164678 | Accuracy: 0.955000\n",
            "Iteration: 745 | Loss: -11.183517 | Accuracy: 0.955000\n",
            "Iteration: 746 | Loss: -11.202360 | Accuracy: 0.955000\n",
            "Iteration: 747 | Loss: -11.221209 | Accuracy: 0.955000\n",
            "Iteration: 748 | Loss: -11.240065 | Accuracy: 0.955000\n",
            "Iteration: 749 | Loss: -11.258924 | Accuracy: 0.955000\n",
            "Iteration: 750 | Loss: -11.277790 | Accuracy: 0.955000\n",
            "Iteration: 751 | Loss: -11.296660 | Accuracy: 0.955000\n",
            "Iteration: 752 | Loss: -11.315537 | Accuracy: 0.955000\n",
            "Iteration: 753 | Loss: -11.334419 | Accuracy: 0.955000\n",
            "Iteration: 754 | Loss: -11.353308 | Accuracy: 0.955000\n",
            "Iteration: 755 | Loss: -11.372199 | Accuracy: 0.955000\n",
            "Iteration: 756 | Loss: -11.391098 | Accuracy: 0.955000\n",
            "Iteration: 757 | Loss: -11.410003 | Accuracy: 0.955000\n",
            "Iteration: 758 | Loss: -11.428912 | Accuracy: 0.955000\n",
            "Iteration: 759 | Loss: -11.447825 | Accuracy: 0.955000\n",
            "Iteration: 760 | Loss: -11.466747 | Accuracy: 0.955000\n",
            "Iteration: 761 | Loss: -11.485672 | Accuracy: 0.955000\n",
            "Iteration: 762 | Loss: -11.504606 | Accuracy: 0.955000\n",
            "Iteration: 763 | Loss: -11.523541 | Accuracy: 0.955000\n",
            "Iteration: 764 | Loss: -11.542482 | Accuracy: 0.955000\n",
            "Iteration: 765 | Loss: -11.561431 | Accuracy: 0.955000\n",
            "Iteration: 766 | Loss: -11.580384 | Accuracy: 0.955000\n",
            "Iteration: 767 | Loss: -11.599342 | Accuracy: 0.955000\n",
            "Iteration: 768 | Loss: -11.618305 | Accuracy: 0.955000\n",
            "Iteration: 769 | Loss: -11.637275 | Accuracy: 0.955000\n",
            "Iteration: 770 | Loss: -11.656249 | Accuracy: 0.955000\n",
            "Iteration: 771 | Loss: -11.675228 | Accuracy: 0.955000\n",
            "Iteration: 772 | Loss: -11.694214 | Accuracy: 0.955000\n",
            "Iteration: 773 | Loss: -11.713204 | Accuracy: 0.955000\n",
            "Iteration: 774 | Loss: -11.732200 | Accuracy: 0.955000\n",
            "Iteration: 775 | Loss: -11.751201 | Accuracy: 0.955000\n",
            "Iteration: 776 | Loss: -11.770208 | Accuracy: 0.955000\n",
            "Iteration: 777 | Loss: -11.789219 | Accuracy: 0.955000\n",
            "Iteration: 778 | Loss: -11.808237 | Accuracy: 0.955000\n",
            "Iteration: 779 | Loss: -11.827259 | Accuracy: 0.955000\n",
            "Iteration: 780 | Loss: -11.846286 | Accuracy: 0.955000\n",
            "Iteration: 781 | Loss: -11.865319 | Accuracy: 0.955000\n",
            "Iteration: 782 | Loss: -11.884358 | Accuracy: 0.955000\n",
            "Iteration: 783 | Loss: -11.903400 | Accuracy: 0.955000\n",
            "Iteration: 784 | Loss: -11.922450 | Accuracy: 0.955000\n",
            "Iteration: 785 | Loss: -11.941504 | Accuracy: 0.955000\n",
            "Iteration: 786 | Loss: -11.960564 | Accuracy: 0.955000\n",
            "Iteration: 787 | Loss: -11.979629 | Accuracy: 0.955000\n",
            "Iteration: 788 | Loss: -11.998697 | Accuracy: 0.955000\n",
            "Iteration: 789 | Loss: -12.017773 | Accuracy: 0.955000\n",
            "Iteration: 790 | Loss: -12.036853 | Accuracy: 0.955000\n",
            "Iteration: 791 | Loss: -12.055938 | Accuracy: 0.955000\n",
            "Iteration: 792 | Loss: -12.075030 | Accuracy: 0.955000\n",
            "Iteration: 793 | Loss: -12.094126 | Accuracy: 0.955000\n",
            "Iteration: 794 | Loss: -12.113227 | Accuracy: 0.955000\n",
            "Iteration: 795 | Loss: -12.132334 | Accuracy: 0.955000\n",
            "Iteration: 796 | Loss: -12.151445 | Accuracy: 0.955000\n",
            "Iteration: 797 | Loss: -12.170562 | Accuracy: 0.955000\n",
            "Iteration: 798 | Loss: -12.189683 | Accuracy: 0.955000\n",
            "Iteration: 799 | Loss: -12.208810 | Accuracy: 0.955000\n",
            "Iteration: 800 | Loss: -12.227942 | Accuracy: 0.955000\n",
            "Iteration: 801 | Loss: -12.247081 | Accuracy: 0.955000\n",
            "Iteration: 802 | Loss: -12.266222 | Accuracy: 0.955000\n",
            "Iteration: 803 | Loss: -12.285369 | Accuracy: 0.955000\n",
            "Iteration: 804 | Loss: -12.304523 | Accuracy: 0.955000\n",
            "Iteration: 805 | Loss: -12.323682 | Accuracy: 0.955000\n",
            "Iteration: 806 | Loss: -12.342844 | Accuracy: 0.955000\n",
            "Iteration: 807 | Loss: -12.362011 | Accuracy: 0.955000\n",
            "Iteration: 808 | Loss: -12.381186 | Accuracy: 0.955000\n",
            "Iteration: 809 | Loss: -12.400364 | Accuracy: 0.955000\n",
            "Iteration: 810 | Loss: -12.419548 | Accuracy: 0.955000\n",
            "Iteration: 811 | Loss: -12.438737 | Accuracy: 0.955000\n",
            "Iteration: 812 | Loss: -12.457930 | Accuracy: 0.955000\n",
            "Iteration: 813 | Loss: -12.477129 | Accuracy: 0.955000\n",
            "Iteration: 814 | Loss: -12.496333 | Accuracy: 0.955000\n",
            "Iteration: 815 | Loss: -12.515543 | Accuracy: 0.955000\n",
            "Iteration: 816 | Loss: -12.534757 | Accuracy: 0.955000\n",
            "Iteration: 817 | Loss: -12.553976 | Accuracy: 0.955000\n",
            "Iteration: 818 | Loss: -12.573200 | Accuracy: 0.955000\n",
            "Iteration: 819 | Loss: -12.592429 | Accuracy: 0.955000\n",
            "Iteration: 820 | Loss: -12.611664 | Accuracy: 0.955000\n",
            "Iteration: 821 | Loss: -12.630903 | Accuracy: 0.955000\n",
            "Iteration: 822 | Loss: -12.650146 | Accuracy: 0.955000\n",
            "Iteration: 823 | Loss: -12.669395 | Accuracy: 0.955000\n",
            "Iteration: 824 | Loss: -12.688651 | Accuracy: 0.955000\n",
            "Iteration: 825 | Loss: -12.707911 | Accuracy: 0.955000\n",
            "Iteration: 826 | Loss: -12.727176 | Accuracy: 0.955000\n",
            "Iteration: 827 | Loss: -12.746446 | Accuracy: 0.955000\n",
            "Iteration: 828 | Loss: -12.765719 | Accuracy: 0.955000\n",
            "Iteration: 829 | Loss: -12.784999 | Accuracy: 0.955000\n",
            "Iteration: 830 | Loss: -12.804282 | Accuracy: 0.955000\n",
            "Iteration: 831 | Loss: -12.823572 | Accuracy: 0.955000\n",
            "Iteration: 832 | Loss: -12.842867 | Accuracy: 0.955000\n",
            "Iteration: 833 | Loss: -12.862166 | Accuracy: 0.955000\n",
            "Iteration: 834 | Loss: -12.881471 | Accuracy: 0.955000\n",
            "Iteration: 835 | Loss: -12.900780 | Accuracy: 0.955000\n",
            "Iteration: 836 | Loss: -12.920093 | Accuracy: 0.955000\n",
            "Iteration: 837 | Loss: -12.939413 | Accuracy: 0.955000\n",
            "Iteration: 838 | Loss: -12.958737 | Accuracy: 0.955000\n",
            "Iteration: 839 | Loss: -12.978067 | Accuracy: 0.955000\n",
            "Iteration: 840 | Loss: -12.997400 | Accuracy: 0.955000\n",
            "Iteration: 841 | Loss: -13.016739 | Accuracy: 0.955000\n",
            "Iteration: 842 | Loss: -13.036083 | Accuracy: 0.955000\n",
            "Iteration: 843 | Loss: -13.055430 | Accuracy: 0.955000\n",
            "Iteration: 844 | Loss: -13.074785 | Accuracy: 0.955000\n",
            "Iteration: 845 | Loss: -13.094145 | Accuracy: 0.955000\n",
            "Iteration: 846 | Loss: -13.113505 | Accuracy: 0.955000\n",
            "Iteration: 847 | Loss: -13.132874 | Accuracy: 0.955000\n",
            "Iteration: 848 | Loss: -13.152247 | Accuracy: 0.955000\n",
            "Iteration: 849 | Loss: -13.171627 | Accuracy: 0.955000\n",
            "Iteration: 850 | Loss: -13.191010 | Accuracy: 0.955000\n",
            "Iteration: 851 | Loss: -13.210398 | Accuracy: 0.955000\n",
            "Iteration: 852 | Loss: -13.229789 | Accuracy: 0.955000\n",
            "Iteration: 853 | Loss: -13.249187 | Accuracy: 0.955000\n",
            "Iteration: 854 | Loss: -13.268590 | Accuracy: 0.955000\n",
            "Iteration: 855 | Loss: -13.287996 | Accuracy: 0.955000\n",
            "Iteration: 856 | Loss: -13.307407 | Accuracy: 0.955000\n",
            "Iteration: 857 | Loss: -13.326824 | Accuracy: 0.955000\n",
            "Iteration: 858 | Loss: -13.346247 | Accuracy: 0.955000\n",
            "Iteration: 859 | Loss: -13.365672 | Accuracy: 0.955000\n",
            "Iteration: 860 | Loss: -13.385103 | Accuracy: 0.955000\n",
            "Iteration: 861 | Loss: -13.404541 | Accuracy: 0.955000\n",
            "Iteration: 862 | Loss: -13.423982 | Accuracy: 0.955000\n",
            "Iteration: 863 | Loss: -13.443427 | Accuracy: 0.955000\n",
            "Iteration: 864 | Loss: -13.462877 | Accuracy: 0.955000\n",
            "Iteration: 865 | Loss: -13.482332 | Accuracy: 0.955000\n",
            "Iteration: 866 | Loss: -13.501792 | Accuracy: 0.955000\n",
            "Iteration: 867 | Loss: -13.521256 | Accuracy: 0.955000\n",
            "Iteration: 868 | Loss: -13.540728 | Accuracy: 0.955000\n",
            "Iteration: 869 | Loss: -13.560200 | Accuracy: 0.955000\n",
            "Iteration: 870 | Loss: -13.579680 | Accuracy: 0.955000\n",
            "Iteration: 871 | Loss: -13.599164 | Accuracy: 0.955000\n",
            "Iteration: 872 | Loss: -13.618652 | Accuracy: 0.955000\n",
            "Iteration: 873 | Loss: -13.638145 | Accuracy: 0.955000\n",
            "Iteration: 874 | Loss: -13.657643 | Accuracy: 0.955000\n",
            "Iteration: 875 | Loss: -13.677146 | Accuracy: 0.955000\n",
            "Iteration: 876 | Loss: -13.696653 | Accuracy: 0.955000\n",
            "Iteration: 877 | Loss: -13.716166 | Accuracy: 0.955000\n",
            "Iteration: 878 | Loss: -13.735682 | Accuracy: 0.955000\n",
            "Iteration: 879 | Loss: -13.755204 | Accuracy: 0.955000\n",
            "Iteration: 880 | Loss: -13.774730 | Accuracy: 0.955000\n",
            "Iteration: 881 | Loss: -13.794261 | Accuracy: 0.955000\n",
            "Iteration: 882 | Loss: -13.813797 | Accuracy: 0.955000\n",
            "Iteration: 883 | Loss: -13.833337 | Accuracy: 0.955000\n",
            "Iteration: 884 | Loss: -13.852881 | Accuracy: 0.955000\n",
            "Iteration: 885 | Loss: -13.872432 | Accuracy: 0.955000\n",
            "Iteration: 886 | Loss: -13.891986 | Accuracy: 0.955000\n",
            "Iteration: 887 | Loss: -13.911545 | Accuracy: 0.955000\n",
            "Iteration: 888 | Loss: -13.931108 | Accuracy: 0.955000\n",
            "Iteration: 889 | Loss: -13.950676 | Accuracy: 0.955000\n",
            "Iteration: 890 | Loss: -13.970249 | Accuracy: 0.955000\n",
            "Iteration: 891 | Loss: -13.989827 | Accuracy: 0.955000\n",
            "Iteration: 892 | Loss: -14.009409 | Accuracy: 0.955000\n",
            "Iteration: 893 | Loss: -14.028996 | Accuracy: 0.955000\n",
            "Iteration: 894 | Loss: -14.048588 | Accuracy: 0.955000\n",
            "Iteration: 895 | Loss: -14.068183 | Accuracy: 0.955000\n",
            "Iteration: 896 | Loss: -14.087784 | Accuracy: 0.955000\n",
            "Iteration: 897 | Loss: -14.107389 | Accuracy: 0.955000\n",
            "Iteration: 898 | Loss: -14.126999 | Accuracy: 0.955000\n",
            "Iteration: 899 | Loss: -14.146615 | Accuracy: 0.955000\n",
            "Iteration: 900 | Loss: -14.166233 | Accuracy: 0.955000\n",
            "Iteration: 901 | Loss: -14.185857 | Accuracy: 0.955000\n",
            "Iteration: 902 | Loss: -14.205485 | Accuracy: 0.955000\n",
            "Iteration: 903 | Loss: -14.225118 | Accuracy: 0.955000\n",
            "Iteration: 904 | Loss: -14.244755 | Accuracy: 0.955000\n",
            "Iteration: 905 | Loss: -14.264398 | Accuracy: 0.955000\n",
            "Iteration: 906 | Loss: -14.284044 | Accuracy: 0.955000\n",
            "Iteration: 907 | Loss: -14.303695 | Accuracy: 0.955000\n",
            "Iteration: 908 | Loss: -14.323350 | Accuracy: 0.955000\n",
            "Iteration: 909 | Loss: -14.343012 | Accuracy: 0.955000\n",
            "Iteration: 910 | Loss: -14.362676 | Accuracy: 0.955000\n",
            "Iteration: 911 | Loss: -14.382345 | Accuracy: 0.955000\n",
            "Iteration: 912 | Loss: -14.402019 | Accuracy: 0.955000\n",
            "Iteration: 913 | Loss: -14.421697 | Accuracy: 0.955000\n",
            "Iteration: 914 | Loss: -14.441379 | Accuracy: 0.955000\n",
            "Iteration: 915 | Loss: -14.461067 | Accuracy: 0.955000\n",
            "Iteration: 916 | Loss: -14.480758 | Accuracy: 0.955000\n",
            "Iteration: 917 | Loss: -14.500455 | Accuracy: 0.955000\n",
            "Iteration: 918 | Loss: -14.520156 | Accuracy: 0.955000\n",
            "Iteration: 919 | Loss: -14.539860 | Accuracy: 0.955000\n",
            "Iteration: 920 | Loss: -14.559572 | Accuracy: 0.955000\n",
            "Iteration: 921 | Loss: -14.579286 | Accuracy: 0.955000\n",
            "Iteration: 922 | Loss: -14.599006 | Accuracy: 0.955000\n",
            "Iteration: 923 | Loss: -14.618728 | Accuracy: 0.955000\n",
            "Iteration: 924 | Loss: -14.638457 | Accuracy: 0.955000\n",
            "Iteration: 925 | Loss: -14.658187 | Accuracy: 0.955000\n",
            "Iteration: 926 | Loss: -14.677925 | Accuracy: 0.955000\n",
            "Iteration: 927 | Loss: -14.697666 | Accuracy: 0.955000\n",
            "Iteration: 928 | Loss: -14.717411 | Accuracy: 0.955000\n",
            "Iteration: 929 | Loss: -14.737163 | Accuracy: 0.955000\n",
            "Iteration: 930 | Loss: -14.756916 | Accuracy: 0.955000\n",
            "Iteration: 931 | Loss: -14.776676 | Accuracy: 0.955000\n",
            "Iteration: 932 | Loss: -14.796437 | Accuracy: 0.955000\n",
            "Iteration: 933 | Loss: -14.816206 | Accuracy: 0.955000\n",
            "Iteration: 934 | Loss: -14.835979 | Accuracy: 0.955000\n",
            "Iteration: 935 | Loss: -14.855755 | Accuracy: 0.955000\n",
            "Iteration: 936 | Loss: -14.875537 | Accuracy: 0.955000\n",
            "Iteration: 937 | Loss: -14.895322 | Accuracy: 0.955000\n",
            "Iteration: 938 | Loss: -14.915113 | Accuracy: 0.955000\n",
            "Iteration: 939 | Loss: -14.934907 | Accuracy: 0.955000\n",
            "Iteration: 940 | Loss: -14.954704 | Accuracy: 0.955000\n",
            "Iteration: 941 | Loss: -14.974507 | Accuracy: 0.955000\n",
            "Iteration: 942 | Loss: -14.994315 | Accuracy: 0.955000\n",
            "Iteration: 943 | Loss: -15.014129 | Accuracy: 0.950000\n",
            "Iteration: 944 | Loss: -15.033943 | Accuracy: 0.950000\n",
            "Iteration: 945 | Loss: -15.053764 | Accuracy: 0.950000\n",
            "Iteration: 946 | Loss: -15.073588 | Accuracy: 0.950000\n",
            "Iteration: 947 | Loss: -15.093419 | Accuracy: 0.950000\n",
            "Iteration: 948 | Loss: -15.113252 | Accuracy: 0.950000\n",
            "Iteration: 949 | Loss: -15.133091 | Accuracy: 0.950000\n",
            "Iteration: 950 | Loss: -15.152933 | Accuracy: 0.950000\n",
            "Iteration: 951 | Loss: -15.172780 | Accuracy: 0.950000\n",
            "Iteration: 952 | Loss: -15.192630 | Accuracy: 0.950000\n",
            "Iteration: 953 | Loss: -15.212486 | Accuracy: 0.950000\n",
            "Iteration: 954 | Loss: -15.232347 | Accuracy: 0.950000\n",
            "Iteration: 955 | Loss: -15.252210 | Accuracy: 0.950000\n",
            "Iteration: 956 | Loss: -15.272079 | Accuracy: 0.950000\n",
            "Iteration: 957 | Loss: -15.291950 | Accuracy: 0.950000\n",
            "Iteration: 958 | Loss: -15.311828 | Accuracy: 0.950000\n",
            "Iteration: 959 | Loss: -15.331710 | Accuracy: 0.950000\n",
            "Iteration: 960 | Loss: -15.351596 | Accuracy: 0.950000\n",
            "Iteration: 961 | Loss: -15.371484 | Accuracy: 0.950000\n",
            "Iteration: 962 | Loss: -15.391379 | Accuracy: 0.950000\n",
            "Iteration: 963 | Loss: -15.411279 | Accuracy: 0.950000\n",
            "Iteration: 964 | Loss: -15.431182 | Accuracy: 0.950000\n",
            "Iteration: 965 | Loss: -15.451088 | Accuracy: 0.950000\n",
            "Iteration: 966 | Loss: -15.471000 | Accuracy: 0.950000\n",
            "Iteration: 967 | Loss: -15.490916 | Accuracy: 0.950000\n",
            "Iteration: 968 | Loss: -15.510836 | Accuracy: 0.950000\n",
            "Iteration: 969 | Loss: -15.530759 | Accuracy: 0.950000\n",
            "Iteration: 970 | Loss: -15.550688 | Accuracy: 0.950000\n",
            "Iteration: 971 | Loss: -15.570621 | Accuracy: 0.950000\n",
            "Iteration: 972 | Loss: -15.590557 | Accuracy: 0.950000\n",
            "Iteration: 973 | Loss: -15.610498 | Accuracy: 0.950000\n",
            "Iteration: 974 | Loss: -15.630444 | Accuracy: 0.950000\n",
            "Iteration: 975 | Loss: -15.650394 | Accuracy: 0.950000\n",
            "Iteration: 976 | Loss: -15.670348 | Accuracy: 0.950000\n",
            "Iteration: 977 | Loss: -15.690307 | Accuracy: 0.950000\n",
            "Iteration: 978 | Loss: -15.710268 | Accuracy: 0.950000\n",
            "Iteration: 979 | Loss: -15.730235 | Accuracy: 0.950000\n",
            "Iteration: 980 | Loss: -15.750206 | Accuracy: 0.950000\n",
            "Iteration: 981 | Loss: -15.770180 | Accuracy: 0.950000\n",
            "Iteration: 982 | Loss: -15.790159 | Accuracy: 0.950000\n",
            "Iteration: 983 | Loss: -15.810141 | Accuracy: 0.950000\n",
            "Iteration: 984 | Loss: -15.830130 | Accuracy: 0.950000\n",
            "Iteration: 985 | Loss: -15.850121 | Accuracy: 0.950000\n",
            "Iteration: 986 | Loss: -15.870118 | Accuracy: 0.950000\n",
            "Iteration: 987 | Loss: -15.890119 | Accuracy: 0.950000\n",
            "Iteration: 988 | Loss: -15.910120 | Accuracy: 0.950000\n",
            "Iteration: 989 | Loss: -15.930129 | Accuracy: 0.950000\n",
            "Iteration: 990 | Loss: -15.950140 | Accuracy: 0.950000\n",
            "Iteration: 991 | Loss: -15.970153 | Accuracy: 0.950000\n",
            "Iteration: 992 | Loss: -15.990174 | Accuracy: 0.950000\n",
            "Iteration: 993 | Loss: -16.010199 | Accuracy: 0.950000\n",
            "Iteration: 994 | Loss: -16.030224 | Accuracy: 0.950000\n",
            "Iteration: 995 | Loss: -16.050253 | Accuracy: 0.950000\n",
            "Iteration: 996 | Loss: -16.070286 | Accuracy: 0.945000\n",
            "Iteration: 997 | Loss: -16.090319 | Accuracy: 0.950000\n",
            "Iteration: 998 | Loss: -16.110361 | Accuracy: 0.945000\n",
            "Iteration: 999 | Loss: -16.130402 | Accuracy: 0.950000\n",
            "Iteration: 1000 | Loss: -16.150444 | Accuracy: 0.945000\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VrU6vfUBo09G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9235346-d7c0-403e-a3e3-afdf4aedac2e"
      },
      "cell_type": "code",
      "source": [
        "print (\"Test Loss: %f | Test Accuracy: %f\" % (testLoss, testAccuracy))\n",
        "\n",
        "testSetPredictions = np.squeeze(testSetPredictions)\n",
        "threshold = 0.5\n",
        "testSetPredictions = testSetPredictions > threshold"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: -12.222353 | Test Accuracy: 0.530000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CPNWD80go5te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1157
        },
        "outputId": "f2606bef-baec-4b1b-adcc-a28d3a4e86ea"
      },
      "cell_type": "code",
      "source": [
        "# Create the predictions plot\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "plotDim = 1000\n",
        "dataPlot = np.ones((plotDim, plotDim, 3), dtype=np.float32)\n",
        "scaledIndices = np.floor(testDataPoints * plotDim).astype(np.int32)\n",
        "\n",
        "# Create decision boundary meshgrid\n",
        "for idx in range(grid.shape[0]):\n",
        "    if gridPredictions[idx] > 0.5:\n",
        "      dataPlot[grid[idx, 1], grid[idx, 0], :] = [0.5, 0.5, 0.5]\n",
        "\n",
        "for i in range(scaledIndices.shape[0]):\n",
        "  color = (0.0, 0.0, 0.0)\n",
        "  if testSetPredictions[i]:\n",
        "    color = (1.0, 0.0, 0.0) # RGB\n",
        "  \n",
        "  cv2.circle(dataPlot, (scaledIndices[i, 0], scaledIndices[i, 1]), 5, color, -1)\n",
        "\n",
        "#drawEllipse(dataPlot, center=center, axes=((a, b) if a > b else (b, a)), angle=0.0, startAngle=0.0, endAngle=360, color=(1.0, 0.0, 1.0), thickness=50)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.axis('off')\n",
        "plt.imshow(dataPlot)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f66fea66358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGUAAARiCAYAAAAN70rmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3dtyo8gSBVDqRP+3ii/nPPRohqZl\nUUhA1mWtCD9I42lj61ZsMrPSsiwTAAAAAPf6X/QBAAAAAIxIKAMAAAAQQCgDAAAAEEAoAwAAABBA\nKAMAAAAQQCgDAAAAEEAoAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABDgV/DPX67+ATnnq38EAPCC\nz2AA4AotrjFyzunV/V1XyrT4QAEAAABj6DaUEcgAAABAf3o63+82lAEA4vS0WAIAuEqXoYyFIAAA\nQDtSejluA7rXZSgDAAAAULvo3ZdOp0oGAACgftvqmPXtZbl8o16oQleVMgIZAIBxaX+Aduy9Xr2e\nGUUXlTLCGACA8bw6aXOlHYCWdFUpAwDAGEquorvSDkDtmg9lVMkAAAC0ozQwTSkJV+le06GMQAYA\n6uPzGYB3SlsLl2XRhkj3mg1lLPgAAMZ05Mq5q+wAfco5d5ELNBvKAAAwpiNXzl1lh+sJP+FzTe6+\n1EMaBgBwlZSSMAK4zBk7ny3L8jbM8R7GKJqrlBHIAAD86TkMcz0Uc3sb4Axn7nz2nBnzDGC2tyFC\nnudbf15TlTICGQCAP5WeIPV2krN3lf35PUAbvF6JtA1i/rr9eFz2s5uplBHIAACwtr6q7ko7AJ8o\nqYy5snqmmVAGAAAAoCdCGQDgNCpbiaQ6Bq5lO3o4XxOhjAUeAMDfnCABd7IdPZyv+lBGIAMA8JoT\nJABG901mcGRWzFVzZaoOZQQyAAAAwBWO7Kp01Q5MVYcyAAAA1KOk6k5lHpT7FX0AP1ElAwCwb1mW\n3XkxTpCAM23fU1JK3mfgQ9WFMsIYAIBjnCABkbzf0LL8eOzOi7mqdWmaKgtlBDIA0C6f4/VwggQA\n5dahS57nS0OYrWpmyljIAQAAAJHuDGSmqaJQBgAAAGAkVYQyqmQAAACA0VQRygAAAACMJjyUUSUD\nAAAAfKrlXCE0lGn5DwcAAADwjfBKGQAAAICa5Hme8jxf/nN+Xf4TAICuqXwFAHrwKoRZ33fFdtkq\nZQAAAIChlVTFXFE5I5QBAAAACCCUAQAAoFoppehDgMsIZQAAAKhGSumPr+19cLYjbUlntzAJZQAA\nAKhCSegimOGVbzYeODLA9+xhv0IZAAAAgABCGQDgY7bDBgD4nFAGAAAAIIBQBgAAgHBHZsWYK8PZ\nSmbFnD1PZpqm6dfp/yIAAAActCxLcdiyLMvFR8OI1qHLc5elK4KYNaEMAAAAwMrVYcyT9iUAAACA\nAEIZgC89SxsBAPhOSVuS1iV6on0J4APbIGZ9+65SRwCAHm1Dl5SSIIZuqZQBOCDP825ljMoZAIDz\nCGQolXOOPoTDhDIAwEdaXPgAANREKAMAAAAQIDyUUeYPtOLI+5X3NgBqk1KaUkrRhwHASuig3+dJ\niwGZQAvy41EctngvA6AGr0KY9X1mdQDECq+U2SoZogkA0BLVCUQoed6d8dz0/Ab4XHWhDAAAAMAI\nQtuXAAB6ta0e2N7WNkLLPL8BzqFSBuCA/HjszosxTwa4q20EInh+A5wnhabYKb394U5sgBbkefZ+\nxXByztGHULXSE1LVBFzlaChy5Lno+Q3UrsZ1Ss755ZtntZUyTnCAVni/AqA2RwIR4QlAnGpDGQCA\nFh2pUNDiQWs8v+FPnud8y6BfAIATLcuivYNueX4zulfP//V9nvccVV0oow0AAAC+VxKgOIGEcqVD\nrr2uOCI0lMmPhwGZAABwkfXJ4fOE0gkjlBOycLXwShmBDADQGxUK1Ois55znN73bPr/NjeFK4aEM\nANCWGreZrNG2QsFJKj3x/KZXZwQwXhMcYfclAICLWZzTM89v+JPXBEcIZQAAAAACCGUAAACAbrTU\nai2UAQAAYHgG+hJBKAMAAMDwzpgFY54MR9l9CQAAAA54hi92WuJbQ1TK5HmOPgQAAAA6I5DhW91W\nymyDmPXt/HjcfTgA0IWWBucBwFHLsrydLSOE4WxdhjJ7lTHP/y6cAQAAYG0dvGhP4mpDtC8BAADA\nUQIZriaUAQAAALrSSst1d6HMkaG+BgADAAAAUfoLZQ7MiTFTBgAAAIjSXSgDAAAA0AKhDAAAAEAA\noQwAUKSVgXkAAK3oMpTZmxWTHw/zZAAAAIBQv6IP4Crr0CXPsxAGAAAAqEqXlTJbAhkAAACgNkOE\nMvQvz3P0IQAAAFCRFubhddu+RP+2Qcz6tuooAAAAaqdShibtVcaonAEAAKB2Qhl4Q7gD8FsL5b8A\nKaXoQwA4RPsSrLwKYbRFAQDUaxvErG8vy3L34QAcolKGpuR5Lq5eOVrlUvL9KmdgTF77AHXaq4xJ\nKameaYzHa5+/UV9UytCUZ6VKUYCiqgX4kKo5AFqSUmq6KuhdtdM0qXiaJhVhPVMpAwAAABBAKAMA\nK1oZAWjBszXrWTGxvd2KkuNt7Xc6mza9vgllYDp2guVkDIBIFt7w25HXQm+vGyfp0A+hDE3am+dw\ndN7Dke83SwKAu/VyRRzOdGSOhpkbMK6cc/QhvGXQL81ahyN5noUlwNeOVs2N8r5T+2KmdyVXxJ1w\nAi06Wu004nudv1H/VMrQhVFOjIBrqZoDoHY9tW2pdtrnb9Q/oQz8o+QEy0kYAEA99k5Cl2Xp7kTV\nSTr0RfsSrGxDl5HaEwCoi5J1KLN+7nstAK1RKQNvCGRgPKrmqMWRK/xOQuE3r4X2lDxmoz+uI1aE\njUSlDABsqJoDoGbLsrytpmvtBF210z5/o34JZQBgh0AGgNr0epLey+9xJX+jvmhfAgB+ZDvseCVl\n68DYvA/AezWvZ4QyAAAAAAG0LwEAVK7XNgUAGJ1KGQCAhghkAKAfQhkAAACAAEIZAAAAgABCGQAA\nAIAAQhkAAACAAEIZAAAAgABCGQDgpZxz9CEA0+9t0AHo06/oAwAAAP60DWLWt22LDnDc82JTbRed\nVMoAsCvP85TnOfowAIawVxmjcgagHyplAHjpVQizvi8/HnceDgAAdEelDAAAAEAAoQwAfylpVdLO\nBAAA3xHKAABAJUrnxaSUzJYB6IBQBgAAKlG6s9KyLHZhAuiAUAaAPxxpS9LC1K+7tot0pR8AGJlQ\nBoA/HNlVyQ5MHPVsuVi3XmxvAwD98Tn/mlAGALhFyWLMgg32W5i0LQGtqPFCzF3VwKV+RR8AAADw\np3XwklISxABNKb0Q471NpQwAL5S0JWldAriHkxaAfqmUAeCldejyHOgriOFTR8qUXTkDAEYhlAFg\nlzCGby3LUhzMCGQAoE1HZ8W4ECOUAQBWaht+BwDU7ZuhvaMHMtNkpgwAAADwAbsmfk8oAwAAABBA\nKAMA3KKkRFkZMwAwEjNlAIDbbEMXA/4AYEx3fv7XvN4QygAAYWpdIAEA730yT+buIObd7Uclu4tq\nXwIAAAAOORKwLMsSGsi8Ms/zDUeyTygDAAAA8KH8RcCjfQkAmKZpmnLO0YcAcIma50kAbdoGMX/d\nLmyPUikDAAB0JaX0x9f2PuAcNe6seOQ1/mkLU0llTGn1jEoZAACgGyUnZCpn4Dzr11INr61lWYqD\nmRqG/aqUAQAAAL4WHci0SCgDAAAAUODIUN+S7xXKAAAAXTgyS8JsGejXlbNuSgf4ln6vUAYAAAAg\ngEG/AABAF44M+DT7AvpW2wDin6iUAQCmnHP0IQAAXKLWQGaahDIAAAAAxUpmxZTOntG+BAAAdKOk\nhanmq+ZAG9ahS57nQwOA14QyAABAV7ahS83zJID2fRrITJP2JQAAoHMCGaBWQhkAAACAAEIZAAAA\ngABCGQAYnO2wAYAR1bAGEsoAAAAABBDKAAAAAAQQygAAAAAEEMoAAAAABBDKAAAAAAQQygAAAAAE\nEMoAAAAABBDKAMDAcs7RhwAAMCyhDAAAAJwopRR9CBSKvkD1K/SnAwAQLqU0LcsSfRgATdsGMdvb\n3md5RSgDADCYV1dw1/c5cQA4pqQyRgDOK9qXAICPKc9uT+mJAwBwPZUyAMAhyrMBAM6hUgYAKKbK\nAgD+dORzz2ckW0IZABhU9G4D3M+JA8D5jlSIqiZlSygDAAAAEEAoAwAwCFdzAaAuQhkAoIjWFwCA\ncwllAIAiqiwA4LWSzz2fjfWKnLNnS2wAgIEsy7JbyeTEAeC49XtnSsl7KUWEMgAAg9meKDh5ADiX\n91RKaV8CAIopz+6TxwwAYqiUAQAOUZ4N7fFaBaiTUAYABnTWQDsneVCv7eyg7W2vX4B42pcAAKAz\nJdvS27oeIJ5QBgAABiWYAYgllAEAAACGdlZr91FCGQAAAIAAQhkAAOiIliSAdghlAACgI0d3VRLi\nAMQRygAAwMBsjQ0QRygDAIOJGmQHAMCfhDIAANAZ1S/AGbQ3Xu9X9AEAAFC/lJIT/cYsy7J7QuUx\nBba27xvb2943zqVSBoAm5HmOPgQYTkrp36/tbVdP27Asy79f7+4DxrZ+ny/9Xs6hUgaAam2DmPXt\n/HjcfTgwlNKFuRP7dnisgLW9ihjuoVIGgCrtVcbkeVY9AwBwkGrHn0VshiCUAQAAAIoJdc4jlAGg\naapljrEdNiWOLLYtzAHGox3yPEIZAKojaIFYRxbbFuYA7RCk10coAwAAABBAKANAdeysBABwPtWN\n9RHKANA87U4AAPcQ7JzrV/QBAMC3VNbA+ZZl2Z09YGEO0K/n54D3+muplAGgSoIWiLcsy79f29sW\n6QBtKnkPX7/vc63qQxkl6QDs8VkB17MwB+jLu+B9ZDnnW39ede1LrxbW6/tcOQUYR348igMXnxX7\n7l5kAABtGD2IiVRVpUzJwtvVUICx5Mfj36/i/8dnBQAADagqlAGAd1TAAADQE6EMAAAAQIBqQpkj\npebK0gHG5LMCAICe1BPKHJkVoHwdYEg+KwAA6Ek1oQwAAADASIQyAAW0wgAAAGf7FX0AALXaBjF/\n3dYeQ0NyztGHAADARlWVMiUnOE6CgDuUVMaononhswIAgCvdeTGrukqZ7UI6z7PFNQB/8FkBAEAP\nqqqUecUiG4A9PisAAGhR9aEMwN2OtCVpYQIAAD4llAHYOFJ1oUIDAAD4lFAGAAAAIEB1g34BgPPY\nChsAoF4qZQBesO0yAABwNZUyAD9Yhy62XK6bxwcAgBaplAEAAAAIoFIGGqQq4H7+3nV5tRX5+j6P\nFwAQKaU0LcsSfRh84TmX7+r5fEIZaIATUPjPq9fDq+/xugAA7pRSentbSMMr2pegcqUnoAAAQIxt\nAPPp9zAeoQwAAABAAKEMAM04UhWmguz6HmgAAL4jlIGKOQGFPx2ZE2OmDABwhyNtSVqY2BLKQMWc\ngAIAQN2ODPA17JctoQwAAABAAKEMAE0pqQpTOQYAwBmuntH369J/Hfhafjx258U4AWU02+d8nmev\nAwAgzLIsu/NitC7xilAGGuAEFN7zegAAoq1Dl5SSEIYi2pegQU5AgT22wwaAOAIZSgllAAAAAAII\nZQAAAAACCGUAAAAAAghl6M7eTkUAAABQA7sv0YVtELO+bSguAAAANVIpQ/P2KmPyPKueAQAAoDpC\nGQAAAIAf5Jwv+7eFMgAAAAABhDI07UhbkhYmYBRXXs0BAOA8QhmadmSIr4G/AAAA1EQoAwAAABBA\nKAMAAFwmpRR9CADVEsoAAAAABBDKdGD0AbZ7s2Ly42GeDADAjVJK/369ug3Ab7+iD4DjXoUw6/tG\nDCDWv3Oe5yH/BgAANdgLXlJK07IsNx0NQN1UyjSmpCpG5YxABgAAgPPknC/5d4UyANCRqxYMQN+0\nFQHE0L4EAAAD2gYx69uftBcdCXa0MAH8plKmIUfakkZvYQIA4Gclc1+OWpalOGgRyAD8JpRpyJFZ\nKeaqAAAAQN2EMgAAABUy6wf6Z6YMAAAMpPRE//l9R1uNlmV5+zO0Lr33btbPNPn7QW9UyjSmpC1J\n6xIAAD85Mvfl0wDg+f8+///tbV4rCcxUz0BfVMo0aBu65HkWxABgO2ygSoIYgJ+plAEAAAAIIJTp\ngCoZAAAAuNYVVclCGQAAGMxeS5GWo/sdmRVjrgz0w0wZAAAY0Dp4SSkJYoLt7Vq1/V6gDyplAABg\ncE7yAWIIZQAAAAACCGUAAAAqUFKxpKoJ+mKmDAB04IrdAAC4n1k/MBaVMgAAABUSyED/hDIAAAAA\nBc6uThbKAAAAAAQQygAAAAAEEMoAAAAABBDKAAAAAAQQygAAAAAEEMoAQOPO3gUAAOBOKaUppRR9\nGCF+RR8AAAAAMJZXIcz6vmVZ7jycMCplAAAAgNuUVMWMUjkjlAEAAAAIIJQBAAAAKHTmPD+hDAAA\nAHCLI21JI7QwCWUAAACAWxwZ4DvCsF+hDAA0zHbYAADtEsoAAAAABBDKAAAAALcpaUsaoXVpmoQy\nAAAAACF+RR8AAAAA9CalNEy1xyfWf5vnLksj/r2EMgAAAHCC7RbO29sjhg4lrv671ByQaV8CAACA\nL20DmE+/h3OklP792t4+43E4awdMoQwAAADQjZYCMqEMADTqrCs0AADEEMoAAADAF45UXdRSoUEd\nhDIAAADwhSNDZGsdONuL1gIyoQwAAADQhdYCMqEMAAAAQAChDAAAAHyppOqihsoM6vIr+gAAgHo8\ne6stGgHguPXnZ0rJ52mQZVl258XU8tiolAFgWHmeow/hY2duh51S+vfr3X0AQLlaTvpHtSzLv1/b\n22c9Nmesx4QyAAAAAAG0LwEwlG11zPp2fjzuPpxwJZUwyq8BgJbVvI5RKQPAEPI877YrtdzOBABA\ne4QyAAAAAAGEMgAwqCNDfA38BQA4n1AGgO4daUsaqYXpSH91zb3YAACtEsoA0L0jA3xHHPYLAEAM\noQwAQAe0mEEdvBaBI2yJDQANyTmf+u8ty7J7AqF1qV7bx25722MH13v1Hrq+z+sQ+pZz/mp9JpQB\nYAjPtqR3M2NGbV1anzA8TyScRNSv5Gp8SsljCRfyOgS+JZQBYCjr4CXP87BBzE+cOAAA3MdMGQCG\nJZABACCSUAYAAAAggFAGAKAxR3Z3sRMMXMPrEDiDUAYAoDFHZv+YEwTX8DoEziCUAQAAAAgglAGA\nRuScow8B3tKiAQDH2BIbAKBBy7LshiB3tExsj2F9W8sGvavldQi0S6UMAECjlmX592t7OyKQOfrf\noQevXnd3vg6BeN9UMwtlAAA64OQP6uC1CHWr7YKB9iUAAACgS69CmJpabVXKAABwWOmVxpRSdVcl\nARhDyedP9GeUUAYAgMNKryyaqwEAPxPKAEADbIcNANAfoQwAAABAAKEMAAAA0JUjs2Ii58oIZQAA\n+MjerBizZLhT9LBOoC5HPoMiP6+EMgAAfOw5yPe5oN3ehis9d/d6BjLb2wB3+XT+n1CGJuR5jj4E\nAGCHIIY77QUvghmgBb+iDwB+sg1i/rr9eNx5OAAAADRkWZbdgDb6goJKGapUUhmjegYAAIB31m21\nNbbaCmUAoHKf9igD9Kq0Ncl8GWCrhiBmTSgDAAA0pfSkqpYr4QA/EcpQnSNtSVqYAAAAaJVQhuoc\nGeBr2C8AAACtEsoAAADN2WtL0rYEtMCW2AAAQJPWwUtKSRADhHpuznBkkwaVMgAAAAABhDJUqWRW\njHkyAAA8qZIBWqR9iWqtQ5c8z0IYYEhHyl8BAGiLShmaIJABrpbnOfoQAAAYjEoZAIa1DWLWt4XB\nAABcTaUMAEPaq4xRORMnpRR9CAAAt1ApAwCE2wYx69uGdwIAvVIpAwCE2quMSSmpngEAuiSUAWA4\npa1JeZ61MQEAcBmhDADDKR3imx+P0IG/tsMGAGjPkTWcUAYACHOkLUkLEwDQG6EMABDmyBBfA38B\ngN4IZQAAAAACCGUAGNLerJjIWTIAAIzhV/QBAECUdfCS51kQQ5iUkvYsABiQUAYAJpUxkZZleTvE\nt9ewYvs7r2/3+jsDAH8SygAA4dYhxAhVI3s7SY3wNwAAzJQBgCrlnKMPIYwwAgAYhVAGAAAA4ESl\nF9iEMgAAN9prXVp/X+n3AgBtEsoAANyotD1rWRatXADQOaFMZfI8Rx9CqNF/fwAAAMZh96Vgr0KI\n9X29b9E6+u8PAADAuFTKBCqpCum5cmT03x+Ace21JWlbAoAxCGUAAAAAAmhfAoDKlG6hSNvW1TAp\nJdUxADAglTIAAMEEMgDcLaUUfQhMQpkwR2al9DhXZfTfHwAA4E4ppT++tvcRQygT5MiuQj3uQNTT\n75/nWXAEAABUqyR0Ecycr6Ql3UwZ+ICtvAEAKGFmFPCOShk4yFbeAAC8o0UEKCWUCVRSTdFzxcXo\nvz8AAP3ZC14EM9ztyHPO8/N+2peCbUOHPM9DBRGj//4AAABXWpalOGzRanc/lTKVGT2QqP33t2sU\ncLWSgXAAAPRBKAMH9LRrFAAA5yqtRjBfBngSygAAAJygtPVjWRZtItyq5PnmORlDKAMAAACde4aB\n61Bwe5vz7bWmC2UAAAAAAghl4CBbeQMAAK1THVMHW2LDB9ahy3OXJUEMAAB72w87EQbWhDLwJWEM\nAABr6+AlpSSIAX6kfQkAKrE3CA6A9ghkgHeEMgAAAAABhDIAAAAAAYQyAAAAAAGEMgDwgefOawAA\n8Cm7LwFAoW0Qs75tJzYAAF55t5mDShkAKLBXGaNyBgCAo4QyAFAB22EDAIxHKAMAAAAQQCgDAAAA\nEEAoAwA7SufF5Hk2WwYAgGJCGQCqVkPIUbqzUn487MIEwOlSSlNKKfowgAvYEhuA6th6GoDRvQph\n1vcty3Ln4QAXUSkDQFVsPQ3A6EqqYlTOQB+EMgBQYK9CRwUPAABHaV8CgELr4CXP82lBTM75lH8H\nAIC2qJQBoBpHdjmKpjIGgCscaUvSwgTtE8oAUI0juxwBQI+ODPA17BfaJ5QBAAAACCCUAQAAAAgg\nlAEAAAAIIJQBoCq2ngZgdCWzYsyTgT7YEhuA6ly19TQAtGIdujx3WRLEQH+EMgBUredAJuccfQgA\nNEAYA/3SvgQAAAAQQCgDAAAAEEAoAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABBAKAMAAWyHDQCA\nUAYAAAAggFAGAAAAIIBQBgAAACCAUAYAAAAggFAGAAAAIIBQhkvleY4+BAAAAKjSr+gDoD/bIGZ9\nOz8edx8OAAAAVEmlDKfJ87xbGaNyBmCacs7RhwAAQAWEMgAAQFVSStGHAHAL7UsAAEC4bRCzvr0s\ny92HA3ALlTKc4khbkhYmAADW9ipjVM4AvRLKcIojA3wN+wUAAAChDAAAMDBVOEAkM2UAAIAQRwKR\nlNJps2XMrwFqoVIGAAAAIIBQhtPkx2N3Xox5MgAAPC3LUlyZclWVzKv/rqUJuIv2JU63Dl7yPAti\nAFZyztGHAABAJVTKcCmBDAAAALwmlAEAAELttSbd1br06fcCfEr7EgAAEG4dvJy509L2Z5SGLXZh\nAu6gUgYAAKiKQAQYhVAGAAAAIIBQBgAAGEbJ/BqVOsBdzJQBgJvYDhugDnfMrwEooVIGAAAYlkAG\niCSUgY7keY4+BAAAAAppX4LGbYOYv24/HnceDgAAAIVUygAAAAAEEMpAw0ralbQ0AQAA1EkoAwAM\nJ6UUfQgAAGbKAABj2AYx69t2X2EktoAGqIdKGWjUkbYkLUwQL+ccfQhD26uMSSmpnqFbz+f3+nm+\nvQ1ADKEMNOrIrkp2YAKAMZWELoIZgDhCGQAAAIAAQhkAoGtHqgBUDAAAdxLKQMNK2pK0LgGjOzLQ\n1PBTeiKQBKif3ZegcevQJc+zEAYAmKbpd8hYGrYIJAFiqJSBjghkAAAA2iGUAYCL2Q473l4VwLIs\nKgUAgNsJZQAAAAACmCkDAAxhXQmTUlIZwxBK5sp4LQDE6bpSJs9z9CEAABVyEspInu156za97W0A\nYnRXKbMNYta3DUEFAGB0ghiAenRVKbNXGaNyBgAAAKhFV6EMAAAAQCuEMgDAZfYGjAIAjKyLmTJH\n2pLyPJstA8Btcs7Rh3C7bRCzvm2WBQDAf7qolMmPR3HQIpAB1vI8mzcFJ9qrjFE5AwDwny4qZQCO\neBXC2KkNAAC4WxeVMgClSqpiVM4AAAB36CqU2bu67eo3AFyntDUppaSNqUIeEwC4X1ehDAAAAEAr\nupsps66GsdMSANxnWZaiags7MNXh1WNlpywAuFfXlTICGWDtyKwYc2WAnpWEZ9qZAOB6XYcyAGtH\nglqhLmfIOUcfAgBAGAH/vu7alwCAOHstTFpiAKBfWmOPE8oAAKdaL7hSShZglTly1dLjB0Cp0tZY\nnyt/0r4EDKWkLUnrEpzHwqs+Rx4Tjx8AXEulDDCc7S5t2/vgTK4IAfTF+zpwphT5hpJz9m4GQHfm\nN7t3WchTg9IWJs9X+M2sLHjv6EDfQV83L/9IKmUA4CTvwpgnV1ipwd5A5uf3APsnm8//7jXDyEo+\nV9bfy3+EMgAAA9ouigWGAHA/g34BABDIAEAAoQwAnKCkdenpaN81ADGObiEPIysJ910A+JtQBgAA\nACCAUAYATvA4sK26q0QAbTjyfu29HX6/DtZf2/v4m1AGguUDLQ8AAACtEMTss/sS3OxVCLO+Lx+4\n2g4AAEC7VMrAjUqqYlTOQLsej8duG5MrRgBt2Xvf1pYBfEOlDACcbL04TylZrAM0zvs6cBWVMgBw\nIQt3gL54XwfOJJSBmxxpS9LCBAAQI6U0pZSiDwMYhFAGbnJkgK9hvwAA93kGMesw5tV9AGcTygAA\nAMMqCV0EM8BVhDIAAEDzBCdAi4QycKOStiStSwAAZbYtRlqOgNYIZQAAAAAC/Io+ABjNthImz7Pq\nGAC6lFKyfTCX2auGKXn+Hamo8XwGriCUgWACGehHzjn6ECDc9iR3e9tJLTVZlqU4mPHcBa6gfQkA\ngFPYxQYAjhHKAAAATSkN9wz9BWonlAEAAJpS2kq0LMsf3/sqoCn5t7QuAVcxUwYAgK8ZmEqNXj0v\n1/c9n4evghvPUeAOKmUAAPggF9U8AAAgAElEQVTakRNYJ7vU4qfKGc9R4C5CGQAAoDmCE6AHQhmA\nIHmeow+BE9kOG+B+z6qWdRuSShegJWbKANxoG8Ssb+fH4+7DATjVsiy7s2WcLHOVvYG+PzHjCIik\nUgbgJnuVMSpngB68q1xw4stdzDgCWiGUAQAAAAgglAEA4BIqEADgPaEMAAAAQAChDMANSufFmCsD\nAOcoqdRSzVWfI0OaoQdCGYAblO6sZAcmADjPq0HTBk/XJ6X079f2tpCG3gllAOBLOefoQwCggCCm\nPiWhi2CGngllAAAAgCa1HtoJZQBusteapHUJAADee9Xe1nK726/oAwAYyTp4yfMsiAEAhnXkBDql\npP2M4na3lp4rKmUAgghkAICRHTlxbukkG44QygAAAAAEEMoAAAAAVTva7tYKoQwAAAAhStqStC4x\nTf22uwllAAAAAALYfQkAvpBzjj4EAGjauqqhtZ1z4FsqZQAAAKiCQIbRCGUAAACA6vU4g0j7EgAA\nANCEbejSesubShkAAACgSS0HMtMklAEAAAAIIZQBAAAACCCUAQAAAAgglAGAD+Wcow8BAICGCWUA\nAAAAAghlAAAAAAIIZQAAAAACCGUAAAAAAghlAAAAAAIIZQAAAAACCGUAAAAAAghlAAAAAAIIZQAA\nAAACCGUAAIDbpJSiD4ETpJQ8lnCCX9EHAAAA9G178r69vSzLnYfDh16FMOv7PI5wnEoZAPhAzjn6\nEACaUFJNoeKifh5HuIZQBgAAACCAUAYAAAAggFAGAAAAIIBQBgAAuMSRGSPmkdTL4wjXEcoAAACX\nOLIbj5176uVxhOsIZQAAAAAu8m7XTqEMAAAAwAXeBTLTJJQBAAAuVNLOouWlfh5HOG4vkJmmafp1\n/WEAAAAjW5+sp5ScvDdq+zi+us9jC7+VBDLTJJQBAABu5KS9D8/Hcbvb0vq2x5pRlQYy0ySUAQDo\ngivUwJ1Ktr72vsRojoQxT13NlMnzHH0IAAC3SSn9+7W9XXLCBACc45NAZpoar5R5FcKs78uPx52H\nAwBwG1epAaAOnwYy09RwpUxJVYzKGQAAgHMdqcRTtUfvvglkpqnhUAYAAID7HanAU61Hz74NZKZJ\nKAMAAAAQQigDAB8448oIfErrAADEyTmfthZsMpQ5MivGXBkAoDdaBwAgxtkX5toMZQ7sqmQHJgAA\ngHMty7Ib+gqF6c0VldJNb4kNAABAnHXwklISxNCtq1rXhTIAAA1almV3XoyTI+BO3nPo0dVzBJsN\nZfLjsTsvRusSANAzV6gB4Bp3berQbCgzTX+HLnmeBTEA3ObMyfvwLYEMAJzjzvVdk4N+fyKQAQAA\nAD519wW3pitlAAAAAL4VVf3cVaUMANxN+xIAQNsi13NCGYAde0PFAQCA6+3tOnhUDfMBhTIAAAAA\nAcyUAXhhWx3z122DxVmp4SoLAECPttUx29uf7j5Yy9pNpQzARkm7kpame/g7AwCMq6Rd6ZOWploC\nmWlSKQNAZd5VKdVcoaRapk0ppY+vsAEA7ah1naZSBoBq7FXGqJzhDCmlf7+2t88eIAgAxKs1kJkm\noQyDcULHniPPEc8ntlTL1O+qMmgA4FxHPo9/+t4W1mbal+jaq5PmVlohiJEfj+KwxfMHAACusSxL\ncTDzqhW59jDmSaUM3TKsFdqR57k8DGvgddvKIgAAoDctVMesCWUACJcfj+LKo1YqlFpaDIzijDJo\nAKBeLa6/hDIAGyUn/a0EA8RqcWHQsyO7LNmRCQDilXweL8vSXHXMmpkydOnosFYn2GytnxOeI3yj\n5UUCAEC0dTCTUvorqGl9nZUirwTlnF2G4jKGtUKb3r12W369tr5g6MU3AwMBgDq0uK7KOb9chKiU\nAaAqvVYpqZipQ8lODgIZAKhXb+spM2UAAAAAAqiUAaBavVTJPKmWqcNebzoAUJ9e11AqZeiWHXSA\nGglm6iKQAYD69bx2UilD17ahS0/zKYC2CWc+p7oFAMYwwlpJKMNQBDJATdYLjREWHZ96NZh3fZ+A\nBgDOU8vFj1HWRl22L5VuhQwAtRhl4XFUyRbW776ndAtsABhZSunfr1e37zRaNXEXlTKvQpj1faoj\nAGjBcwEy0kLkbKpqAOCYveDlzsqZEddAzYcyJVUx5ogA0BLhzGdKq2oEMwBQj1rXO8+s4eosoflQ\nBgB6NXo4c6RkWpsSALSntjVORBdOlzNlAKAno/VWPx2paFH9AgDHlV7UOHu+TI1rm9IunLM1XSlz\n5A+ihQmA1tmt6WdHq2qEOADw+6JGyWfoWZ+b1i9/a7pS5kjIIpABoCfPK0wWN7+pqgGuoDUSzmPN\n8lrToQwAAABAq5puXwIA+h4IXFJWrfIFOMur95v1fd5v4LgW1ieRo1GEMgDQiV5nzmxPgsyEAa5Q\n0qrk/Yce7V0A+eY538p6JD8excHM2aNRmg9lSv545skAMJreq2d+ul9VDQAct/58/DZ87HHtcaXm\nQ5lp+jt0sdMSAPy2XRj1vlBSVQMA3xHI3CtFLlRyzlZJABDEwgngt6O7LAl74U+9rCmu7MLJOb98\no+miUuZTKmoAGFmvM2gAjippf1x/L/Bbb+uHdT7wDGiuzgyGqpQxewYAyvW20AJ4RygDx1gnHDN8\npUzJJGWVMwDwH5U0AMCWNcG5hgllAIDPCWiA3tnBDfZZA5xPKAMAHDLajk7AOOzgBn/zOX+tIUKZ\nktal9fdqYQKAcq8WaxZwQA8EMozMZ/k9/hd9AAAAAAAjGqNS5vEorpZRJQMA39PiBADt8rl9nyFC\nGQAglhYnAK72HNSs7exzPpvvJ5QBAEKopgHgW692zFrfJ6Ap4zM4zjChTEkLk9YlAIgjpIF22aWI\nCHtbmD+/x3PzPZ+3sYYJZabp79DFTksAUK+fFokWj1CH7Qnx9rYTYaibz9M6DBXKbAlkAKA95tNA\nPBUK0C6fmXUZOpQBAPqg9QlgLCXB4Pp7BYQ+G2sllAEAuqP1CRjZCLsQLctSHMz0/Hco4bOvbin0\nCZrSooUIAIhksQrHHKlQmCYnxHfZe1x6fByEMvt8xtUj5/zyCRseyqxvCmgAgFpYyMLPnAzXZdTH\nY9Tfu4TPsPo0EcpMk2AGAKiXRS785mS4LiM/HiNWCL3jc6peP4UyZsoAABR6t9i1EAa43zp0GWGW\nzjs+h9r0v+gDAAAAABiRShkAgBO8ukLpqiW9Ktn5ZtRqhbvZGvo/Pf9uP/E5077qZso8mS0DAPTM\nQpqe9H6yX7uRZ8qMzOdIW5qZKSOMAQBGoLKGnjjZh/v4rOhLdaEMAMCoDBIG4Cc+B/oklAEAaMBP\ni3GLdMCMn/55r+9XVaGM1iUAgGNU1wDTZGvoXnkf7194KCOIAfhPnmfvi8BpVNfAmIQxbfMePZbQ\n3Zdyzt4tgOHleX7/34U0wI2cDADE8R7cr2Z2XwIYyV4g8/wewQxwF9U1APfzHjsuoQwAALv2Thic\nUAAc570ToQwAAF9TYQNwjPdHpmma/hd9AAAAAAAjUikDEKRknsz6e82VAVqk7QngT973WBPKAATJ\nj0dxMCOQAXr17uTEiQvQE+9pvCKUAQCgSgIboHXeq9gjlAGAg7STQTxtUUDtvA9RIi3LEvbDc85x\nPxygEnstTE7+6/DucfIYQTucJAFX8z7DKznn9Op+oQxARVRg1Klk9o/HDfrgZAr4lPcP3vkplNG+\nBFARJ/YAsbRFAUd5X+AbQhkAAChk+DCw5nXPt4QyAPCD0i3Ln9+r0gnGpsoGxuH1zFmEMgDwg2fI\nYqYMcAZVNtA2r1OuIJQBAIBgJSd7TgghhtceV/pf9AEAAAAAjEilDAAANMDMGriX1xR3EMoAwI78\neLydK2OeDFADM2vgHF4v3EkoAwAF1sGLnZaA1phZA/u8BogglAGAgwQyQI+0RzEqz20iCWUAAIBd\nQht64vlKLYQyAADA14Q2tMDzkNoIZQAAgMuZa0Mkzy1qJZQBAACqYAcpzuZ5Q+2EMgAAQPVU2nCE\n5wKtEMoAAABdENyMy+NKq4QyAADAMAwk7ovH63p5nqf8eEQfRreEMgDAkCwygVdU29TP3/96eZ7f\n3/b5eZr/RR8AAAAAwIjSsixhPzznHPfDAYDhbK/0/fXfXfkDTqKa41z+nvfZ+6z89/t8Zh6Sc06v\n7te+BAAMoWSRqaUJOIs2qM/5uzASoQwA/OB5Eu8kHYArCG5+G+F3hJ8IZQBg5VU1xfo+AQ1AG3qp\nfCsNLFoJNlo5zlGVti49v7eH11g0M2UA4B96qPt1ZJE5TR5jaJW5UZ85IygRtvTDeugaZsoAAMPK\nj4dFJnTO3KjPCVQgji2xAQAAKnS0yg9oj0oZAJj0UANQh+3n0V+3ff5wsZLqUs/D85gpAwD/0N7S\nP4tM6FMvc6N8DlEjF6POYaYMADC89aLSIhP6YW4UXMdr5lpmygAAQ7LIBACiCWUAAAAAAghlAOAf\nJZUTqisAuMLRgfPA+SJeWwb9AsAPnh/MghgYk7lD7Wl9mLe5OHC/u943DPoFgIMsemE8tiNum2He\nwBElQejV7yXalwAAYCpfnNMGgQzQAqEMAABABcw2g/FoX+qUck0AAGiPFiy4x9Hh2le9Fg367ci7\nJ5U3cwCAnx1tS7K2AmjfncO1fxr0q32pE7sTo/U/AwD86MiCWyADwFmEMgAAAAABhDIAAADAcGoY\nrm2mTAf0QAMAnGO3Jdw6CqBbVw70NVMGAAAAoCIqZTpRUi3jyg4AQDnbEQNwFpUyAABwgEAGgKsJ\nZaBStjEHAGA01sCMRvtSR969gbnS0wbDBQEAGI01MCP4qX1JKNMpPdA/q/VvU3pVoMZjBwCAT1gD\nM4qfQplfdx8I9/Cm9Z9Xb/Tr+/ytAAAAiGCmDF0r2pVK3yoAAAABhDJQgSPBkBAJAIAeWAODUAaq\ncKSFSrsVAAA9sAYGoQwdk7wDAABQM6EM3ZK8AwAAUDOhDFSiJBgSHgEA0BNrYEYnlAEAAAAI8Cv6\nAID/rK8C5Hl2VQAAgO5ZAzMylTJ0reVyyFqPCwAArmINzGhUytC97Rv7kfRdUg8AAFCHHs/P0rIs\nYT885xz3w+GFva2xe3sDAAAAqFkv52g55/Tqfu1L8I+9F3vp9wAAAPC9Ec7RhDIAwF9aX+AAALTA\nTBkA4GUIs76vldJgAICWqJSB6dgVYVePgd6MUBoMALRllHM0oQxMx64Au1oMAABwrVHO0YQyAAAA\nAAGEMgAwsFFKgwEAaiSUAQAAAAgglIF/lPQhttyrCPDKKP3aAEB7RjhHsyX2ifI8N/+EGN328fOY\nAgAAxFmfj/V4fiaU+dK2v/6v2509YUbj8QMAAKhDj+dn2pe+UDLw0FBEAGo3QmkwAECNVMoAANo3\nAQACqJQBAP4ikAEAuJ5Q5kNH2pK0MAEAAABbQpkP2UIUAAAA+IZQBgAAACCAUAYAAAAggFDmC7YQ\nBQAAAD5lS+wvrUMX24cCAAAApVTKAAAAAAQQypxIlQwAAABQSigDAAAAEEAoA2/keY4+BAAAADpl\n0C+svAph1vdpUQMAAOAsKmXgHyVVMSpnAAAAOItQBgAAACCAUAYAAAAggFAGpmNtSVqYAAAAOINQ\nBqZjA3wN+wUAAOAMQhkAAACAAEIZAAAAgABCGQAAAIAAQhn4R8msGPNk2mMwMwAAUKtf0QcANdmG\nLnmeBTEN2gYx69seTwAAoBYqZeANJ/BtyfO8WxmjcgYAAKiFUAYAAAAggFAGAADYpdoU4HxmygBd\nOLJQNCsIAPa9+mw1pw3gXCplgC4cWRhaRALAeyUXO1TOAHxPKAMAAAAQQCgDAAAAEEAoA3QjPx67\nrUlalwDgvaNz2gD4nEG/QHfWwYuhvgBwTH48isMWn7EA31EpAwAAABBAKAN0zRU8AACgVkKZD+id\nBQAAAL5lpkyBVyHM+j5X4gEA6EnJXBlrYIDvCWV2lFTFGCQKAEBvtutba16A82lfAgAAdglkAM4n\nlAEAAAAIIJR548hAX8N/AQAAgCOEMm8cKdFUzgkAAAAcIZQBAAAACCCUAQAAAAgglNlR0pakdQkA\nAAA46lf0AbRgG7rkeRbEAAAAAF9RKQMAAAAQQCjzAVUyAAAAwLeEMgAAAAABhDIAAAAAAYQyAAAA\nAAGEMgAAAAABhDIAAAAAAYQyAAAAAAGEMgAAAAABhDIAAAAAAYQyAAAAAAGEMgAAAAABhDIAAAAA\nAYQyAAAAAAGEMgAAAAABhDIAAAAAAYQynCbP85TnOfowAAAAoAm/og+Atr0KYdb35cfjzsMBAACA\nZqiU4WMlVTEqZwAAAOA1oQwAAABAAKEMAAAAQAChDB850pakhQkAAAD+JpThI0cG+Br2CwAAAH8T\nygAAAAAEEMoAAAAABBDKAAAAAAQQyvCxklkx5skAAADAa7+iD4C2rUOX5y5LghgAAADYJ5ThNMIY\n4P/t3d2S4riyBlATUe9t6cmZi+6KoV0UlsB26metO9h9Dkxhy9bnTAkAACinfQkAAAAggFAGAAAA\nIIBQBgA69L2OFwAA/bKmDAB04FkI8/iedb0AAPqjUgYAGldSFaNyBgCgP0IZAAAAgABCGQAAAIAA\nQhkAaFhNW5IWJgCAvghlAAAAAAIIZQCgYTW7KtmBCQCgL0IZAAAAgABCGQAAAIAAQhkAaFxJW5LW\nJQCA/nxFfwEAYN82dEk5C2IAADqnUgYAOiSQAQDon1AGAAAAIIBQBgAAACCAUAYAAAAggFAGAAAA\nIIBQBgAAACCAUAYAAAAggFAGAAAAIIBQBgAAACCAUAYAAAAggFAGAAAAIIBQBgAAACCAUAYAAAAg\ngFAGAAAAIIBQBgAAACCAUAYAAAAggFAGAAAAIIBQBgAAACCAUAYAAAAggFAGAAAAIIBQBgAAACCA\nUAYAAAAggFAGAAAAIIBQBgA+kHKO/goAAHTqK/oLAEBvtkHMj9freuXXAQCgUyplAKBCSWWM6hkA\nYATuac6nUgYAAABYlkVF8NVUygAAAKfwlJ0zOK7OoyL4eiplAKBQzU1IytmTJGBKr56yGxd5l+OK\nUamUAYBCNTd9bhCB2aScd8NrT9h5h+OKkQllAAAAYHK1FcEcQygDAAAAEEAoAwAAAJPTph1DKAMA\nFUpuQtyoALPR9sAZStYpevy30CO7LwFApcfQxS5LAH/GxeLJszGTQt/HStE2zY4rOqVSBgA+4CYQ\nABiFiuDrqZQBAAAAlmVREXw1lTIAAMDH0rruTt5M7niH4yqOv+35VMoAAACH8ZSdMziuGJVKGQAA\n4BQmzpzBccVIhDIAAAAAAYQyAAAAAAGEMgAAAAABhDIAAAAAAYQyAAAAAAGEMgAAADColHP0V+CF\nr+gvAAAAABxnG8Q8vraleFtUygAAAMAg9ipjUs6qZxoilAEAAAAIIJQBAAAACCCUAQAAgAHUtCVp\nYWqDUAYAAKhmQgftqVnE14K/bbD7EgAAUMSOLgDHUikDAADssqMLwPGEMgAAADCIvaq1tK4q2xqi\nfQkAAAAG8hi6pJyFMA1TKQMAAAAQQCgDAAC8ZJtd6JcqmbYJZQAAgJdss3sOARZgTRkAAIALPAth\nbCsOc1MpAwAAcLKSqhiVMzAfoQwAALDLNrsAx9O+BAAAFLHNLsCxVMoAAADVBDLl7F4F/EYoAwAA\ncCK7VwG/EcoAAAAABBDKAAAAAAQQygAAAJyspC1J69JnUs7W5KE7QhkAAACAALbEnphtDAHaYlwG\nGNt2jDfuf+5ZZczje/6+tO52v9/DPjylFPfhE9or5TNgAVzLuAwA7yttVXI9pQUppduz97UvTaJk\nwNJ/CXAd4zIAAEIZAAAAgABCGQAAALpSU02q8pSWCWUmYMACaItxGQA+U7NOjDVlaJlQZgIGLIC2\nGJcBAFgWoQwAAABACKEMAABD0wYIYyqpJlVxSutu9/s97MNTSnEfPqG9GxIDFsC1jMtwnlfnl3ML\nxvR93jvHaVFK6fbsfaHMxFLOBiyAhhiX4RillTHONwCu8lsoo30JAAAAIIBQZmKeDgG0xbgMADAX\noQwAAABAAKEMAADDqNlpya5MQA1jBmf4iv4CAABwlLSuFvoFDrMdT368No7wIZUyAAAAsFES8Kqe\n4VNCGQAAAIAAQhkAAIay106Q1lXLAQBNsKYMAADDeQxdUs5CGKBK7aLhxhjepVIGAIChmSwBtWrG\nDWMMnxDKAAAAAAQQygAAAAAEEMoAAADARklbktYlPiWUAQAAAAhg9yUAAAB4wk5udfyN6gllAAAA\nYIew4bnt9uE/Xvu7vaR9CQAAAKi2DWDe/TczE8oAAAAABBDKAAAAAAQQygAAAABVatqStDD9TigD\nAAAAVLGA7zGEMgAAAAABhDIAAADAqVLO2pieEMoAAAAA1d5pYRLM/EsoAwAAABBAKAMAAAAQQCgD\nAAAAVNGGdIyv6C8AAAAAtG8bxLwbzKScban9l1AGAAAA+NXRVTECmf9pXwIAAAAIIJQBAAAACCCU\nAQAAAJ7SunQua8pAJYtSAQAAs0jrWhzMPJsnmT+9JpSBAq9WGTfAAAAAPGe+9Jr2JdixlwqnnA8v\n6QMAAGB8QhkAAADgV2lddyteVMS8R/vSX/rc4BzOLQAAGMPjfb37/GOolAEAAAAIcLvf72EfnlKK\n+/Dl9VohEj+WpX77N8fNH7vr8Pg7AQAAE0kp3Z69P2WlTMnCrBZuZVnqwgNBwx8l547zCwAAYNJQ\nBgAAACCaUAYGowoFAACgD9OtKWONEN7R+vpDrazh4vwCAAD46bc1ZaYLZZalfOJowsgzrW391trx\n3Nr3AQAAiGahXziIMAEAAIAjCGUAAAAAAkwZyqR13a12UA1BD2rWcLlqAeCSc8f5BQAAMOmaMs+0\ntk4IlGp9DRfnFgAAMDtryuwwaYRzOLcAAACeE8oAAAAABBDKAAAAAAQQykDnLKwLAADQp6/oLwB8\n7jF0sbAuAABAH1TKwGAEMgAAAH0QygAAAAAEEMoAAAAABBDKAAAAAAQQygAAAAAEEMoAAAAABBDK\nAAAAAAQQygAAAAAEEMoAh0s5R38FAACA5gllAAAAAAJ8RX8BYAzb6pjH12ldr/46wBtSzs5XAIAL\nCWWAj+21K5nocTXHXLlXgeqyCFUBAM4klAFgCKq16pWs/yTgAgA4jzVlAOheSbUWAAC0RigDfKR0\nsptyNjEGAAB4IJQBPlLa1pDWVQsENKQmJBWoAgCcQygDQNdUa72nJiQVqAIAnEMoA0DXVGsBANAr\noQzwsb2JrokwAADAT7bE5jDfbQEm4HN6/N1toTsvv31f0rru71zl9wQAOM3tfr+HfXhKKe7DTzTT\npMTNPNDKOPDqexiLysx0/QJ4h3ESeFdK6fbsfe1LAAAAAAFUyhyklSfFVyre8WTA/3aYXe0uRleP\nA55kAnAUlZjAEVTKnKhkcmIbVmAUPYxnbpJpWQ/nEPDH7oNX5zPwIQv9AgCc6Nmk7fE9ISIAzEul\nDG+peSrgCQJgHGBWqmkBgFeEMh+aNZyoearnCSCM491xzDgAQG+K10/Meaj7fOBaQpkPCSeAmRjH\nAJhF6TUvravrI/A2oQwAwAlmrablX35bAF6x0C9vS+s65VbgQB3jALMquU4+/lvGYGFnAGrc7vd7\n2IenlOI+/GDCif//BjP8t8LsXo15xgB6l3I+7DgWyszF7z0m1zzgCCml27P3Vcoc5HFAPvJmricz\n/jfDrIx5jERlA/CKax5wJmvKnMBADczEmEfPzt6yuuT8cA5BP5yvwNFUygAAnGg7ifOkfUy1Czs7\nBgBYFpUyAAAAACGEMgAAF1IhMaaa39UxAMA3oQwM4JP1DgBmVdtuAgBwNGvKQKe2E4QWdwrRMw+0\nLK2rLYwBgFBCGejQ3iTi+3+PmES8CouWxcQGgHGVBH2ugwA8EsrApM6oYindWtYNKQCjstsWADWE\nMjCRHlqeAK6ksoGzOX4AeEUoA52pXZjy+2aw5ZYngEgqGwCAKHZfgs60uuWmXUyAUQhkAICrCGWA\nQ7QaFkHPBJgAAGPTvgQTeLflCbietZ8AAOahUgY6tDcxS+v6z79RxQJ9KF37iTn5/QFgPEIZAAAA\ngADal6BT/1TCNNJyZGtZgOM8G0+1swHAWIQyMICWbsxbDIsAelPSqmSMBYD+aV+CSdSuQ3PFZwL/\nK10vJOVsbREAgEGolIGJqGKBdpW0/33/OwAAxqBSBiZlYgfQpppKKFVTANA3oQwAwME+CUtqQnMB\nOwD0TfsSADRir4XJBLxt29/OTkkAwB6hDAA0xNpPfdqrjPFbAgDPaF8CgEaZxM+r5Ld3fABA/1TK\nAAA0aBu6qLYBgPGolAEA+EDpor4p58sWAAYA+iCUAQD4QGlYktZVsAIA/EMoAwAAABBAKAMAAAAQ\nQCgDAAAAEEAoAwDwob21YqwlAwA8Y0tsAIADPAYvtq8GAEqolAEAOJhABgAoIZQBgEop5+ivAADA\nALQvAcP4nih7Qs0ZtkHM42vHHAC8T8snM7vd7/ewD08pxX04MIS9igUXeI5QUhnjWAOAcq+ura6p\njCildHv2vvYloFtFE2VtJgAATdl9qOb+jYkIZQAAgGomzgCfs6YMAPyiZsKhHx6YgfW1AI6lUgYA\nAAAggFAG6FJtBQO8I61r8ZNfT4iB0ZWsA+Kay57SY8SxxCyEMkCXaibAJssAAG3wsAP+JZQBAAAA\nCCCUeUKpHACP9p7WeZoHjE7bMMA5bvf7PezDU0pxH77x6uLhZhvatdvf7vzlBHZaAmZUvBaI8ZEC\n5l/MJqV0e/a+LbGXskXLlsXgAC16PC+dq1zFMQYAn9new7m2MiuhDDAMF3MAgP64h2Nm1pQBAAB2\nlayvZXINUGf6SpnaRctcaAAAmJWWE4BjTV8pU3MhcdEBAIA/3BsTzU5fjGD6ShkAAAD6sA1iHl8L\nCunR9JUyAAAAABGEMkhyhLEAAAZPSURBVAAAADRvr10p5aylqTN+L+1Ly7L8XSn+xcGgDA4AAAA+\n96oFbVnmm38LZf6ykjzQA+MTAAC9KqmMme1+VyjzxEwHANA+C9oBALOraXOZbVJP36wpA9Cwkt5p\nAIDR1YQsAhl6IpQBAAAATlVb7TQLoQwAAABwKtVOzwllABpV+oTA9o8AwAz2JuppXaeazDMGC/0C\nNCqta9kK9W4+AIBJ2DWX0aiUAQAAoDsCmf6U/Gaz/a4qZQAAAIBLqHb6l0oZgIaV9E4DAECP3MsK\nZQAAAABCaF8CaJwSTwAAGJNKGYCOCGQAAGAcQhkAAACAAEIZAAAAgABCGQAAAIAAQhkAAACAAEIZ\ngAGlnKO/AgAAsMOW2ACD2AYxj6/t2gQAAO1RKQPQuZTzbmWMyhkAAGiPUAYAADiNBwMAv9O+BAAA\nHEpLLUAZlTIAAAAAAYQyAB2rKQlXPg7AFaxzBlBOKAPQsZoScOXiAADQFqEMAAAAQAChDAAA8LGU\nc3FrkhYm4Gi9jiu3+/0e9uEppbgPBxjMqwuR1iUArlIyMXJdAj61u37VG+NMyvm08SmldHv2vi2x\nAQbxeAE584ICAACRisLfwvvh7f+vH69PvqfWvgQwIIEMAAC8VhrunEkoAwAAHGbvwYAHBwD/074E\nAAAcSkstcJaaypUexh+VMgAAwGlanxABfakZU17929pw5yxCGQAAAGAqR4U7nxLKAAAAAAQQygAA\nAAAEEMoAAAAA3ShpJ+plPSu7LwEAAABd2YYu7+y0lNZ1dxHfs8MdoQwAAADQtXfDk8f/u4gttLUv\nAQAAAM06c0vqfz4noOVJpQwAAADQlG0Q8/i6l/ViSqiUAQAAAJqxu87LRZUzVxDKAAAAAAQQygAA\nAAAEEMoAAAAATShtTUo5D9HGJJQBAAAAmlC6iG9a1yEW/BXKAAAA/DXK03egD0IZAAAAgABf0V8A\nAAAg0rPKmMf3RmiRANqkUgYAAJhWSauSdia41l4QOlJQqlIGAAAAaMpj8JJyHiqIeaRSBgAAAGjW\nqIHMsghlAKAbyucBjlUzrhqDgTNoXwKAhm0nAT9eD/zkCOBsaV2LwxbjLXAGlTIA0CiLTwLAuFzD\nWRaVMgAAAHCJVxWwqrHmpFIGAACYVslE2GSZI+xVxqicmZNKGQBoUO3ikyYMAO/bbr27fQ/gLCpl\nAKBBNZMBEweA46R1Na4ClxHKAAAAAAQQygAAAMCJirdet67MdIQyAAAAcKLSljitc/MRygBAo+wI\nAgAwNrsvAUDDtjuCCGEAAMahUgYAOiGQAYB+7V3HXefbdtZ6PyplAAAA4AIqYPvxLIR5fO+o306l\nDAzCSu0AANAPgUy7SuZWR82/pq+UkU7Ss+1A8OO1YxsAAKBZ04UyV5UgsU8g9pnS9NbfmNY4LgEA\n4I+pQhmT2HgqO2BOr8595z0AAK2oaUs6Ij+wpgyXubIvD2jH3nntvAcAoBU1IcsRDxeFMgAAAAAB\nhDLQodqSOgAAANozTShjEstIri6pAwAA4HjzhDImsaGEYjCn0vM55ezcBwCgCSWZwFG5wVS7LxEn\nrWv55EwoBsMoPfed9wAQw+6z8Nz2vDjrXBHKQKdKJrsusAAAbG3vIX+8dg8JP5x1XkwVypjEMprH\n49VTDgAA9hRVsLqvhMtMFcosy3UlSPwkFDuXvx2t2jv3HbtwDPc0ANCf6UKZLTcv11LZAXNy7sM5\ntCAAQN+m2X2J9rhRhDk59+EYpS0IAN/siArtEcoAAABMoObBiIco1BDivW/69iUAAACgzqsWWqFe\nOZUyAACd0YIAQKTdDVxydv0pJJQBAAAACCCUAQDojHUhgHeVjAnGDbiONWUAAAAm8hi6pJyFMBBI\npQwAAMCkBDLUsq7ZsYQyAAAd0oIAQAQttMfSvgQA0CktCADQt9v9fo/+DgAAAADT0b4EAAAAEEAo\nAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABBAKAMAAAAQQCgD\nAAAAEEAoAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABBAKAMA\nAAAQQCgDAAAAEEAoAwAAABBAKAMAAAAQQCgDAAAAEEAoAwAAABDgP0b//O1ISSYpAAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f67013c50f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}